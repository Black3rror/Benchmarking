{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EdgeMark","text":"<p>The EdgeMark project comes with two goals in mind:</p> <ul> <li>To provide an automation tool that can help developers, engineers, and researchers to save time and effort in the process of creating an embedded AI (eAI) system.</li> <li>To benchmark several eAI tools and libraries and provide a comparison of their performance and resource usage.</li> </ul>"},{"location":"#modules","title":"Modules","text":"<p>We have tried to follow a modular approach in the design of this tool. This allows users to not only extend the benchmarking process by adding new tools and libraries but also enables them to utilize specific parts of the tool for their own applications. For example, the tool can be easily employed for generation and conversion of models to different formats using various optimization techniques.</p> <p> </p> <p>Currently, the following modules are available:</p> <ul> <li>Generate TF models: Generates, compiles, trains, and evaluates TensorFlow models based on user-provided configuration files.</li> <li>Generate Ekkono models: Similar to the Generate TF models module and based on the same configuration files, it can generate and train models for the Ekkono platform. The main outputs of this module are the Ekkono Edge models and their Crystal representations in C.</li> <li>Convert to TFLite: Converts TensorFlow models to TensorFlow Lite format. Various optimization options are available during the conversion process.</li> <li>Convert to TFLM: Converts TensorFlow Lite models to their representation in TensorFlow Lite for Microcontrollers format (C++ code).</li> <li>Convert to Edge Impulse: Converts TensorFlow Lite models into the Edge Impulse format (C++ code).</li> <li>Convert to eAI Translator: Prepares the necessary data files for the eAI Translator's C project. Please note, however, the translation of the model cannot be automated - you will need to manually complete this step using the eAI Translator tool.</li> <li>Test on NUCLEO-L4R5ZI: Tests TFLM/Edge Impulse/Ekkono models on the NUCLEO-L4R5ZI board. The corresponding project will be compiled and flashed to the board. The results will be collected and stored in a YAML file, as well as being summarized in an Excel file. The results include: inference time, flash size, RAM usage, and error.</li> <li>Test on RenesasRX65N: Similar to the Test on NUCLEO-L4R5ZI module, but designed for testing on the Renesas RX65N board.</li> </ul> <p></p> <p>In the next section, we will cover the details of how to use this tool.</p>"},{"location":"how_to_use/","title":"How To Use","text":"<p>It is easy to use EdgeMark. You just need to:</p> <ol> <li> <p>Clone the repository and navigate to the root directory of the project.     <pre><code>git clone https://github.com/Black3rror/EdgeMark.git\ncd EdgeMark\n</code></pre></p> </li> <li> <p>Run the main Python script. (1)</p> <ol> <li> We recommend running the script inside a virtual environment like Conda.</li> </ol> PythonMakefile <pre><code>python -m edgemark.main\n</code></pre> <p><pre><code>make # (1)!\n</code></pre></p> <ol> <li> The Makefile is a wrapper around the Python script. It will run the script with the same command as in Python. It exists just for convenience.</li> </ol> </li> <li> <p>Follow the instructions on the screen.</p> </li> </ol> <p>On the first run, the script will automatically install the necessary dependencies for itself. On the second run, the user can ask the script to install any additional Python dependencies required for the automation process. Third time is the charm! You should be greeted with the tool's main menu, where you'll be asked to choose your desired action.</p>"},{"location":"how_to_use/#project-setup","title":"Project Setup","text":"<p>You can obtain the project's repository by either cloning it using git (1) or downloading it as a ZIP file directly from the GitHub page. To run the project, you'll need to have Python installed on your machine (2). We recommend using a virtual environment to isolate dependencies and prevent conflicts with other Python projects. If you prefer, you can use Conda, which conveniently manages virtual environments and can also install Python for you. (3)</p> <ol> <li> If you don't have git installed, you can download it from here.</li> <li> The project has been tested with Python 3.11. You can download the appropriate version from here.</li> <li> To use Conda, you can download and install Anaconda from here.</li> </ol> <p>After obtaining the repository, several tools need to be connected to the project in order to provide certain functionalities. While these requirements are mostly guided through the instructions of the main script, we will provide more detailed explanations here.</p>"},{"location":"how_to_use/#edge-impulse-secrets","title":"Edge Impulse secrets","text":"<p>  Edge Impulse v1.56.13 (date 12-09-2024) was used in our tests.</p> <p>To use the Convert to Edge Impulse module, you need to provide your Edge Impulse API Key and Project ID. For this purpose, you need to have an account on Edge Impulse. If you don't have one, you can create it here.</p> <p>Once your account is set up, locate the API Key and Project ID:</p> <ul> <li> <p>The API key can be found in the Keys section of your project. (1)</p> <ol> <li> You can also access this page by its address: https://studio.edgeimpulse.com/studio/{project_id}/keys.</li> </ol> </li> <li> <p>The Project ID can be found on the project's main page under the Project info section. (1)</p> <ol> <li> You can also find it by looking at the URL of the page: https://studio.edgeimpulse.com/studio/{project_id}.</li> </ol> </li> </ul> <p>After obtaining this information, provide it to the project's main script in order to make the connection. The script will ask you for the API Key and Project ID (if they don't already exist) and saves them in edgemark/models/platforms/EI/configs/EI_converter_user_config.yaml.</p> <p>Now, you are ready to use the Convert to Edge Impulse module.</p>"},{"location":"how_to_use/#ekkono-sdk","title":"Ekkono SDK","text":"<p>  Ekkono SDK v23.10 was used in our tests.</p> <p>As Ekkono is not a free tool, you will need a valid license to use it. You can learn more about Ekkono on their website. After obtaining the license, you can either install it using pip or provide its path (1) to the main script.</p> <ol> <li> For example, if you place the files in edgemark/models/platforms/Ekkono, the path to the Python wheel would be edgemark/models/platforms/Ekkono/ekkono-sdk/primer/python/{distribution}/{python-version}/ekkono.primer-{name-suffix}.whl.</li> </ol> <p>Further, you need to download Ekkono's C inference code and place it in the corresponding unzipped project directory for your specific hardware platform. (1)</p> <ol> <li> For NUCLEO-L4R5ZI, it will be edgemark/Hardware/STM32/NUCLEO-L4R5ZI_Ekkono/Core/Inc/Ekkono_lib and for RenesasRX65N, it will be edgemark/Hardware/Renesas/RenesasRX_Ekkono/src/Ekkono_lib.</li> </ol> <p>At this point, you should be able to both generate Ekkono models using the Generate Ekkono models module and proceed with Test on NUCLEO-L4R5ZI/RenesasRX65N.</p>"},{"location":"how_to_use/#stm32cubeide","title":"STM32CubeIDE","text":"<p>  STM32CubeIDE v1.14.1 was used in our tests.</p> <p>STM32CubeIDE is required to compile the C/C++ projects for the STM32 boards. You can download it from the ST website. After installation, you need to provide its executable path (1) and the workspace directory (2) to the main script. When prompted by the script, enter these details, which will be saved in the file edgemark/models/automate/hardware_types/NUCLEO-L4R5ZI/configs/hardware_user_config.yaml.</p> <ol> <li> The executable path can be found at {installation_dir}/STM32CubeIDE_{version}/STM32CubeIDE/stm32cubeide.exe. For example, C:/ST/STM32CubeIDE_1.14.1/STM32CubeIDE/stm32cubeide.exe.</li> <li> You can locate the workspace directory by opening STM32CubeIDE and navigating to File &gt; Switch Workspace &gt; Other....</li> </ol> <p>The next step is to ensure the projects are correctly set up and added to the workspace. The zipped projects can be found in edgemark/Hardware/STM32. Please unzip them, and then add them to the workspace by opening the STM32CubeIDE and selecting File &gt; Open Projects from File System.... Choose the unzipped project folder and click Finish. The project should now appear in the Project Explorer.</p> <p>Note</p> <p>Please note that the Ekkono library is excluded from the NUCLEO-L4R5ZI_Ekkono project. You need to add it manually. After obtaining the license, you need to download their C inference code and place it in edgemark/Hardware/STM32/NUCLEO-L4R5ZI_Ekkono/Core/Inc/Ekkono_lib</p> <p>Now, the automation script should be able to compile the projects. To upload the compiled programs to the board, the automation script requires the STM32CubeProgrammer CLI, which we'll cover in the next step.</p>"},{"location":"how_to_use/#stm32cubeclt","title":"STM32CubeCLT","text":"<p>  STM32CubeProgrammer v2.16.0 was used in our tests.</p> <p>STM32Programmer CLI as a part of STM32CubeCLT allows the automation script to upload the compiled programs to the STM32 boards. You can download it from the ST website. After installation, provide its executable path (1) to the main script when prompted. The path will be saved in edgemark/models/automate/hardware_types/NUCLEO-L4R5ZI/configs/hardware_user_config.yaml.</p> <ol> <li> The executable will be in {installation_dir}/STM32CubeCLT_{version}/STM32CubeProgrammer/bin/STM32_programmer_CLI.exe. For example, C:/ST/STM32CubeCLT_1.15.1/STM32CubeProgrammer/bin/STM32_programmer_CLI.exe.</li> </ol> <p>Tip</p> <p>When connecting your NUCLUEO-L4R5ZI board to your PC, you need to have the ST-Link driver installed. It's recommended to upload a project to your board for the first time using STM32CubeIDE to verify that the driver is correctly installed and everything is functioning as expected.</p> <p>At this point, you should be ready to use the Test on NUCLEO-L4R5ZI module.</p>"},{"location":"how_to_use/#renesas-e2-studio","title":"Renesas e2 studio","text":"<p>  Renesas e2 studio v24.1.1 was used in our tests.</p> <p>Renesas e2 studio is required to compile the C/C++ projects for the Renesas boards. You can download it from the Renesas website. After installation, you need to provide its executable path (1) and the workspace directory (2) to the main script. When prompted by the script, enter these details, which will be saved in the file edgemark/models/automate/hardware_types/RenesasRX65N/configs/hardware_user_config.yaml.</p> <ol> <li> The executable will be {installation_dir}/Renesas/e2_studio/eclipse/e2studioc.exe. For example, C:/Renesas/e2_studio/eclipse/e2studioc.exe.</li> <li> You can locate the workspace directory by opening e2 studio and navigating to File &gt; Switch Workspace &gt; Other....</li> </ol> <p>The next step is to ensure the projects are correctly set up and added to the workspace. The zipped projects can be found in edgemark/Hardware/Renesas. Please unzip them, and then add them to the workspace by opening e2 studio and selecting File &gt; Open Projects from File System.... Choose the unzipped project folder and click Finish. The project should now appear in the Project Explorer.</p> <p>Note</p> <p>Please note that the Ekkono library is excluded from the RenesasRX_Ekkono project. You need to add it manually. After obtaining the license, you need to download their C inference code and place it in edgemark/Hardware/Renesas/RenesasRX_Ekkono/src/Ekkono_lib</p> <p>Now, the automation script should be able to compile the projects. To upload the compiled programs to the board, the automation script requires the Renesas Flash Programmer, which we'll cover in the next step.</p>"},{"location":"how_to_use/#renesas-flash-programmer","title":"Renesas Flash Programmer","text":"<p>  Renesas Flash Programmer v3.15.00 was used in our tests.</p> <p>Renesas Flash Programmer allows the automation script to upload the compiled programs to the Renesas boards. You can download it from the Renesas website. After installation, open it and create a new project by clicking File &gt; New Project.... In the project creation window, configure it as follows:</p> <ul> <li>Microcontroller: RX65x</li> <li>Project Name: [Your preferred name]</li> <li>Project Folder: [Your preferred folder]</li> <li>Tool: E2 emulator Lite</li> <li>Interface: FINE</li> </ul> <p>Additionally, click Tool Details, go to Reset Settings, and configure Reset signal at Disconnection to Reset Pin as Hi-Z. After these steps, click Connect.</p> <p>In the next step you need to provide the RFP's executable path (1) and its project path (2) to the main script. Run the script and provide them when asked. The script will save them in edgemark/models/automate/hardware_types/RenesasRX65N/configs/hardware_user_config.yaml.</p> <ol> <li> The executable will be {installation_dir}/Renesas Electronics/Programming Tools/Renesas Flash Programmer V{version}/RFPV{version}.exe. For example, C:/Program Files (x86)/Renesas Electronics/Programming Tools/Renesas Flash Programmer V3.15/RFPV3.exe</li> <li> The project file is a .rpj file, located in the folder you selected earlier.</li> </ol> <p>Tip</p> <p>We recommend uploading a project to your board manually via Renesas e2 studio the first time to ensure everything is set up correctly.</p> <p>Unlike the NUCLEO-L4R5ZI board, the Renesas RX65N target board does not connect to the PC via its programmer for serial communication. To establish a serial connection, you\u2019ll need to connect the board to the PC using a USB-to-TTL cable. Connect the USB side to the PC, and on the TTL side, connect the GND to the GND pin on the board (pin 61), and the RX to the TX pin on the board (pin 45). Be sure to install the necessary driver on your PC before proceeding.</p> <p>Now, you're ready to use the Test on RenesasRX65N module.</p>"},{"location":"how_to_use/#modules","title":"Modules","text":"<p>As mentioned in the Home page, the project consists of multiple modules. The modules can be run one after another to reach the final goal. The edgemark.main script provides an intuitive interface for managing the execution of these modules. Below, we describe each module, their requirements, relationships, and key configurations that might be of your interest to change.</p> <p> </p>"},{"location":"how_to_use/#generate-tf-models","title":"Generate TF models","text":"<p>You should describe the desired models in Model Description Files. The Model Generator produces TensorFlow models based on these files.</p> <p>The Model Description Files are YAML files located in target_models directory. The name of the model will be the path to the file without the extension. If the name of the file or any directory leading to that file begins with a dot (.), the file will be ignored. Please refer to the Model Description Files page for more detailed instructions on creating these files.</p> <p>Configurations for this module are located in edgemark/models/platforms/TensorFlow/configs/model_generator_config.yaml. Below are the primary configurations you may want to modify:</p> <ul> <li>wandb_online: Enables cloud-based logging through W&amp;B (Weights &amp; Biases). Otherwise, logs are saved locally.</li> <li>wandb_project_name: Specifies the W&amp;B project name.</li> <li>train_models: If enabled, the generated models will undergo training.</li> <li>evaluate_models: If enabled, the models will be evaluated after training.</li> <li>measure_execution_time: If enabled, the execution time of the models on your machine will be measured.</li> <li>n_representative_data: Number of representative data that will be used in some conversions of the Convert to TFLite module.</li> <li>n_eqcheck_data: Number of data samples that will be used to check the equivalence of the original and on-board models.</li> <li>epochs: If defined, overrides the number of epochs specified in the Model Description Files. This is useful for demonstration or debugging purposes.</li> </ul>"},{"location":"how_to_use/#generate-ekkono-models","title":"Generate Ekkono models","text":"<p>License Required</p> <p>Ekkono is a commercial product and requires a valid license. You can find more information on their website.</p> <p>Same as Generate TF models, this module takes in the Model Description Files. These files can be the same ones used for generating TensorFlow models; however, please note the limitations of Ekkono. For example, Ekkono does not support CNNs, so any file containing CNN layers will result in an error.</p> <p>Since Ekkono operates in its own environment, we have streamlined the generation and conversion steps into a single module. The output of this module consists of Ekkono's Crystal models along with the necessary C files for benchmarking.</p> <p>The configuration is similar to the TensorFlow module and can be found in edgemark/models/platforms/Ekkono/configs/model_generator_config.yaml. Key configurations include:</p> <ul> <li>wandb_online: Enables cloud-based logging through W&amp;B (Weights &amp; Biases). Otherwise, logs are saved locally.</li> <li>wandb_project_name: Specifies the W&amp;B project name.</li> <li>train_models: If enabled, the generated models will undergo training.</li> <li>evaluate_models: If enabled, the models will be evaluated after training.</li> <li>measure_execution_time: If enabled, the execution time of the models on your machine will be measured.</li> <li>n_eqcheck_data: Number of data samples that will be used to check the equivalence of the original and on-board models.</li> <li>epochs: If defined, overrides the number of epochs specified in the Model Description Files. This is useful for demonstration or debugging purposes.</li> </ul>"},{"location":"how_to_use/#convert-to-tflite","title":"Convert to TFLite","text":"<p>Once TensorFlow models are generated, you can use the Convert to TFLite module to convert the models into TFLite format. Various optimizations can be applied to the models during this conversion.</p> <p>The configuration file is located at edgemark/models/platforms/TFLite/configs/TFLite_converter_config.yaml. Key configurations include:</p> <ul> <li>conversion_timeout: The maximum time in seconds that the conversion process can take.</li> <li>optimizations: List of optimizations that should be applied to each model.</li> </ul> <p>The available optimizations are:</p> <ul> <li>basic: No optimization, just the standard conversion.</li> <li>q_dynamic: Dynamic range quantization. Weights are quantized to 8-bit integers, while activations are stored in 32-bit floats. Activations can be dynamically quantized to 8-bit integers to accelerate inference and then dequantized back to 32-bit floats for storage. Learn more here.</li> <li>q_full_int: Full integer quantization. Both weights and activations are quantized to 8-bit integers. However, the input and output remain in 32-bit floats. Learn more here.</li> <li>q_full_int_only: Similar to q_full_int, but in this case, everything is quantized to integers without fallback to floats. Learn more here.</li> <li>q_16x8: 16x8 quantization. To improve the accuracy of the quantized model, the activations are quantized to 16-bit integers while the weights are quantized to 8-bit integers. Similar to q_full_int, the input and output are left in 32-bit floats. Learn more here.</li> <li>q_16x8_int_only: Same as q_16x8, but everything is quantized to integers without fallback to floats. Learn more here.</li> <li>q_float16: Float16 quantization. Weights are quantized to 16-bit floats, reducing model size with minimal impact on accuracy. Learn more here.</li> <li>p_{percentage}: Post-training weight pruning. For example, \"p_75\" means 75% of the weights will be pruned. Learn more here.</li> <li>c_{clusters}: Weight clustering. For example, \"c_16\" clusters the weights into 16 groups. Learn more here.</li> </ul> <p>Tip</p> <p>You can combine multiple optimizations using a plus (+) sign. For example, \"p_25 + c_16 + q_full_int_only\" will prune 25% of the weights, cluster the weights into 16 groups, and fully quantize everything into integers. Note: Quantization schemes should always be applied as a final step.</p> <p>Once converted, the model is ready for further conversion to TFLM, Edge Impulse, or eAI Translator formats.</p>"},{"location":"how_to_use/#convert-to-tflm","title":"Convert to TFLM","text":"<p>This module requires the TFLite models generated by the Convert to TFLite module and their corresponding TFLM_info files from Generate TF models. The latter contains information like an estimation of the required arena_size and the necessary operator resolver functions.</p> <p>Note</p> <p>TFLM does not support q_float16 quantization. Models with this optimization will be ignored.</p> <p>The output of this module is a set of C++ files for each model, which can be used for benchmarking or integrated into any other C++ project.</p>"},{"location":"how_to_use/#convert-to-edge-impulse","title":"Convert to Edge Impulse","text":"<p>This module converts TFLite models into the Edge Impulse format by uploading the models to the Edge Impulse cloud and downloading the required files. The user must have an Edge Impulse account and provide their API Key and Project ID. These should be placed in the file located at edgemark/models/platforms/EI/configs/EI_converter_user_config.yaml under the keys ei_api_key and ei_project_id, respectively.</p> <p>Note</p> <p>Edge Impulse only supports the basic and q_full_int_only optimizations. Other types of optimizations will be ignored.</p>"},{"location":"how_to_use/#convert-to-eai-translator","title":"Convert to eAI Translator","text":"<p>This module in fact does not convert the models to eAI Translator format, but it generates the necessary C data files required for benchmarking. Since Renesas eAI Translator does not provide an automated method for converting models, users must manually perform the conversion. In the stage of testing the models on the board, the script will check if the eAI Translator models are available and if not, it will inform the user where they can find the TFLite models and where the eAI Translator models should be placed.</p> <p>Note</p> <p>Renesas eAI Translator only supports the basic and q_full_int_only optimizations. Other types of optimizations will be ignored.</p>"},{"location":"how_to_use/#test-on-nucleo-l4r5zirenesasrx65n","title":"Test on NUCLEO-L4R5ZI/RenesasRX65N","text":"<p>This module uses the generated C/C++ files from Generate Ekkono models, Convert to TFLM, Convert to Edge Impulse, or Convert to eAI Translator and integrates them into a benchmarking project for either the NUCLEO-L4R5ZI or RenesasRX65N board. Once integrated, the module compiles the program, uploads it to the board, and captures the benchmarking output.</p> <p>An overview of the results are saved in an Excel file, while detailed results can be found in each model's directory. Further, comparison plots will be generated to visualize the performance of different model types.</p> <p>The results are: Execution Time, Flash and RAM Usage, and Error.</p> <p>Tip</p> <p>All details, generated files, results and error messages related to each model can be found in the model's directory under saved_models/{TensorFlow/Ekkono}/{model_type}/{time_tag}</p> <p>The configuration options for this module can be found in edgemark/models/automate/configs/automate_config.yaml which includes:</p> <ul> <li>arena_finder: If enabled, the script will attempt to find the optimal arena size for each TFLM model.</li> <li>benchmark_overall_timeout: Specifies the maximum duration in seconds for the entire benchmarking process.</li> <li>benchmark_silence_timeout: Specifies the maximum duration in seconds that the board is allowed to remain silent without any output.</li> </ul>"},{"location":"how_to_use/#bonus-result-plotter","title":"(Bonus) Result plotter","text":"<p>The Result plotter script in edgemark/models/utils/result_plotter.py will help you generate figures comparing the results of your benchmark. Please see its API for more information.</p> <p>You are ready to go!</p> <p>You now have all the information you need to use the EdgeMark. We hope you find it useful \ud83d\ude0a</p>"},{"location":"model_description_files/","title":"Model description files","text":"<p>Each YAML file in this directory or its subdirectories represents a target model. In order to exclude a target model from the model generation process, simply add dot (.) to the beginning of the file name. For example, to exclude the target model target_model.yaml, rename it to .target_model.yaml. All targets in a directory can be excluded by adding dot (.) to the beginning of the directory name.</p> <p>Each target can be one of the following types:</p> <ul> <li>CNN: A combination of CNN layers followed by a number of fully connected layers. CNN layers can be left empty in order to create a fully connected network.</li> <li>CNN_MBNet: The MobileNetV2 network.</li> <li>RNN: Recurrent neural network, including simple RNN, LSTM, and GRU.</li> <li>TinyMLPerf_AE: The model used in the TinyMLPerf benchmarking suite for anomaly detection.</li> <li>TinyMLPerf_DS_CNN: The model used in the TinyMLPerf benchmarking suite for keyword spotting.</li> <li>TinyMLPerf_MBNet: The model used in the TinyMLPerf benchmarking suite for image classification.</li> <li>TinyMLPerf_ResNet: The model used in the TinyMLPerf benchmarking suite for image classification.</li> </ul>"},{"location":"model_description_files/#cnn","title":"CNN","text":"<p>The CNN target model should be defined like this: <pre><code>model_type: \"CNN\"\nconvs_params: [\n    # each element creates a convolutional layer\n    # [c, k, s] where c is the number of channels, k is the kernel size, and s is the stride. For example: [32, 3, 1]\n    # k and s can be a tuple. For example: [32, [3, 1], [2, 1]]\n    # if c is zero, but k and s have values, it means a max pooling layer. [0, 2, 2]\n    # if c and k are zero, it means a global average pooling layer. [0, 0, 0]\n]\ndenses_params: []       # each element creates a dense layer. For example: [128, 64] creates two dense layers with 128 and 64 units.\nconvs_dropout: 0.00     # should be a float between 0 and 1\ndenses_dropout: 0.00    # should be a float between 0 and 1\nactivation: \"relu\"      # activation function for all layers (except the output layer)\nuse_batch_norm: True    # whether to use batch normalization\nepochs: 10              # number of epochs for training\nbatch_size: 32          # batch size for training\ndataset:                # dataset configuration\n  name: \"sinus\"         # dataset name. shouold match the directory name of the dataset in the datasets directory\n  args:\n    # arguments to be passed to the dataset class\n    # for example, it can be as follows:\n    n_samples: 100000\n    test_ratio: 0.2\n    random_seed: 42\nrandom_seed: 42         # random seed for reproducibility. If null, a random seed will be used\n</code></pre></p>"},{"location":"model_description_files/#cnn_mbnet","title":"CNN_MBNet","text":"<p>The MobileNetV2 target model should be defined like this: <pre><code>model_type: \"CNN_MBNet\"\nepochs: 10              # number of epochs for training\nbatch_size: 32          # batch size for training\ndataset:                # dataset configuration\n  name: \"imagenet_v2\"   # dataset name. shouold match the directory name of the dataset in the datasets directory\n  args:\n    # arguments to be passed to the dataset class\n    # for example, it can be as follows:\n    test_ratio: 0.2\n    random_seed: 42\nrandom_seed: 42         # random seed for reproducibility. If null, a random seed will be used\n</code></pre></p>"},{"location":"model_description_files/#rnn","title":"RNN","text":"<p>The RNN target model should be defined like this: <pre><code>model_type: \"RNN\"\nrnn_type: \"LSTM\"        # RNN type. Can be \"SimpleRNN\", \"LSTM\", or \"GRU\"\nembedding_dim: null     # embedding dimension. If null, no embedding layer will be used\nrnn_units: 64           # number of units in the RNN layer\nepochs: 50              # number of epochs for training\nbatch_size: 64          # batch size for training\ndataset:                # dataset configuration\n  name: \"randomset_seq\" # dataset name. shouold match the directory name of the dataset in the datasets directory\n  args:\n    # arguments to be passed to the dataset class\n    # for example, it can be as follows:\n    n_samples: 1000\n    test_ratio: 0.2\n    input_size: 32\n    output_size: 32\n    sequence_length: 100\n    sequential_output: true\n    using_embedding: false\n    random_seed: 42\nrandom_seed: 42         # random seed for reproducibility. If null, a random seed will be used\n</code></pre></p>"},{"location":"model_description_files/#tinymlperf_ae","title":"TinyMLPerf_AE","text":"<p>The TinyMLPerf AutoEncoder target model should be defined like this: <pre><code>model_type: \"TinyMLPerf_AE\"\nload_pretrained_model: True   # whether to load the pretrained model or create a new one\n# in case a new model is created, the following commented parameters will be used\n# denses_params: [128, 128, 128, 128, 8, 128, 128, 128]   # each element creates a dense layer\n# denses_dropout: 0.00  # should be a float between 0 and 1\n# activation: \"relu\"    # activation function for all layers (except the output layer)\n# use_batch_norm: True  # whether to use batch normalization\nepochs: 0               # number of epochs for training\nbatch_size: 512         # batch size for training\ndataset:                # dataset configuration\n  name: \"toyADMOS\"      # dataset name. shouold match the directory name of the dataset in the datasets directory\n  args:\n    # arguments to be passed to the dataset class\n    # for example, it can be as follows:\n    n_mels: 128\n    frames: 5\n    n_fft: 1024\n    hop_length: 512\n    power: 2.0\n    test_ratio: 0.1\n    random_seed: 42\nrandom_seed: 42         # random seed for reproducibility. If null, a random seed will be used\n</code></pre></p>"},{"location":"model_description_files/#tinymlperf_ds_cnn","title":"TinyMLPerf_DS_CNN","text":"<p>The TinyMLPerf Depthwise Separable Convolutional Neural Network (DS-CNN) target model should be defined like this: <pre><code>model_type: \"TinyMLPerf_DS_CNN\"\nload_pretrained_model: True   # whether to load the pretrained model or create a new one\n# in case a new model is created, the following commented parameters will be used\n# num_filters: 64       # number of filters in the convolutional layers\n# activation: \"relu\"    # activation function for all layers (except the output layer)\n# use_batch_norm: True  # whether to use batch normalization\nepochs: 0               # number of epochs for training\nbatch_size: 100         # batch size for training\ndataset:                # dataset configuration\n  name: \"randomset_classification\"  # dataset name. shouold match the directory name of the dataset in the datasets directory\n  args:\n    # arguments to be passed to the dataset class\n    # for example, it can be as follows:\n    n_samples: 10000\n    test_ratio: 0.2\n    feature_shape: [49, 10, 1]\n    num_labels: 12\n    random_seed: 42\nrandom_seed: 42         # random seed for reproducibility. If null, a random seed will be used\n</code></pre></p>"},{"location":"model_description_files/#tinymlperf_mbnet","title":"TinyMLPerf_MBNet","text":"<p>The TinyMLPerf MobileNetV1 target model should be defined like this: <pre><code>model_type: \"TinyMLPerf_MBNet\"\nload_pretrained_model: True   # whether to load the pretrained model or create a new one\n# in case a new model is created, the following commented parameters will be used\n# num_filters: 8        # number of filters in the first convolutional layer\n# activation: \"relu\"    # activation function for all layers (except the output layer)\n# use_batch_norm: True  # whether to use batch normalization\nepochs: 0               # number of epochs for training\nbatch_size: 32          # batch size for training\ndataset:                # dataset configuration\n  name: \"vww\"           # dataset name. shouold match the directory name of the dataset in the datasets directory\n  args:\n    # arguments to be passed to the dataset class\n    # for example, it can be as follows:\n    image_size: [96, 96]\n    dataset_ratio: 0.1\n    test_ratio: 0.1\n    flat_features: False\n    random_seed: 42\nrandom_seed: 42         # random seed for reproducibility. If null, a random seed will be used\n</code></pre></p>"},{"location":"api/main/","title":"Main","text":""},{"location":"api/main/#edgemark.main","title":"edgemark.main","text":""},{"location":"api/main/#edgemark.main.check_package_requirements","title":"check_package_requirements","text":"<pre><code>check_package_requirements(requirements_file)\n</code></pre> <p>Check if the required packages are installed.</p> <p>Parameters:</p> Name Type Description Default <code>requirements_file</code> <code>str</code> <p>The path to the requirements file.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all the required packages are installed, False otherwise.</p> Source code in <code>edgemark/main.py</code> <pre><code>def check_package_requirements(requirements_file):\n    \"\"\"\n    Check if the required packages are installed.\n\n    Args:\n        requirements_file (str): The path to the requirements file.\n\n    Returns:\n        bool: True if all the required packages are installed, False otherwise.\n    \"\"\"\n    with open(requirements_file, 'r') as file:\n        requirements = file.readlines()\n\n    try:\n        pkg_resources.require(requirements)\n    except pkg_resources.UnknownExtra:  # Bug: we'll get this error even if everything is fine, so we'll ignore it\n        pass\n    except (pkg_resources.DistributionNotFound, pkg_resources.VersionConflict):\n        return False\n\n    return True\n</code></pre>"},{"location":"api/main/#edgemark.main.install_requirements","title":"install_requirements","text":"<pre><code>install_requirements(requirements_file)\n</code></pre> <p>Install the required packages.</p> <p>Parameters:</p> Name Type Description Default <code>requirements_file</code> <code>str</code> <p>The path to the requirements file.</p> required Source code in <code>edgemark/main.py</code> <pre><code>def install_requirements(requirements_file):\n    \"\"\"\n    Install the required packages.\n\n    Args:\n        requirements_file (str): The path to the requirements file.\n    \"\"\"\n    subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', requirements_file, '--no-warn-script-location'])\n</code></pre>"},{"location":"api/main/#edgemark.main.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>The main function to give a user-friendly interface to run the project.</p> Source code in <code>edgemark/main.py</code> <pre><code>def main():\n    \"\"\"\n    The main function to give a user-friendly interface to run the project.\n    \"\"\"\n    # configs\n    reqirements_file = \"requirements.txt\"\n    ei_converter_config_path = \"edgemark/models/platforms/EI/configs/EI_converter_config.yaml\"\n    stm32_automate_config_path = \"edgemark/models/automate/hardware_types/NUCLEO-L4R5ZI/configs/hardware_config.yaml\"\n    renesas_automate_config_path = \"edgemark/models/automate/hardware_types/RenesasRX65N/configs/hardware_config.yaml\"\n    target_dir = \"target_models\"\n\n    time_tag = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    save_dir = os.path.join(\"benchmarking_results\", time_tag)\n\n    _clear_console()\n    page = \"\"\n\n    # check the all the required packages are installed\n    installed_requirements = False\n    while not check_package_requirements(reqirements_file):\n        q = _Question(\"Packages are missing. What do you want to do\",\n                      description=\"Please make sure that you are in the correct environment.\\nIt's best to create a new Conda environment with 'python 3.11.7'. The command would be 'conda create -n edgemark python=3.11.7'. Then activate the environment with 'conda activate edgemark' and run the project again.\",\n                      options=[\"Install packages for me\", \"I will install them myself\"],\n                      one_line_options=False,\n                      default=\"Install packages for me\")\n        response = q.ask()\n        _clear_console()\n        page = q.summarize() + \"\\n\"\n\n        if response == \"Install packages for me\":\n            install_requirements(reqirements_file)\n            installed_requirements = True\n\n        else:\n            page += \"You can find the requirements in the file '{}'. Please install them manually.\".format(reqirements_file)\n            print(page)\n            return\n\n    if installed_requirements:\n        page += \"Requirements are installed successfully.\\n\"\n        page += \"Please run the script again.\"\n        _clear_console()\n        print(page)\n        return\n\n    if import_error:\n        page += \"There was an \" + COLOR_TERTIARY +  \"error\" + COLOR_RESET + \" while importing the required modules.\\n\"\n        page += \"Please check the error below and fix the issue.\\n\"\n        page += import_error_traceback\n        _clear_console()\n        print(page)\n        return\n\n    page = \"\"\n    _clear_console()\n\n    page += \"Hello \" + COLOR_SECONDARY + \":)\" + COLOR_RESET + \"\\n\"\n    print(page)\n\n    # get the modules to run\n\n    available_modules = [\n        \"Generate TF models\",\n        \"Generate Ekkono models\",\n        \"Convert to TFLite\",\n        \"Convert to TFLM\",\n        \"Convert to Edge Impulse\",\n        \"Convert to eAI Translator\",\n        \"Test on NUCLEO-L4R5ZI (TFLM)\",\n        \"Test on NUCLEO-L4R5ZI (Edge Impulse)\",\n        \"Test on NUCLEO-L4R5ZI (Ekkono)\",\n        \"Test on RenesasRX65N (TFLM)\",\n        \"Test on RenesasRX65N (Edge Impulse)\",\n        \"Test on RenesasRX65N (Ekkono)\",\n        \"Test on RenesasRX65N (eAI Translator)\"\n    ]\n\n    q = _Question(\"What do you want to do\",\n                  description=\"Here you can see the options for running the full pipeline (generating models, converting, testing). Choose 'Others' if you want to run specific modules.\",\n                  options=[\n                      \"TFLM + NUCLEO-L4R5ZI\",\n                      \"TFLM + RenesasRX65N\",\n                      \"Edge Impulse + NUCLEO-L4R5ZI\",\n                      \"Edge Impulse + RenesasRX65N\",\n                      \"Ekkono + NUCLEO-L4R5ZI\",\n                      \"Ekkono + RenesasRX65N\",\n                      \"eAI Translator + RenesasRX65N\",\n                      \"Others\"\n                  ])\n    response = q.ask()\n    _clear_console()\n    page += q.summarize() + \"\\n\"\n    print(page)\n\n    if response == \"TFLM + NUCLEO-L4R5ZI\":\n        modules = [\n            \"Generate TF models\",\n            \"Convert to TFLite\",\n            \"Convert to TFLM\",\n            \"Test on NUCLEO-L4R5ZI (TFLM)\"\n        ]\n\n    elif response == \"TFLM + RenesasRX65N\":\n        modules = [\n            \"Generate TF models\",\n            \"Convert to TFLite\",\n            \"Convert to TFLM\",\n            \"Test on RenesasRX65N (TFLM)\"\n        ]\n\n    elif response == \"Edge Impulse + NUCLEO-L4R5ZI\":\n        modules = [\n            \"Generate TF models\",\n            \"Convert to TFLite\",\n            \"Convert to Edge Impulse\",\n            \"Test on NUCLEO-L4R5ZI (Edge Impulse)\"\n        ]\n\n    elif response == \"Edge Impulse + RenesasRX65N\":\n        modules = [\n            \"Generate TF models\",\n            \"Convert to TFLite\",\n            \"Convert to Edge Impulse\",\n            \"Test on RenesasRX65N (Edge Impulse)\"\n        ]\n\n    elif response == \"Ekkono + NUCLEO-L4R5ZI\":\n        modules = [\n            \"Generate Ekkono models\",\n            \"Test on NUCLEO-L4R5ZI (Ekkono)\"\n        ]\n\n    elif response == \"Ekkono + RenesasRX65N\":\n        modules = [\n            \"Generate Ekkono models\",\n            \"Test on RenesasRX65N (Ekkono)\"\n        ]\n\n    elif response == \"eAI Translator + RenesasRX65N\":\n        modules = [\n            \"Generate TF models\",\n            \"Convert to TFLite\",\n            \"Convert to eAI Translator\",\n            \"Test on RenesasRX65N (eAI Translator)\"\n        ]\n\n    elif response == \"Others\":\n        q = _Question(\"Which module(s) do you want to run\",\n                      description=\"You can choose multiple options by separating them with a plus sign (+). Example: 1+2\\nThe rationality of the sequence of the modules is user's responsibility.\",\n                      options=available_modules)\n        def _check_response(response):\n            valid = True\n            for m in response.split(\"+\"):\n                if not m.isdigit() or int(m) &lt; 1 or int(m) &gt; len(available_modules):\n                    valid = False\n                    break\n            if valid:\n                response = [available_modules[int(m) - 1] for m in response.split(\"+\")]\n                response = \" + \".join(response)\n            return valid, response\n        q.check_response = _check_response\n        response = q.ask()\n        _clear_console()\n        page += q.summarize() + \"\\n\"\n        print(page)\n\n        modules = [module.strip() for module in response.split(\"+\")]\n\n    # check requirements of the modules\n    modules_requiremets = []\n\n    if \"Generate Ekkono models\" in modules:\n        if \"ekkono_sdk\" not in modules_requiremets:\n            modules_requiremets.append(\"ekkono_sdk\")\n\n    if \"Convert to Edge Impulse\" in modules:\n        if \"edge_impulse_secrets\" not in modules_requiremets:\n            modules_requiremets.append(\"edge_impulse_secrets\")\n\n    if (\"Test on NUCLEO-L4R5ZI (TFLM)\" in modules or\n        \"Test on NUCLEO-L4R5ZI (Edge Impulse)\" in modules or\n        \"Test on NUCLEO-L4R5ZI (Ekkono)\" in modules):\n        if \"stm32cubeide\" not in modules_requiremets:\n            modules_requiremets.append(\"stm32cubeide\")\n        if \"stm32_programmer_cli\" not in modules_requiremets:\n            modules_requiremets.append(\"stm32_programmer_cli\")\n\n    if (\"Test on RenesasRX65N (TFLM)\" in modules or\n        \"Test on RenesasRX65N (Edge Impulse)\" in modules or\n        \"Test on RenesasRX65N (Ekkono)\" in modules or\n        \"Test on RenesasRX65N (eAI Translator)\" in modules):\n        if \"e2studio\" not in modules_requiremets:\n            modules_requiremets.append(\"e2studio\")\n        if \"rfp\" not in modules_requiremets:\n            modules_requiremets.append(\"rfp\")\n\n    # check if the requirements are satisfied\n    if \"ekkono_sdk\" in modules_requiremets:\n        while not _check_ekkono_installation():\n            _clear_console()\n            print(page)\n            q = _Question(\"Ekkono is not installed. Please provide the path to the wheel file\",\n                          description=\"Ekkono is not free. If you want to use it, you need to buy this product from https://www.ekkono.ai, download the its files and provide the wheel path here.\\nFor example, put the files in 'edgemark/models/platforms/Ekkono' and provide the path 'edgemark/models/platforms/Ekkono/ekkono-sdk/primer/python/{distribution}/{python-version}/ekkono.primer-{name-suffix}.whl'\")\n            q.check_response = lambda response: (True, response) if (os.path.exists(response) and response.endswith(\".whl\")) else (False, None)\n            response = q.ask()\n            subprocess.run([sys.executable, '-m', 'pip', 'install', response])\n            print(\"Ekkono has been installed. Please run the script again.\")\n            return\n        _clear_console()\n        print(page)\n\n    if \"edge_impulse_secrets\" in modules_requiremets:\n        ei_converter_config = OmegaConf.load(ei_converter_config_path)\n        if os.path.exists(ei_converter_config.user_config):\n            ei_converter_user_config = OmegaConf.load(ei_converter_config.user_config)\n        else:\n            ei_converter_user_config = OmegaConf.create()\n\n        valid_ei_api_key = False\n        if \"ei_api_key\" in ei_converter_user_config:\n            if ei_converter_user_config.ei_api_key.startswith(\"ei_\"):\n                valid_ei_api_key = True\n\n        if not valid_ei_api_key:\n            q = _Question(\"Edge Impulse information is missing. Please provide the API key\",\n                          description=\"If you don't have an account, please create one at https://www.edgeimpulse.com. The API key can be found in the 'Keys' section of the project. You can also access this page by its address: https://studio.edgeimpulse.com/studio/{project_id}/keys\",)\n            q.check_response = lambda response: (True, response) if response.startswith(\"ei_\") else (False, None)\n            response = q.ask()\n            _clear_console()\n            print(page)\n\n            ei_converter_user_config.ei_api_key = response\n            OmegaConf.save(ei_converter_user_config, ei_converter_config.user_config)\n\n        valid_ei_project_id = False\n        if \"ei_project_id\" in ei_converter_user_config:\n            valid_ei_project_id = True\n\n        if not valid_ei_project_id:\n            q = _Question(\"Edge Impulse information is missing. Please provide the project ID\",\n                          description=\"The project ID can be found in the main page of the project ('project info' section). You can also find it by looking at the URL of the page: https://studio.edgeimpulse.com/studio/{project_id}\",)\n            q.check_response = lambda response: (True, response) if response.isdigit() else (False, None)\n            response = q.ask()\n            _clear_console()\n            print(page)\n\n            ei_converter_user_config.ei_project_id = response\n            OmegaConf.save(ei_converter_user_config, ei_converter_config.user_config)\n\n    if \"stm32cubeide\" in modules_requiremets:\n        stm32_automate_config = OmegaConf.load(stm32_automate_config_path)\n        if os.path.exists(stm32_automate_config.user_config):\n            stm32_automate_user_config = OmegaConf.load(stm32_automate_config.user_config)\n        else:\n            stm32_automate_user_config = OmegaConf.create()\n\n        valid_stm32cubeide_path = False\n        if \"stm32cubeide_path\" in stm32_automate_user_config:\n            if os.path.exists(stm32_automate_user_config.stm32cubeide_path) or os.path.exists(stm32_automate_user_config.stm32cubeide_path + \".exe\"):\n                valid_stm32cubeide_path = True\n\n        if not valid_stm32cubeide_path:\n            q = _Question(\"Could not find a valid STM32CubeIDE path. Please provide the path to the STM32CubeIDE executable\",\n                          description=\"If you don't have STM32CubeIDE, you can download it from https://www.st.com/en/development-tools/stm32cubeide.html\\nIf you have already installed it, the executable path will be {installation_dir}/STM32CubeIDE_{version}/STM32CubeIDE/stm32cubeide.exe. For example, C:/ST/STM32CubeIDE_1.14.1/STM32CubeIDE/stm32cubeide.exe\\nThe project was tested against STM32CubeIDE version 1.14.1\")\n            q.check_response = lambda response: (True, response) if os.path.exists(response) or os.path.exists(response + \".exe\") else (False, None)\n            response = q.ask()\n            _clear_console()\n            print(page)\n\n            stm32_automate_user_config.stm32cubeide_path = response\n            OmegaConf.save(stm32_automate_user_config, stm32_automate_config.user_config)\n\n        valid_workspace_dir = False\n        if \"workspace_dir\" in stm32_automate_user_config:\n            if os.path.exists(stm32_automate_user_config.workspace_dir):\n                valid_workspace_dir = True\n\n        if not valid_workspace_dir:\n            q = _Question(\"STM32CubeIDE workspace directory does not exist. Please provide the workspace directory\",\n                          description=\"When STM32CubeIDE is open, you can find the workspace directory in File &gt; Switch Workspace &gt; Other...\")\n            q.check_response = lambda response: (True, response) if os.path.exists(response) else (False, None)\n            response = q.ask()\n            _clear_console()\n            print(page)\n\n            stm32_automate_user_config.workspace_dir = response\n            OmegaConf.save(stm32_automate_user_config, stm32_automate_config.user_config)\n\n        if \"Test on NUCLEO-L4R5ZI (TFLM)\" in modules:\n            stm32_automate_config.project_name = \"NUCLEO-L4R5ZI_TFLM\"\n            q = _Question(\"Can you confirm that the NUCLEO-L4R5ZI_TFLM project exists in your STM32CubeIDE's projects and also in this location: {}\".format(stm32_automate_config.project_dir),\n                          description=\"If you don't have the project in the specified location, probably you can find the zipped project in that directory. You can extract it and import it in STM32CubeIDE.\",\n                          options=[\"y\", \"n\"],\n                          one_line_options=True,\n                          default=\"y\")\n            response = q.ask()\n            if response == \"n\":\n                print(\"Please create the project and run the script again.\")\n                return\n            _clear_console()\n            print(page)\n\n        if \"Test on NUCLEO-L4R5ZI (Edge Impulse)\" in modules:\n            stm32_automate_config.project_name = \"NUCLEO-L4R5ZI_EI\"\n            q = _Question(\"Can you confirm that the NUCLEO-L4R5ZI_EI project exists in your STM32CubeIDE's projects and also in this location: {}\".format(stm32_automate_config.project_dir),\n                          description=\"If you don't have the project in the specified location, probably you can find the zipped project in that directory. You can extract it and import it in STM32CubeIDE.\",\n                          options=[\"y\", \"n\"],\n                          one_line_options=True,\n                          default=\"y\")\n            response = q.ask()\n            if response == \"n\":\n                print(\"Please create the project and run the script again.\")\n                return\n            _clear_console()\n            print(page)\n\n        if \"Test on NUCLEO-L4R5ZI (Ekkono)\" in modules:\n            stm32_automate_config.project_name = \"NUCLEO-L4R5ZI_Ekkono\"\n            q = _Question(\"Can you confirm that the NUCLEO-L4R5ZI_Ekkono project exists in your STM32CubeIDE's projects and also in this location: {}\".format(stm32_automate_config.project_dir),\n                          description=\"If you don't have the project in the specified location, probably you can find the zipped project in that directory. You can extract it and import it in STM32CubeIDE.\",\n                          options=[\"y\", \"n\"],\n                          one_line_options=True,\n                          default=\"y\")\n            response = q.ask()\n            if response == \"n\":\n                print(\"Please create the project and run the script again.\")\n                return\n            _clear_console()\n            print(page)\n\n    if \"stm32_programmer_cli\" in modules_requiremets:\n        stm32_automate_config = OmegaConf.load(stm32_automate_config_path)\n        if os.path.exists(stm32_automate_config.user_config):\n            stm32_automate_user_config = OmegaConf.load(stm32_automate_config.user_config)\n        else:\n            stm32_automate_user_config = OmegaConf.create()\n\n        valid_stm32_programmer_path = False\n        if \"stm32_programmer_path\" in stm32_automate_user_config:\n            if os.path.exists(stm32_automate_user_config.stm32_programmer_path) or os.path.exists(stm32_automate_user_config.stm32_programmer_path + \".exe\"):\n                valid_stm32_programmer_path = True\n\n        if not valid_stm32_programmer_path:\n            q = _Question(\"Could not find a valid STM32 Programmer CLI path. Please provide the path to the STM32 Programmer CLI executable\",\n                          description=\"STM32 Programmer CLI is a part of STM32CubeCLT. So, if you don't have STM32 Programmer CLI, you can download the STM32CubeCLT from https://www.st.com/en/development-tools/stm32cubeclt.html\\nOnce you have installed it, the executable should be in {installation_dir}/STM32CubeCLT_{version}/STM32CubeProgrammer/bin/STM32_programmer_CLI.exe. For example, C:/ST/STM32CubeCLT_1.15.1/STM32CubeProgrammer/bin/STM32_programmer_CLI.exe\\nThe project was tested against STM32CubeProgrammer version 2.16.0\",)\n            q.check_response = lambda response: (True, response) if os.path.exists(response) or os.path.exists(response + \".exe\") else (False, None)\n            response = q.ask()\n            _clear_console()\n            print(page)\n\n            stm32_automate_user_config.stm32_programmer_path = response\n            OmegaConf.save(stm32_automate_user_config, stm32_automate_config.user_config)\n\n    if \"e2studio\" in modules_requiremets:\n        renesas_automate_config = OmegaConf.load(renesas_automate_config_path)\n        if os.path.exists(renesas_automate_config.user_config):\n            renesas_automate_user_config = OmegaConf.load(renesas_automate_config.user_config)\n        else:\n            renesas_automate_user_config = OmegaConf.create()\n\n        valid_e2studio_path = False\n        if \"e2studio_path\" in renesas_automate_user_config:\n            if os.path.exists(renesas_automate_user_config.e2studio_path) or os.path.exists(renesas_automate_user_config.e2studio_path + \".exe\"):\n                valid_e2studio_path = True\n\n        if not valid_e2studio_path:\n            q = _Question(\"Could not find a valid e2 studio path. Please provide the path to the e2 studio executable\",\n                          description=\"If you don't have e2 studio, you can download it from https://www.renesas.com/us/en/software-tool/e-studio\\nOnce you have installed it, the executable should be {installation_dir}/Renesas/e2_studio/eclipse/e2studioc.exe. For example, C:/Renesas/e2_studio/eclipse/e2studioc.exe\\nThe project was tested against e2 studio version 24.1.1\",)\n            q.check_response = lambda response: (True, response) if os.path.exists(response) or os.path.exists(response + \".exe\") else (False, None)\n            response = q.ask()\n            _clear_console()\n            print(page)\n\n            renesas_automate_user_config.e2studio_path = response\n            OmegaConf.save(renesas_automate_user_config, renesas_automate_config.user_config)\n\n        valid_workspace_dir = False\n        if \"workspace_dir\" in renesas_automate_user_config:\n            if os.path.exists(renesas_automate_user_config.workspace_dir):\n                valid_workspace_dir = True\n\n        if not valid_workspace_dir:\n            q = _Question(\"e2 studio workspace directory does not exist. Please provide the workspace directory\",\n                          description=\"When e2 studio is open, you can find the workspace directory in File &gt; Switch Workspace &gt; Other...\")\n            q.check_response = lambda response: (True, response) if os.path.exists(response) else (False, None)\n            response = q.ask()\n            _clear_console()\n            print(page)\n\n            renesas_automate_user_config.workspace_dir = response\n            OmegaConf.save(renesas_automate_user_config, renesas_automate_config.user_config)\n\n        if \"Test on RenesasRX65N (TFLM)\" in modules:\n            renesas_automate_config.project_name = \"RenesasRX_TFLM\"\n            q = _Question(\"Can you confirm that the RenesasRX_TFLM project exists in your e2 studio's projects and also in this location: {}\".format(renesas_automate_config.project_dir),\n                          description=\"If you don't have the project in the specified location, probably you can find the zipped project in that directory. You can extract it and import it in e2 studio.\",\n                          options=[\"y\", \"n\"],\n                          one_line_options=True,\n                          default=\"y\")\n            response = q.ask()\n            if response == \"n\":\n                print(\"Please create the project and run the script again.\")\n                return\n            _clear_console()\n            print(page)\n\n        if \"Test on RenesasRX65N (Edge Impulse)\" in modules:\n            renesas_automate_config.project_name = \"RenesasRX_EI\"\n            q = _Question(\"Can you confirm that the RenesasRX_EI project exists in your e2 studio's projects and also in this location: {}\".format(renesas_automate_config.project_dir),\n                          description=\"If you don't have the project in the specified location, probably you can find the zipped project in that directory. You can extract it and import it in e2 studio.\",\n                          options=[\"y\", \"n\"],\n                          one_line_options=True,\n                          default=\"y\")\n            response = q.ask()\n            if response == \"n\":\n                print(\"Please create the project and run the script again.\")\n                return\n            _clear_console()\n            print(page)\n\n        if \"Test on RenesasRX65N (Ekkono)\" in modules:\n            renesas_automate_config.project_name = \"RenesasRX_Ekkono\"\n            q = _Question(\"Can you confirm that the RenesasRX_Ekkono project exists in your e2 studio's projects and also in this location: {}\".format(renesas_automate_config.project_dir),\n                          description=\"If you don't have the project in the specified location, probably you can find the zipped project in that directory. You can extract it and import it in e2 studio.\",\n                          options=[\"y\", \"n\"],\n                          one_line_options=True,\n                          default=\"y\")\n            response = q.ask()\n            if response == \"n\":\n                print(\"Please create the project and run the script again.\")\n                return\n            _clear_console()\n            print(page)\n\n        if \"Test on RenesasRX65N (eAI Translator)\" in modules:\n            renesas_automate_config.project_name = \"RenesasRX_eAI_Translator\"\n            q = _Question(\"Can you confirm that the RenesasRX_eAI_Translator project exists in your e2 studio's projects and also in this location: {}\".format(renesas_automate_config.project_dir),\n                          description=\"If you don't have the project in the specified location, probably you can find the zipped project in that directory. You can extract it and import it in e2 studio.\",\n                          options=[\"y\", \"n\"],\n                          one_line_options=True,\n                          default=\"y\")\n            response = q.ask()\n            if response == \"n\":\n                print(\"Please create the project and run the script again.\")\n                return\n            _clear_console()\n            print(page)\n\n    if \"rfp\" in modules_requiremets:\n        renesas_automate_config = OmegaConf.load(renesas_automate_config_path)\n        if os.path.exists(renesas_automate_config.user_config):\n            renesas_automate_user_config = OmegaConf.load(renesas_automate_config.user_config)\n        else:\n            renesas_automate_user_config = OmegaConf.create()\n\n        valid_rfp_path = False\n        if \"rfp_path\" in renesas_automate_user_config:\n            if os.path.exists(renesas_automate_user_config.rfp_path) or os.path.exists(renesas_automate_user_config.rfp_path + \".exe\"):\n                valid_rfp_path = True\n\n        if not valid_rfp_path:\n            q = _Question(\"Could not find a valid Renesas Flash Programmer path. Please provide the path to the Renesas Flash Programmer executable\",\n                          description=\"If you don't have Renesas Flash Programmer, you can download it from https://www.renesas.com/us/en/software-tool/renesas-flash-programmer-programming-gui#downloads\\nOnce you have installed it, the executable should be {installation_dir}/Renesas Electronics/Programming Tools/Renesas Flash Programmer V{version}/RFPV{version}.exe. For example, C:/Program Files (x86)/Renesas Electronics/Programming Tools/Renesas Flash Programmer V3.15/RFPV3.exe\\nThe project was tested against Renesas Flash Programmer version 3.15.00\",)\n            q.check_response = lambda response: (True, response) if os.path.exists(response) or os.path.exists(response + \".exe\") else (False, None)\n            response = q.ask()\n            _clear_console()\n            print(page)\n\n            renesas_automate_user_config.rfp_path = response\n            OmegaConf.save(renesas_automate_user_config, renesas_automate_config.user_config)\n\n        valid_rfp_project_path = False\n        if \"rfp_project_path\" in renesas_automate_user_config:\n            if os.path.exists(renesas_automate_user_config.rfp_project_path):\n                valid_rfp_project_path = True\n\n        if not valid_rfp_project_path:\n            q = _Question(\"Could not find a valid Renesas Flash Programmer project path. Please provide the path to the Renesas Flash Programmer project file\",\n                          description=\"The project file is a '.rpj' file that you can create in Renesas Flash Programmer.\\nTo create a project file, open Renesas Flash Programmer and do the following steps:\\n- File &gt; New Project...\\n- Microcontroller: RX65x\\n- Tool: E2 emulator Lite\\n- Interface: FINE\\n- Tool Details... &gt; Reset Settings &gt; Reset signal at Disconnect: Reset Pin as Hi-Z\\nOnce you have created the project, enter the path to the project file here\",\n                          check_response=lambda response: (True, response) if response.endswith(\".rpj\") and os.path.exists(response) else (False, None))\n            response = q.ask()\n            _clear_console()\n            print(page)\n\n            renesas_automate_user_config.rfp_project_path = response\n            OmegaConf.save(renesas_automate_user_config, renesas_automate_config.user_config)\n\n    if (\"Generate TF models\" in modules or\n        \"Generate Ekkono models\" in modules):\n        while True:\n            q = _Question(\"Please put all your model files in the {} directory. Can you confirm that this is done\".format(target_dir),\n                          description=\"You can follow the instructions in the {}. In short, the files that do not have dot (.) in their path will be generated\".format(target_dir + \"/README.md\"),\n                          options=[\"y\"],\n                          one_line_options=True,\n                          default=\"y\")\n            response = q.ask()\n            _clear_console()\n            print(page)\n            if response == \"y\":\n                break\n\n    if (\"Test on NUCLEO-L4R5ZI (TFLM)\" in modules or\n        \"Test on NUCLEO-L4R5ZI (Edge Impulse)\" in modules or\n        \"Test on NUCLEO-L4R5ZI (Ekkono)\" in modules):\n        while True:\n            q = _Question(\"Please\\n- Connect the NUCLEO-L4R5ZI board to the computer\\n- Close STM32CubeIDE\\n- Close any serial monitor applications (e.g. PuTTY)\\nCan you confirm that these items are addressed\",\n                        options=[\"y\"],\n                        one_line_options=True,\n                        default=\"y\")\n            response = q.ask()\n            _clear_console()\n            print(page)\n            if response == \"y\":\n                break\n\n    if (\"Test on RenesasRX65N (TFLM)\" in modules or\n        \"Test on RenesasRX65N (Edge Impulse)\" in modules or\n        \"Test on RenesasRX65N (Ekkono)\" in modules or\n        \"Test on RenesasRX65N (eAI Translator)\" in modules):\n        while True:\n            q = _Question(\"Please\\n- Connect the Renesas RX65N board to the computer\\n- Connect a USB to TTL cable between your computer and the board\\n- Close e2 studio\\n- Close Renesas Flash Programmer\\n- Close any serial monitor applications (e.g. PuTTY)\\nCan you confirm that these items are addressed\",\n                        options=[\"y\"],\n                        one_line_options=True,\n                        default=\"y\")\n            response = q.ask()\n            _clear_console()\n            print(page)\n            if response == \"y\":\n                break\n\n    # run the modules\n    page += \"\\n\" + COLOR_TERTIARY + \"Running the modules\" + COLOR_RESET + \"\\n\"\n    _clear_console()\n    print(page)\n\n    flawless = True\n    for i, module in enumerate(modules):\n        page += colorama.Style.DIM + \"[{}/{}]\".format(i + 1, len(modules)) + COLOR_RESET + \" \"\n\n        if module == \"Generate TF models\":\n            page += \"Generating TensorFlow models\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = tf_model_generator.main()\n\n            n_targets = len(output)\n            n_success = 0\n            failures = []\n            for target in output:\n                if target[\"result\"] == \"success\":\n                    n_success += 1\n                else:\n                    report = \"Model: {}\\n\\n\".format(target[\"name\"])\n                    report += \"Traceback:\\n{}\".format(target[\"traceback\"])\n\n                    report_name = target[\"name\"].replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                    report_path = os.path.join(save_dir, \"errors/Generate TF models\", \"{}.txt\".format(report_name))\n                    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                    with open(report_path, \"w\") as file:\n                        file.write(report)\n\n                    failures.append({\n                        \"name\": target[\"name\"],\n                        \"error\": target[\"error\"],\n                        \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                    })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {}\\n\".format(failure[\"name\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n            if n_success == 0 and i &lt; len(modules) - 1:\n                q = _Question(\"No model has survived. Do you want to continue\",\n                              description=\"If the following modules are dependent on the output of this module, this will probably lead to errors.\",\n                              options=[\"y\", \"n\"],\n                              one_line_options=True,\n                              default=\"n\")\n                response = q.ask()\n                if response == \"n\":\n                    return\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Generate Ekkono models\":\n            page += \"Generating Ekkono models\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = ekkono_model_generator.main()\n\n            n_targets = len(output)\n            n_success = 0\n            failures = []\n            for target in output:\n                if target[\"result\"] == \"success\":\n                    n_success += 1\n                else:\n                    report = \"Model: {}\\n\\n\".format(target[\"name\"])\n                    report += \"Traceback:\\n{}\".format(target[\"traceback\"])\n\n                    report_name = target[\"name\"].replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                    report_path = os.path.join(save_dir, \"errors/Generate Ekkono models\", \"{}.txt\".format(report_name))\n                    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                    with open(report_path, \"w\") as file:\n                        file.write(report)\n\n                    failures.append({\n                        \"name\": target[\"name\"],\n                        \"error\": target[\"error\"],\n                        \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                    })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {}\\n\".format(failure[\"name\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n            if n_success == 0 and i &lt; len(modules) - 1:\n                q = _Question(\"No model has survived. Do you want to continue\",\n                              description=\"If the following modules are dependent on the output of this module, this will probably lead to errors.\",\n                              options=[\"y\", \"n\"],\n                              one_line_options=True,\n                              default=\"n\")\n                response = q.ask()\n                if response == \"n\":\n                    return\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Convert to TFLite\":\n            page += \"Converting TensorFlow models to TFLite\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = tflite_converter.main()\n\n            n_targets = 0\n            n_success = 0\n            failures = []\n            for target in output:\n                for flavor in target[\"flavors\"]:\n                    n_targets += 1\n                    if flavor[\"result\"] == \"success\":\n                        n_success += 1\n                    else:\n                        report = \"Model: {}\\nFlavor: {}\\n\\n\".format(target[\"dir\"], flavor[\"flavor\"])\n                        if \"traceback\" in flavor:\n                            report += \"Traceback:\\n{}\".format(flavor[\"traceback\"])\n                        else:\n                            report += \"Exception file path: {}\\n\".format(flavor[\"exception_file\"])\n\n                        report_name = os.path.basename(target[\"dir\"])\n                        report_name += \"_\" + flavor[\"flavor\"]\n                        report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                        report_path = os.path.join(save_dir, \"errors/Convert to TFLite\", \"{}.txt\".format(report_name))\n                        os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                        with open(report_path, \"w\") as file:\n                            file.write(report)\n\n                        failures.append({\n                            \"dir\": target[\"dir\"],\n                            \"flavor\": flavor[\"flavor\"],\n                            \"error\": flavor[\"error\"],\n                            \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                        })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {} &gt;&gt; {}\\n\".format(failure[\"dir\"], failure[\"flavor\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n            if n_success == 0 and i &lt; len(modules) - 1:\n                q = _Question(\"No model has survived. Do you want to continue\",\n                              description=\"If the following modules are dependent on the output of this module, this will probably lead to errors.\",\n                              options=[\"y\", \"n\"],\n                              one_line_options=True,\n                              default=\"n\")\n                response = q.ask()\n                if response == \"n\":\n                    return\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Convert to TFLM\":\n            page += \"Converting TFLite models to TFLM\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = tflm_converter.main()\n\n            n_targets = 0\n            n_success = 0\n            failures = []\n            for target in output:\n                for flavor in target[\"flavors\"]:\n                    n_targets += 1\n                    if flavor[\"result\"] == \"success\":\n                        n_success += 1\n                    else:\n                        report = \"Model: {}\\nFlavor: {}\\n\\n\".format(target[\"dir\"], flavor[\"flavor\"])\n                        report += \"Traceback:\\n{}\".format(flavor[\"traceback\"])\n\n                        report_name = os.path.basename(target[\"dir\"])\n                        report_name += \"_\" + flavor[\"flavor\"]\n                        report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                        report_path = os.path.join(save_dir, \"errors/Convert to TFLM\", \"{}.txt\".format(report_name))\n                        os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                        with open(report_path, \"w\") as file:\n                            file.write(report)\n\n                        failures.append({\n                            \"dir\": target[\"dir\"],\n                            \"flavor\": flavor[\"flavor\"],\n                            \"error\": flavor[\"error\"],\n                            \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                        })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {} &gt;&gt; {}\\n\".format(failure[\"dir\"], failure[\"flavor\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n            if n_success == 0 and i &lt; len(modules) - 1:\n                q = _Question(\"No model has survived. Do you want to continue\",\n                              description=\"If the following modules are dependent on the output of this module, this will probably lead to errors.\",\n                              options=[\"y\", \"n\"],\n                              one_line_options=True,\n                              default=\"n\")\n                response = q.ask()\n                if response == \"n\":\n                    return\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Convert to Edge Impulse\":\n            page += \"Converting TFLite models to Edge Impulse\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = ei_converter.main()\n\n            n_targets = 0\n            n_success = 0\n            failures = []\n            for target in output:\n                for flavor in target[\"flavors\"]:\n                    n_targets += 1\n                    if flavor[\"result\"] == \"success\":\n                        n_success += 1\n                    else:\n                        report = \"Model: {}\\nFlavor: {}\\n\\n\".format(target[\"dir\"], flavor[\"flavor\"])\n                        report += \"Traceback:\\n{}\".format(flavor[\"traceback\"])\n\n                        report_name = os.path.basename(target[\"dir\"])\n                        report_name += \"_\" + flavor[\"flavor\"]\n                        report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                        report_path = os.path.join(save_dir, \"errors/Convert to Edge Impulse\", \"{}.txt\".format(report_name))\n                        os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                        with open(report_path, \"w\") as file:\n                            file.write(report)\n\n                        failures.append({\n                            \"dir\": target[\"dir\"],\n                            \"flavor\": flavor[\"flavor\"],\n                            \"error\": flavor[\"error\"],\n                            \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                        })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {} &gt;&gt; {}\\n\".format(failure[\"dir\"], failure[\"flavor\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n            if n_success == 0 and i &lt; len(modules) - 1:\n                q = _Question(\"No model has survived. Do you want to continue\",\n                              description=\"If the following modules are dependent on the output of this module, this will probably lead to errors.\",\n                              options=[\"y\", \"n\"],\n                              one_line_options=True,\n                              default=\"n\")\n                response = q.ask()\n                if response == \"n\":\n                    return\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Convert to eAI Translator\":\n            page += \"Converting TFLite models to eAI Translator\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = eai_translator_converter.main()\n\n            n_targets = 0\n            n_success = 0\n            failures = []\n            for target in output:\n                for flavor in target[\"flavors\"]:\n                    n_targets += 1\n                    if flavor[\"result\"] == \"success\":\n                        n_success += 1\n                    else:\n                        report = \"Model: {}\\nFlavor: {}\\n\\n\".format(target[\"dir\"], flavor[\"flavor\"])\n                        report += \"Traceback:\\n{}\".format(flavor[\"traceback\"])\n\n                        report_name = os.path.basename(target[\"dir\"])\n                        report_name += \"_\" + flavor[\"flavor\"]\n                        report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                        report_path = os.path.join(save_dir, \"errors/Convert to eAI Translator\", \"{}.txt\".format(report_name))\n                        os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                        with open(report_path, \"w\") as file:\n                            file.write(report)\n\n                        failures.append({\n                            \"dir\": target[\"dir\"],\n                            \"flavor\": flavor[\"flavor\"],\n                            \"error\": flavor[\"error\"],\n                            \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                        })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {} &gt;&gt; {}\\n\".format(failure[\"dir\"], failure[\"flavor\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n            if n_success == 0 and i &lt; len(modules) - 1:\n                q = _Question(\"No model has survived. Do you want to continue\",\n                              description=\"If the following modules are dependent on the output of this module, this will probably lead to errors.\",\n                              options=[\"y\", \"n\"],\n                              one_line_options=True,\n                              default=\"n\")\n                response = q.ask()\n                if response == \"n\":\n                    return\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Test on NUCLEO-L4R5ZI (TFLM)\":\n            page += \"Testing TFLM models on NUCLEO-L4R5ZI\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = automate.main(software_platform=\"TFLM\", hardware_platform=\"NUCLEO-L4R5ZI\", save_path=os.path.join(save_dir, \"TFLM + NUCLEO-L4R5ZI.xlsx\"))\n\n            n_targets = len(output)\n            n_success = 0\n            failures = []\n            for target in output:\n                if target[\"result\"] == \"success\":\n                    n_success += 1\n                else:\n                    report = \"Model: {}\\n\\n\".format(target[\"dir\"])\n                    if \"traceback\" in target:\n                        report += \"Traceback:\\n{}\".format(target[\"traceback\"])\n                    else:\n                        report += \"Error file path: {}\\n\".format(target[\"error_file\"])\n\n                    report_name = target[\"dir\"]\n                    report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                    report_path = os.path.join(save_dir, \"errors/Test on NUCLEO-L4R5ZI (TFLM)\", \"{}.txt\".format(report_name))\n                    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                    with open(report_path, \"w\") as file:\n                        file.write(report)\n\n                    failures.append({\n                        \"dir\": target[\"dir\"],\n                        \"error\": target[\"error\"],\n                        \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                    })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {}\\n\".format(failure[\"dir\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Test on NUCLEO-L4R5ZI (Edge Impulse)\":\n            page += \"Testing Edge Impulse models on NUCLEO-L4R5ZI\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = automate.main(software_platform=\"EI\", hardware_platform=\"NUCLEO-L4R5ZI\", save_path=os.path.join(save_dir, \"Edge Impulse + NUCLEO-L4R5ZI.xlsx\"))\n\n            n_targets = len(output)\n            n_success = 0\n            failures = []\n            for target in output:\n                if target[\"result\"] == \"success\":\n                    n_success += 1\n                else:\n                    report = \"Model: {}\\n\\n\".format(target[\"dir\"])\n                    if \"traceback\" in target:\n                        report += \"Traceback:\\n{}\".format(target[\"traceback\"])\n                    else:\n                        report += \"Error file path: {}\\n\".format(target[\"error_file\"])\n\n                    report_name = target[\"dir\"]\n                    report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                    report_path = os.path.join(save_dir, \"errors/Test on NUCLEO-L4R5ZI (Edge Impulse)\", \"{}.txt\".format(report_name))\n                    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                    with open(report_path, \"w\") as file:\n                        file.write(report)\n\n                    failures.append({\n                        \"dir\": target[\"dir\"],\n                        \"error\": target[\"error\"],\n                        \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                    })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {}\\n\".format(failure[\"dir\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Test on NUCLEO-L4R5ZI (Ekkono)\":\n            page += \"Testing Ekkono models on NUCLEO-L4R5ZI\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = automate.main(software_platform=\"Ekkono\", hardware_platform=\"NUCLEO-L4R5ZI\", save_path=os.path.join(save_dir, \"Ekkono + NUCLEO-L4R5ZI.xlsx\"))\n\n            n_targets = len(output)\n            n_success = 0\n            failures = []\n            for target in output:\n                if target[\"result\"] == \"success\":\n                    n_success += 1\n                else:\n                    report = \"Model: {}\\n\\n\".format(target[\"dir\"])\n                    if \"traceback\" in target:\n                        report += \"Traceback:\\n{}\".format(target[\"traceback\"])\n                    else:\n                        report += \"Error file path: {}\\n\".format(target[\"error_file\"])\n\n                    report_name = target[\"dir\"]\n                    report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                    report_path = os.path.join(save_dir, \"errors/Test on NUCLEO-L4R5ZI (Ekkono)\", \"{}.txt\".format(report_name))\n                    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                    with open(report_path, \"w\") as file:\n                        file.write(report)\n\n                    failures.append({\n                        \"dir\": target[\"dir\"],\n                        \"error\": target[\"error\"],\n                        \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                    })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {}\\n\".format(failure[\"dir\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Test on RenesasRX65N (TFLM)\":\n            page += \"Testing TFLM models on RenesasRX65N\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = automate.main(software_platform=\"TFLM\", hardware_platform=\"RenesasRX65N\", save_path=os.path.join(save_dir, \"TFLM + RenesasRX65N.xlsx\"))\n\n            n_targets = len(output)\n            n_success = 0\n            failures = []\n            for target in output:\n                if target[\"result\"] == \"success\":\n                    n_success += 1\n                else:\n                    report = \"Model: {}\\n\\n\".format(target[\"dir\"])\n                    if \"traceback\" in target:\n                        report += \"Traceback:\\n{}\".format(target[\"traceback\"])\n                    else:\n                        report += \"Error file path: {}\\n\".format(target[\"error_file\"])\n\n                    report_name = target[\"dir\"]\n                    report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                    report_path = os.path.join(save_dir, \"errors/Test on RenesasRX65N (TFLM)\", \"{}.txt\".format(report_name))\n                    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                    with open(report_path, \"w\") as file:\n                        file.write(report)\n\n                    failures.append({\n                        \"dir\": target[\"dir\"],\n                        \"error\": target[\"error\"],\n                        \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                    })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {}\\n\".format(failure[\"dir\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Test on RenesasRX65N (Edge Impulse)\":\n            page += \"Testing Edge Impulse models on RenesasRX65N\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = automate.main(software_platform=\"EI\", hardware_platform=\"RenesasRX65N\", save_path=os.path.join(save_dir, \"Edge Impulse + RenesasRX65N.xlsx\"))\n\n            n_targets = len(output)\n            n_success = 0\n            failures = []\n            for target in output:\n                if target[\"result\"] == \"success\":\n                    n_success += 1\n                else:\n                    report = \"Model: {}\\n\\n\".format(target[\"dir\"])\n                    if \"traceback\" in target:\n                        report += \"Traceback:\\n{}\".format(target[\"traceback\"])\n                    else:\n                        report += \"Error file path: {}\\n\".format(target[\"error_file\"])\n\n                    report_name = target[\"dir\"]\n                    report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                    report_path = os.path.join(save_dir, \"errors/Test on RenesasRX65N (Edge Impulse)\", \"{}.txt\".format(report_name))\n                    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                    with open(report_path, \"w\") as file:\n                        file.write(report)\n\n                    failures.append({\n                        \"dir\": target[\"dir\"],\n                        \"error\": target[\"error\"],\n                        \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                    })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {}\\n\".format(failure[\"dir\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Test on RenesasRX65N (Ekkono)\":\n            page += \"Testing Ekkono models on RenesasRX65N\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = automate.main(software_platform=\"Ekkono\", hardware_platform=\"RenesasRX65N\", save_path=os.path.join(save_dir, \"Ekkono + RenesasRX65N.xlsx\"))\n\n            n_targets = len(output)\n            n_success = 0\n            failures = []\n            for target in output:\n                if target[\"result\"] == \"success\":\n                    n_success += 1\n                else:\n                    report = \"Model: {}\\n\\n\".format(target[\"dir\"])\n                    if \"traceback\" in target:\n                        report += \"Traceback:\\n{}\".format(target[\"traceback\"])\n                    else:\n                        report += \"Error file path: {}\\n\".format(target[\"error_file\"])\n\n                    report_name = target[\"dir\"]\n                    report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                    report_path = os.path.join(save_dir, \"errors/Test on RenesasRX65N (Ekkono)\", \"{}.txt\".format(report_name))\n                    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                    with open(report_path, \"w\") as file:\n                        file.write(report)\n\n                    failures.append({\n                        \"dir\": target[\"dir\"],\n                        \"error\": target[\"error\"],\n                        \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                    })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {}\\n\".format(failure[\"dir\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n        elif module == \"Test on RenesasRX65N (eAI Translator)\":\n            page += \"Testing eAI Translator models on RenesasRX65N\"\n            _clear_console()\n            print(page)\n\n            print(colorama.Style.DIM + \"\\n\\n\\n\" + \"=\"*80 + \"\\n\" + \"Module output:\")\n            output = automate.main(software_platform=\"eAI_Translator\", hardware_platform=\"RenesasRX65N\", save_path=os.path.join(save_dir, \"eAI_Translator + RenesasRX65N.xlsx\"))\n\n            n_targets = len(output)\n            n_success = 0\n            failures = []\n            for target in output:\n                if target[\"result\"] == \"success\":\n                    n_success += 1\n                else:\n                    report = \"Model: {}\\n\\n\".format(target[\"dir\"])\n                    if \"traceback\" in target:\n                        report += \"Traceback:\\n{}\".format(target[\"traceback\"])\n                    else:\n                        report += \"Error file path: {}\\n\".format(target[\"error_file\"])\n\n                    report_name = target[\"dir\"]\n                    report_name = report_name.replace(\"/\", \" - \").replace(\"\\\\\", \" - \")\n                    report_path = os.path.join(save_dir, \"errors/Test on RenesasRX65N (eAI Translator)\", \"{}.txt\".format(report_name))\n                    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n                    with open(report_path, \"w\") as file:\n                        file.write(report)\n\n                    failures.append({\n                        \"dir\": target[\"dir\"],\n                        \"error\": target[\"error\"],\n                        \"report_path\": report_path.replace(\"\\\\\", \"/\")\n                    })\n\n            if n_success == n_targets:\n                page += COLOR_SECONDARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n            else:\n                flawless = False\n                page += COLOR_TERTIARY + \" ({}/{})\\n\".format(n_success, n_targets) + COLOR_RESET\n                page += \"Failed models:\\n\"\n                for failure in failures:\n                    page += \"- Model: {}\\n\".format(failure[\"dir\"])\n                    page += \"  Error: {}\\n\".format(failure[\"error\"])\n                    page += \"  Details: {}\\n\".format(failure[\"report_path\"])\n\n            _clear_console()\n            print(page)\n\n    if flawless:\n        page += \"\\nAll the modules were \" + COLOR_SECONDARY + \"successfully\" + COLOR_RESET + \" executed\\n\"\n    else:\n        page += \"\\nAll the modules are executed. Some of them \" + COLOR_TERTIARY + \"failed\" + COLOR_RESET + \" in this process\\n\"\n\n    if (\"Test on NUCLEO-L4R5ZI (TFLM)\" in modules or\n        \"Test on NUCLEO-L4R5ZI (Edge Impulse)\" in modules or\n        \"Test on NUCLEO-L4R5ZI (Ekkono)\" in modules or\n        \"Test on RenesasRX65N (TFLM)\" in modules or\n        \"Test on RenesasRX65N (Edge Impulse)\" in modules or\n        \"Test on RenesasRX65N (Ekkono)\" in modules or\n        \"Test on RenesasRX65N (eAI Translator)\" in modules):\n        page += \"You can find the results in the {} directory\\n\".format(save_dir)\n    _clear_console()\n    print(page)\n</code></pre>"},{"location":"api/automate/automate/","title":"Automate","text":""},{"location":"api/automate/automate/#edgemark.models.automate.automate","title":"edgemark.models.automate.automate","text":""},{"location":"api/automate/automate/#edgemark.models.automate.automate.main","title":"main","text":"<pre><code>main(cfg_path=config_file_path, **kwargs)\n</code></pre> <p>Compile, upload, and test the models on the hardware device.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_path</code> <code>str</code> <p>The path to the configuration file. The configuration file that this path points to should contain the following keys:     - software_platform (str): The software platform to be tested. It should be one of the following: ['TFLM', 'EI', 'Ekkono', 'eAI_Translator']     - hardware_platform (str): The hardware platform to be tested.     - linkers_dir (str): The directory containing the linker files.     - save_dir (str): The directory to save the benchmarking results.     - benchmark_overall_timeout (float): The overall timeout for reading the benchmark output in seconds.     - benchmark_silence_timeout (float): The silence timeout for reading the benchmark output in seconds.</p> <code>config_file_path</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments to be passed to the configuration file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing the following keys for each target model: - dir (str): The directory of the target model. - result (str): Result of the model generation. It can be either \"success\" or \"failed\". - error (str): Error message in case of failure. - traceback (str): Traceback in case of failure. Either this or 'error_file' will be present. - error_file (str): The path to the error file in case of failure. Either this or 'traceback' will be present.</p> Source code in <code>edgemark/models/automate/automate.py</code> <pre><code>def main(cfg_path=config_file_path, **kwargs):\n    \"\"\"\n    Compile, upload, and test the models on the hardware device.\n\n    Args:\n        cfg_path (str): The path to the configuration file.\n            The configuration file that this path points to should contain the following keys:\n                - software_platform (str): The software platform to be tested. It should be one of the following: ['TFLM', 'EI', 'Ekkono', 'eAI_Translator']\n                - hardware_platform (str): The hardware platform to be tested.\n                - linkers_dir (str): The directory containing the linker files.\n                - save_dir (str): The directory to save the benchmarking results.\n                - benchmark_overall_timeout (float): The overall timeout for reading the benchmark output in seconds.\n                - benchmark_silence_timeout (float): The silence timeout for reading the benchmark output in seconds.\n        **kwargs (dict): Keyword arguments to be passed to the configuration file.\n\n    Returns:\n        list: A list of dictionaries containing the following keys for each target model:\n            - dir (str): The directory of the target model.\n            - result (str): Result of the model generation. It can be either \"success\" or \"failed\".\n            - error (str): Error message in case of failure.\n            - traceback (str): Traceback in case of failure. Either this or 'error_file' will be present.\n            - error_file (str): The path to the error file in case of failure. Either this or 'traceback' will be present.\n    \"\"\"\n    cfg = OmegaConf.load(cfg_path)\n    cfg.update(OmegaConf.create(kwargs))\n\n    assert cfg.software_platform is not None, \"The software platform must be provided.\"\n    assert cfg.software_platform in [\"TFLM\", \"EI\", \"Ekkono\", \"eAI_Translator\"], \"The software platform must be either TFLM, EI, Ekkono, or eAI_Translator.\"\n    assert cfg.hardware_platform is not None, \"The hardware platform must be provided.\"\n\n    spec = importlib.util.spec_from_file_location(\"imported_module\", cfg.hardware_platform_path)\n    imported_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(imported_module)\n    hardware = imported_module.Hardware(cfg.software_platform)\n\n    if cfg.software_platform == \"TFLM\":\n        targets = OmegaConf.load(os.path.join(cfg.linkers_dir, \"tflm_converted_models_list.yaml\"))\n    elif cfg.software_platform == \"EI\":\n        targets = OmegaConf.load(os.path.join(cfg.linkers_dir, \"ei_converted_models_list.yaml\"))\n    elif cfg.software_platform == \"Ekkono\":\n        targets = OmegaConf.load(os.path.join(cfg.linkers_dir, \"ekkono_converted_models_list.yaml\"))\n    elif cfg.software_platform == \"eAI_Translator\":\n        targets = OmegaConf.load(os.path.join(cfg.linkers_dir, \"translator_converted_models_list.yaml\"))\n\n        # check if the user has put the eAI_Translator files in the target directories\n        eAI_Translator_dirs_exist = True\n        for target_dir in targets:\n            if not os.path.exists(os.path.join(target_dir, \"Translator\")):\n                eAI_Translator_dirs_exist = False\n                break\n\n        if not eAI_Translator_dirs_exist:\n            print(\"Please use the Renesas eAI_Translator and convert TFLite models to eAI_Translator files.\")\n            print(\"You can skip a model by creating an empty 'Translator' folder in the target directory.\")\n\n        while not eAI_Translator_dirs_exist:\n            eAI_Translator_dirs_exist = True\n            for target_dir in targets:\n                if not os.path.exists(os.path.join(target_dir, \"Translator\")):\n                    eAI_Translator_dirs_exist = False\n                    print(\"\")\n                    print(\"TFLite model should be found in: {}\".format(target_dir.replace(\"eAI_Translator\", \"tflite\")))\n                    print(\"eAI_Translator model should be placed in: {}\".format(target_dir))\n\n            if not eAI_Translator_dirs_exist:\n                print(\"\")\n                input(\"Press Enter when you have put the eAI_Translator files in the target directories ...\")\n\n        for target_dir in targets:\n            if os.path.exists(os.path.join(target_dir, \"Translator\")) and not os.listdir(os.path.join(target_dir, \"Translator\")):\n                targets.remove(target_dir)\n\n    if cfg.software_platform == \"TFLM\":\n        replacing_items = [\"model.h\", \"model.cpp\", \"data.h\", \"data.cpp\"]\n    elif cfg.software_platform == \"EI\":\n        replacing_items = [\"model\", \"data.h\", \"data.cpp\"]\n    elif cfg.software_platform == \"Ekkono\":\n        replacing_items = [\"model.h\", \"model.c\", \"data.h\", \"data.c\"]\n    elif cfg.software_platform == \"eAI_Translator\":\n        replacing_items = [\"Translator\", \"data.h\", \"data.c\"]\n\n    output = [{\"dir\": target_dir} for target_dir in targets]\n\n    benchmarking_result = []\n    for i, target_dir in enumerate(targets):\n        try:\n            target_dir = target_dir.replace(\"\\\\\", \"/\")\n            save_dir = os.path.join(target_dir, \"benchmark_result\", cfg.software_platform + \" + \" + cfg.hardware_platform)\n\n            title = \"Testing the model in {} ({}/{})\".format(target_dir, i+1, len(targets))\n            print(\"\\n\")\n            print(\"=\"*110)\n            print(\"-\"*((110-len(title)-2)//2), end=\" \")\n            print(title, end=\" \")\n            print(\"-\"*((110-len(title)-2)//2))\n            print(\"=\"*110)\n\n            print(\"Placing the model's files/folders ...\", end=\" \", flush=True)\n            destination_dir = hardware.get_model_dir()\n            _placer(target_dir, destination_dir, replacing_items)\n\n            if cfg.software_platform == \"EI\":\n                if cfg.hardware_platform == \"NUCLEO-L4R5ZI\":\n                    # remove all folders inside \"{destination_dir}/model/edge-impulse-sdk/porting\" except \"stm32-cubeai\"\n                    for subdir in next(os.walk(os.path.join(destination_dir, \"model/edge-impulse-sdk/porting\")))[1]:\n                        if subdir not in \"stm32-cubeai\":\n                            # shutil.rmtree(os.path.join(destination_dir, \"model/edge-impulse-sdk/porting\", subdir))\n                            _delete_files_in_dir(os.path.join(destination_dir, \"model/edge-impulse-sdk/porting\", subdir))\n                elif cfg.hardware_platform == \"RenesasRX65N\":\n                    # remove all folders inside \"{destination_dir}/model/edge-impulse-sdk/porting\"\n                    for subdir in next(os.walk(os.path.join(destination_dir, \"model/edge-impulse-sdk/porting\")))[1]:\n                        # shutil.rmtree(os.path.join(destination_dir, \"model/edge-impulse-sdk/porting\", subdir))\n                        _delete_files_in_dir(os.path.join(destination_dir, \"model/edge-impulse-sdk/porting\", subdir))\n                    _placer(os.path.dirname(cfg.EI_general_porting_dir), os.path.join(destination_dir, \"model/edge-impulse-sdk/porting\"), [\"general\"])\n\n            print(\"Done\")\n\n            # find the best arena_size for the TFLM model. Although it's not general, we'll do it for TFLM :)\n            if cfg.arena_finder and cfg.software_platform == \"TFLM\":\n                print(\"Finding the best arena size ...\")\n\n                model_header_path = os.path.join(destination_dir, \"model.h\")\n\n                original_arena_size = None\n                with open(model_header_path, \"r\") as f:\n                    pattern = r\"#define ARENA_SIZE (\\d+)\"\n                    matches = re.findall(pattern, f.read())\n                    if matches:\n                        original_arena_size = int(matches[0])\n\n                if original_arena_size is not None:\n                    arena_size = original_arena_size - 10240 + 2048     # reducing 10kB that was added for safety and adding 2kB have a bit of room for breath\n                else:\n                    arena_size = 4096\n\n                if arena_size &lt; 16384:\n                    search_resolution = 512\n                elif arena_size &lt; 65536:\n                    search_resolution = 1024\n                else:\n                    search_resolution = 2048\n\n                recommender = _arena_size_recommender(arena_size, search_resolution)\n\n                founded_arena = None\n                while True:\n                    arena_size, recommender_status = recommender.recommend()\n\n                    if recommender_status == 1:\n                        print(\"The best arena size is found to be: {}\".format(arena_size))\n                        founded_arena = arena_size\n                        _arena_placer(model_header_path, arena_size)\n                        break\n\n                    if recommender_status == -1:\n                        print(\"The best arena size cannot be found\")\n                        _arena_placer(model_header_path, original_arena_size)\n                        break\n\n                    print(\"Building the project with arena size of {} ...\".format(arena_size), end=\" \", flush=True)\n                    _arena_placer(model_header_path, arena_size)\n                    try:\n                        text_size, data_size, bss_size = hardware.build_project()\n                        print(\"Done\")\n                    except hardware.RAMExceededError:\n                        print(\"Failed\")\n                        recommender.update(arena_size, -1)\n                        continue\n                    except Exception:\n                        print(\"Failed\")\n                        print(\"Unknown build error!\")\n                        _arena_placer(model_header_path, original_arena_size)\n                        break\n\n                    print(\"Uploading the program ...\", end=\" \", flush=True)\n                    try:\n                        hardware.upload_program()\n                        print(\"Done\")\n                    except Exception:\n                        print(\"Failed\")\n                        _arena_placer(model_header_path, original_arena_size)\n                        break\n\n                    print(\"Reading the output ...\", end=\" \", flush=True)\n\n                    try:\n                        hardware.read_output(overall_timeout=cfg.benchmark_overall_timeout, silence_timeout=cfg.benchmark_silence_timeout, keyword=\"Benchmark end\")\n                        recommender.update(arena_size, 0)\n                        print(\"Done\")\n                    except hardware.BoardNotFoundError:\n                        print(\"Failed\")\n                        print(\"Board not found!\")\n                        _arena_placer(model_header_path, original_arena_size)\n                        break\n                    except TimeoutError as e:\n                        if \"missing\" in str(e) or \"Too many buffers\" in str(e):\n                            print(\"Failed\")\n                            recommender.update(arena_size, 1)\n                            continue\n                        else:\n                            print(\"Failed\")\n                            print(\"Receiving output timeout!\")\n                            _arena_placer(model_header_path, original_arena_size)\n                            break\n                    except Exception:\n                        print(\"Failed\")\n                        print(\"Unknown error!\")\n                        _arena_placer(model_header_path, original_arena_size)\n                        break\n                print(\"\")\n\n            print(\"Building the project ...\", end=\" \", flush=True)\n            try:\n                text_size, data_size, bss_size = hardware.build_project()\n                print(\"Done\")\n\n            except hardware.RAMExceededError as e:\n                print(\"Failed\")\n                shutil.rmtree(save_dir, ignore_errors=True)\n                os.makedirs(save_dir, exist_ok=True)\n                with open(os.path.join(save_dir, \"build_error.txt\"), \"w\") as f:\n                    f.write(str(e))\n                output[i][\"result\"] = \"failed\"\n                output[i][\"error\"] = \"RAM size exceeded\"\n                output[i][\"error_file\"] = os.path.join(save_dir, \"build_error.txt\")\n                continue\n\n            except hardware.FlashExceededError as e:\n                print(\"Failed\")\n                shutil.rmtree(save_dir, ignore_errors=True)\n                os.makedirs(save_dir, exist_ok=True)\n                with open(os.path.join(save_dir, \"build_error.txt\"), \"w\") as f:\n                    f.write(str(e))\n                output[i][\"result\"] = \"failed\"\n                output[i][\"error\"] = \"Flash size exceeded\"\n                output[i][\"error_file\"] = os.path.join(save_dir, \"build_error.txt\")\n                continue\n\n            except Exception as e:\n                print(\"Failed\")\n                shutil.rmtree(save_dir, ignore_errors=True)\n                os.makedirs(save_dir, exist_ok=True)\n                with open(os.path.join(save_dir, \"build_error.txt\"), \"w\") as f:\n                    f.write(str(e))\n                output[i][\"result\"] = \"failed\"\n                output[i][\"error\"] = \"Build failed\"\n                output[i][\"error_file\"] = os.path.join(save_dir, \"build_error.txt\")\n                continue\n\n            result = {\"text_size\": text_size, \"data_size\": data_size, \"bss_size\": bss_size}\n\n            if cfg.software_platform == \"TFLM\":\n                result.update({\"arena_finder\": cfg.arena_finder})\n                if cfg.arena_finder:\n                    result.update({\"arena_resolution\": search_resolution, \"founded_arena\": founded_arena})\n\n            print(\"Uploading the program ...\", end=\" \", flush=True)\n            try:\n                hardware.upload_program()\n                print(\"Done\")\n            except Exception as e:\n                print(\"Failed\")\n                shutil.rmtree(save_dir, ignore_errors=True)\n                os.makedirs(save_dir, exist_ok=True)\n                with open(os.path.join(save_dir, \"upload_error.txt\"), \"w\") as f:\n                    f.write(str(e))\n                output[i][\"result\"] = \"failed\"\n                output[i][\"error\"] = \"Upload failed\"\n                output[i][\"error_file\"] = os.path.join(save_dir, \"upload_error.txt\")\n                continue\n\n            print(\"Reading the output ...\", end=\" \", flush=True)\n\n            try:\n                benchmark_output = hardware.read_output(overall_timeout=cfg.benchmark_overall_timeout, silence_timeout=cfg.benchmark_silence_timeout, keyword=\"Benchmark end\")\n                print(\"Done\")\n\n            except hardware.BoardNotFoundError as e:\n                print(\"Failed\")\n                shutil.rmtree(save_dir, ignore_errors=True)\n                os.makedirs(save_dir, exist_ok=True)\n                with open(os.path.join(save_dir, \"connection_error.txt\"), \"w\") as f:\n                    f.write(str(e))\n                output[i][\"result\"] = \"failed\"\n                output[i][\"error\"] = \"Board not found\"\n                output[i][\"error_file\"] = os.path.join(save_dir, \"connection_error.txt\")\n                continue\n\n            except TimeoutError as e:\n                print(\"Failed\")\n                shutil.rmtree(save_dir, ignore_errors=True)\n                os.makedirs(save_dir, exist_ok=True)\n                with open(os.path.join(save_dir, \"timeout_error.txt\"), \"w\") as f:\n                    f.write(str(e))\n                output[i][\"result\"] = \"failed\"\n                if \"missing\" in str(e) or \"Too many buffers\" in str(e):\n                    output[i][\"error\"] = \"Arena size is too small\"\n                else:\n                    output[i][\"error\"] = \"Receiving output timeout\"\n                output[i][\"error_file\"] = os.path.join(save_dir, \"timeout_error.txt\")\n                continue\n\n            except Exception as e:\n                print(\"Failed\")\n                shutil.rmtree(save_dir, ignore_errors=True)\n                os.makedirs(save_dir, exist_ok=True)\n                with open(os.path.join(save_dir, \"benchmark_output_error.txt\"), \"w\") as f:\n                    f.write(str(e))\n                output[i][\"result\"] = \"failed\"\n                output[i][\"error\"] = \"Benchmark output error\"\n                output[i][\"error_file\"] = os.path.join(save_dir, \"benchmark_output_error.txt\")\n                continue\n\n            result.update({\"serial_output\": benchmark_output})\n\n            n_timing_tests, avg_ms, std_ms, avg_ticks, std_ticks = find_exe_time(benchmark_output)\n            result.update({\n                \"n_timing_tests\": n_timing_tests,\n                \"avg_ms\": float(avg_ms) if avg_ms is not None else None,\n                \"std_ms\": float(std_ms) if std_ms is not None else None,\n                \"avg_ticks\": float(avg_ticks) if avg_ticks is not None else None,\n                \"std_ticks\": float(std_ticks) if std_ticks is not None else None\n            })\n\n            n_accuracy_tests, avg_mae, std_mae = find_prediction_mae(benchmark_output)\n            result.update({\n                \"n_accuracy_tests\": n_accuracy_tests,\n                \"avg_mae\": float(avg_mae) if avg_mae is not None else None,\n                \"std_mae\": float(std_mae) if std_mae is not None else None\n            })\n\n            benchmarking_element = result.copy()\n            benchmarking_element.update({\n                \"model_name\": _find_model_name(target_dir),\n                \"model_type\": _find_model_type(target_dir),\n                \"model_directory\": target_dir\n            })\n            benchmarking_result.append(benchmarking_element)\n\n            # 'data' section is taken by our samples data and platforms don't affect it\n            # (TFLM and Ekkono don't, but EI will slightly affect it which we ignore).\n            # So, we can safely remove it from Flash and RAM sizes.\n            print(\"Benchmarking result:\")\n            print(\"Flash size: {} bytes\".format(text_size))\n            print(\"RAM size: {} bytes\".format(bss_size))\n\n            if n_timing_tests &gt; 0:\n                print(\"n_timing_tests: {}\".format(n_timing_tests))\n                print(\"avg_ms: {} ms\".format(avg_ms))\n                print(\"std_ms: {} ms\".format(std_ms))\n                print(\"avg_ticks: {}\".format(avg_ticks))\n                print(\"std_ticks: {}\".format(std_ticks))\n            else:\n                print(\"No timing information was found\")\n\n            if n_accuracy_tests &gt; 0:\n                print(\"n_accuracy_tests: {}\".format(n_accuracy_tests))\n                print(\"avg_mae: {}\".format(avg_mae))\n                print(\"std_mae: {}\".format(std_mae))\n            else:\n                print(\"No accuracy information was found\")\n\n            shutil.rmtree(save_dir, ignore_errors=True)\n            os.makedirs(save_dir, exist_ok=True)\n            with open(os.path.join(save_dir, \"result.yaml\"), \"w\") as f:\n                yaml.dump(result, f, indent=4, sort_keys=False)\n\n            output[i][\"result\"] = \"success\"\n\n        except Exception as e:\n            output[i][\"result\"] = \"failed\"\n            output[i][\"error\"] = type(e).__name__\n            output[i][\"traceback\"] = traceback.format_exc()\n            print(\"Error:\")\n            print(traceback.format_exc())\n\n    _save_to_excel(benchmarking_result, cfg.software_platform, cfg.save_path)\n\n    test_name = os.path.basename(cfg.save_path)\n    test_name = os.path.splitext(test_name)[0]\n    figures_save_dir = os.path.join(os.path.dirname(cfg.save_path), \"figures\", test_name)\n    result_plotter(cfg.save_path, figures_save_dir)\n\n    return output\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.investigator","title":"edgemark.models.automate.investigator","text":""},{"location":"api/automate/automate/#edgemark.models.automate.investigator.find_exe_time","title":"find_exe_time","text":"<pre><code>find_exe_time(text)\n</code></pre> <p>Find the execution time from the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to search.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of (n_tests, avg_ms, std_ms, avg_ticks, std_ticks).</p> Source code in <code>edgemark/models/automate/investigator.py</code> <pre><code>def find_exe_time(text):\n    \"\"\"\n    Find the execution time from the text.\n\n    Args:\n        text (str): The text to search.\n\n    Returns:\n        tuple: A tuple of (n_tests, avg_ms, std_ms, avg_ticks, std_ticks).\n    \"\"\"\n    ms = []\n    ticks = []\n\n    pattern = r\"Execution time: ([\\d.]+) ms \\((\\d+) ticks\\)\"\n    matches = re.findall(pattern, text)\n\n    for match in matches:\n        ms.append(float(match[0]))\n        ticks.append(int(match[1]))\n\n    pattern = r\"Execution time: (\\d+) ticks\"\n    matches = re.findall(pattern, text)\n\n    for match in matches:\n        ticks.append(int(match))\n\n    ms = np.array(ms)\n    ticks = np.array(ticks)\n\n    if len(ms) &gt; 0:\n        assert len(ticks) == len(ms)\n    n_tests = len(ticks)\n\n    if len(ms) &gt; 0:\n        avg_ms = np.mean(ms)\n        std_ms = np.std(ms)\n    else:\n        avg_ms = None\n        std_ms = None\n\n    if len(ticks) &gt; 0:\n        avg_ticks = np.mean(ticks)\n        std_ticks = np.std(ticks)\n    else:\n        avg_ticks = None\n        std_ticks = None\n\n    return n_tests, avg_ms, std_ms, avg_ticks, std_ticks\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.investigator.find_prediction_mae","title":"find_prediction_mae","text":"<pre><code>find_prediction_mae(text)\n</code></pre> <p>Find the mean absolute error (MAE) from the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to search.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of (n_tests, avg_mae, std_mae).</p> Source code in <code>edgemark/models/automate/investigator.py</code> <pre><code>def find_prediction_mae(text):\n    \"\"\"\n    Find the mean absolute error (MAE) from the text.\n\n    Args:\n        text (str): The text to search.\n\n    Returns:\n        tuple: A tuple of (n_tests, avg_mae, std_mae).\n    \"\"\"\n    maes = []\n\n    y_expected_min = None\n    y_expected_max = None\n    for line in text.split(\"\\n\"):\n        pattern = r\"\\[(-?[\\d.]+), (-?[\\d.]+)\\]\"\n        matches = re.findall(pattern, line)\n\n        y_expected = []\n        y_predicted = []\n        for match in matches:\n            y_expected.append(float(match[0]))\n            y_predicted.append(float(match[1]))\n            if y_expected_min is None or float(match[0]) &lt; y_expected_min:\n                y_expected_min = float(match[0])\n            if y_expected_max is None or float(match[0]) &gt; y_expected_max:\n                y_expected_max = float(match[0])\n\n        if len(y_expected) &gt; 0:\n            maes.append(np.abs(np.array(y_expected) - np.array(y_predicted)))\n\n    maes = np.array(maes)\n    if y_expected_min is not None and y_expected_max is not None and (y_expected_max - y_expected_min) &gt; 0:\n        maes = maes / (y_expected_max - y_expected_min)\n\n    n_tests = len(maes)\n    if len(maes) &gt; 0:\n        avg_mae = np.mean(maes)\n        std_mae = np.std(maes)\n    else:\n        avg_mae = None\n        std_mae = None\n\n    return n_tests, avg_mae, std_mae\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template","title":"edgemark.models.automate.hardware_template","text":"<p>This module contains a template class that other hardware classes should inherit from.</p>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template.HardwareTemplate","title":"HardwareTemplate","text":"<p>This class is a template for hardware classes. In order to create a new hardware class, you should inherit from this class and implement its abstract functions.</p> Source code in <code>edgemark/models/automate/hardware_template.py</code> <pre><code>class HardwareTemplate:\n    \"\"\"\n    This class is a template for hardware classes. In order to create a new hardware class,\n    you should inherit from this class and implement its abstract functions.\n    \"\"\"\n\n    class RAMExceededError(Exception):\n        \"\"\"\n        Exception raised when the hardware's required RAM usage exceeds its size.\n        \"\"\"\n        pass\n\n\n    class FlashExceededError(Exception):\n        \"\"\"\n        Exception raised when the hardware's required flash usage exceeds its size.\n        \"\"\"\n        pass\n\n\n    class BoardNotFoundError(Exception):\n        \"\"\"\n        Exception raised when the board is not found.\n        \"\"\"\n        pass\n\n\n    def __init__(self, software_platform):\n        \"\"\"\n        Initializes the hardware class.\n\n        Args:\n            software_platform (str): The software platform that will be used with this hardware.\n        \"\"\"\n        pass\n\n\n    def get_model_dir(self):\n        \"\"\"\n        Returns the directory where the model files can go.\n\n        Returns:\n            str: The directory where the model files can go.\n        \"\"\"\n        raise NotImplementedError\n\n\n    def build_project(self, clean=False):\n        \"\"\"\n        Builds the project.\n\n        Args:\n            clean (bool): Whether to clean the project before building.\n\n        Returns:\n            tuple: A tuple containing text_size, data_size, and bss_size.\n\n        Raises:\n            HardwareTemplate.RAMExceededError: If the hardware's required RAM usage exceeds its size.\n            Exception: If for other reasons, the project cannot be built.\n        \"\"\"\n        raise NotImplementedError\n\n\n    def upload_program(self):\n        \"\"\"\n        Uploads the program to the hardware.\n\n        Raises:\n            Exception: If the program cannot be uploaded.\n        \"\"\"\n        raise NotImplementedError\n\n\n    @staticmethod\n    def read_output(overall_timeout, silence_timeout, keyword=None, verbose=False):\n        \"\"\"\n        Reads the output from the hardware.\n\n        Args:\n            overall_timeout (int): The overall timeout for reading.\n            silence_timeout (int): The silence timeout for reading.\n            keyword (str): The keyword to stop reading at.\n            verbose (bool): Whether to print the output as it is read.\n\n        Returns:\n            str: The output read from the hardware.\n\n        Raises:\n            HardwareTemplate.BoardNotFoundError: If the board is not found.\n            TimeoutError: If reading times out.\n            Exception: If for other reasons, the output is not complete.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template.HardwareTemplate.BoardNotFoundError","title":"BoardNotFoundError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the board is not found.</p> Source code in <code>edgemark/models/automate/hardware_template.py</code> <pre><code>class BoardNotFoundError(Exception):\n    \"\"\"\n    Exception raised when the board is not found.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template.HardwareTemplate.FlashExceededError","title":"FlashExceededError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the hardware's required flash usage exceeds its size.</p> Source code in <code>edgemark/models/automate/hardware_template.py</code> <pre><code>class FlashExceededError(Exception):\n    \"\"\"\n    Exception raised when the hardware's required flash usage exceeds its size.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template.HardwareTemplate.RAMExceededError","title":"RAMExceededError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the hardware's required RAM usage exceeds its size.</p> Source code in <code>edgemark/models/automate/hardware_template.py</code> <pre><code>class RAMExceededError(Exception):\n    \"\"\"\n    Exception raised when the hardware's required RAM usage exceeds its size.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template.HardwareTemplate.__init__","title":"__init__","text":"<pre><code>__init__(software_platform)\n</code></pre> <p>Initializes the hardware class.</p> <p>Parameters:</p> Name Type Description Default <code>software_platform</code> <code>str</code> <p>The software platform that will be used with this hardware.</p> required Source code in <code>edgemark/models/automate/hardware_template.py</code> <pre><code>def __init__(self, software_platform):\n    \"\"\"\n    Initializes the hardware class.\n\n    Args:\n        software_platform (str): The software platform that will be used with this hardware.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template.HardwareTemplate.build_project","title":"build_project","text":"<pre><code>build_project(clean=False)\n</code></pre> <p>Builds the project.</p> <p>Parameters:</p> Name Type Description Default <code>clean</code> <code>bool</code> <p>Whether to clean the project before building.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing text_size, data_size, and bss_size.</p> <p>Raises:</p> Type Description <code>RAMExceededError</code> <p>If the hardware's required RAM usage exceeds its size.</p> <code>Exception</code> <p>If for other reasons, the project cannot be built.</p> Source code in <code>edgemark/models/automate/hardware_template.py</code> <pre><code>def build_project(self, clean=False):\n    \"\"\"\n    Builds the project.\n\n    Args:\n        clean (bool): Whether to clean the project before building.\n\n    Returns:\n        tuple: A tuple containing text_size, data_size, and bss_size.\n\n    Raises:\n        HardwareTemplate.RAMExceededError: If the hardware's required RAM usage exceeds its size.\n        Exception: If for other reasons, the project cannot be built.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template.HardwareTemplate.get_model_dir","title":"get_model_dir","text":"<pre><code>get_model_dir()\n</code></pre> <p>Returns the directory where the model files can go.</p> <p>Returns:</p> Type Description <code>str</code> <p>The directory where the model files can go.</p> Source code in <code>edgemark/models/automate/hardware_template.py</code> <pre><code>def get_model_dir(self):\n    \"\"\"\n    Returns the directory where the model files can go.\n\n    Returns:\n        str: The directory where the model files can go.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template.HardwareTemplate.read_output","title":"read_output  <code>staticmethod</code>","text":"<pre><code>read_output(overall_timeout, silence_timeout, keyword=None, verbose=False)\n</code></pre> <p>Reads the output from the hardware.</p> <p>Parameters:</p> Name Type Description Default <code>overall_timeout</code> <code>int</code> <p>The overall timeout for reading.</p> required <code>silence_timeout</code> <code>int</code> <p>The silence timeout for reading.</p> required <code>keyword</code> <code>str</code> <p>The keyword to stop reading at.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the output as it is read.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The output read from the hardware.</p> <p>Raises:</p> Type Description <code>BoardNotFoundError</code> <p>If the board is not found.</p> <code>TimeoutError</code> <p>If reading times out.</p> <code>Exception</code> <p>If for other reasons, the output is not complete.</p> Source code in <code>edgemark/models/automate/hardware_template.py</code> <pre><code>@staticmethod\ndef read_output(overall_timeout, silence_timeout, keyword=None, verbose=False):\n    \"\"\"\n    Reads the output from the hardware.\n\n    Args:\n        overall_timeout (int): The overall timeout for reading.\n        silence_timeout (int): The silence timeout for reading.\n        keyword (str): The keyword to stop reading at.\n        verbose (bool): Whether to print the output as it is read.\n\n    Returns:\n        str: The output read from the hardware.\n\n    Raises:\n        HardwareTemplate.BoardNotFoundError: If the board is not found.\n        TimeoutError: If reading times out.\n        Exception: If for other reasons, the output is not complete.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/automate/automate/#edgemark.models.automate.hardware_template.HardwareTemplate.upload_program","title":"upload_program","text":"<pre><code>upload_program()\n</code></pre> <p>Uploads the program to the hardware.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the program cannot be uploaded.</p> Source code in <code>edgemark/models/automate/hardware_template.py</code> <pre><code>def upload_program(self):\n    \"\"\"\n    Uploads the program to the hardware.\n\n    Raises:\n        Exception: If the program cannot be uploaded.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/datasets/data_template/","title":"Datasets","text":""},{"location":"api/datasets/data_template/#edgemark.models.datasets.data_template","title":"edgemark.models.datasets.data_template","text":"<p>This module contains a template class that other datasets should inherit from.</p>"},{"location":"api/datasets/data_template/#edgemark.models.datasets.data_template.DatasetSupervisorTemplate","title":"DatasetSupervisorTemplate","text":"<p>This class is a template for datasets. In order to create a new dataset, you should inherit from this class and implement its abstract functions.</p> Source code in <code>edgemark/models/datasets/data_template.py</code> <pre><code>class DatasetSupervisorTemplate:\n    \"\"\"\n    This class is a template for datasets. In order to create a new dataset, you should\n    inherit from this class and implement its abstract functions.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes the class by setting the following attributes:\n            self.train_x (numpy.ndarray): The training data.\n            self.train_y (numpy.ndarray): The training labels.\n            self.test_x (numpy.ndarray): The test data.\n            self.test_y (numpy.ndarray): The test labels.\n\n        For FC and CNN models, the following attributes should also be set:\n            self.feature_shape (tuple): The shape of the features.\n            self.num_labels (int): The number of labels.\n            self.output_activation (str): The name of the activation function of the output layer.\n            self.loss_function (str | tf.keras.losses.Loss): The name of the loss function or the loss function itself.\n            self.metrics (list): The list of metrics.\n\n        For RNN models, the following attributes should also be set:\n            self.input_size (int): The size of each element in the sequence.\n            self.output_size (int): The size of the output.\n            self.sequence_length (int): The length of the sequences.\n            self.sequential_output (bool): If True, the output is sequential.\n            self.output_activation (str): The name of the activation function of the output layer.\n            self.loss_function (str | tf.keras.losses.Loss): The name of the loss function or the loss function itself.\n            self.metrics (list): The list of metrics.\n            self.char2index (dict, optional): The dictionary that maps characters to indices. Can be used for text generation.\n            self.index2char (list, optional): The list that maps indices to characters. Can be used for text generation.\n        \"\"\"\n        self.train_x = None\n        self.train_y = None\n        self.test_x = None\n        self.test_y = None\n\n        self.feature_shape = None\n        self.num_labels = None\n        self.output_activation = None\n        self.loss_function = None\n        self.metrics = None\n\n        self.input_size = None\n        self.output_size = None\n        self.sequence_length = None\n        self.sequential_output = None\n        self.output_activation = None\n        self.loss_function = None\n        self.metrics = None\n        self.char2index = None\n        self.index2char = None\n\n\n    def load_dataset(**kwargs):\n        \"\"\"\n        Loads the dataset and returns it in the format ((trainX, trainY), (testX, testY)).\n        The data should be in numpy float32 and shuffled.\n\n        Returns:\n            tuple: ((trainX, trainY), (testX, testY))\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/datasets/data_template/#edgemark.models.datasets.data_template.DatasetSupervisorTemplate.__init__","title":"__init__","text":"<pre><code>__init__(**kwargs)\n</code></pre> Initializes the class by setting the following attributes <p>self.train_x (numpy.ndarray): The training data. self.train_y (numpy.ndarray): The training labels. self.test_x (numpy.ndarray): The test data. self.test_y (numpy.ndarray): The test labels.</p> <p>For FC and CNN models, the following attributes should also be set:     self.feature_shape (tuple): The shape of the features.     self.num_labels (int): The number of labels.     self.output_activation (str): The name of the activation function of the output layer.     self.loss_function (str | tf.keras.losses.Loss): The name of the loss function or the loss function itself.     self.metrics (list): The list of metrics.</p> <p>For RNN models, the following attributes should also be set:     self.input_size (int): The size of each element in the sequence.     self.output_size (int): The size of the output.     self.sequence_length (int): The length of the sequences.     self.sequential_output (bool): If True, the output is sequential.     self.output_activation (str): The name of the activation function of the output layer.     self.loss_function (str | tf.keras.losses.Loss): The name of the loss function or the loss function itself.     self.metrics (list): The list of metrics.     self.char2index (dict, optional): The dictionary that maps characters to indices. Can be used for text generation.     self.index2char (list, optional): The list that maps indices to characters. Can be used for text generation.</p> Source code in <code>edgemark/models/datasets/data_template.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes the class by setting the following attributes:\n        self.train_x (numpy.ndarray): The training data.\n        self.train_y (numpy.ndarray): The training labels.\n        self.test_x (numpy.ndarray): The test data.\n        self.test_y (numpy.ndarray): The test labels.\n\n    For FC and CNN models, the following attributes should also be set:\n        self.feature_shape (tuple): The shape of the features.\n        self.num_labels (int): The number of labels.\n        self.output_activation (str): The name of the activation function of the output layer.\n        self.loss_function (str | tf.keras.losses.Loss): The name of the loss function or the loss function itself.\n        self.metrics (list): The list of metrics.\n\n    For RNN models, the following attributes should also be set:\n        self.input_size (int): The size of each element in the sequence.\n        self.output_size (int): The size of the output.\n        self.sequence_length (int): The length of the sequences.\n        self.sequential_output (bool): If True, the output is sequential.\n        self.output_activation (str): The name of the activation function of the output layer.\n        self.loss_function (str | tf.keras.losses.Loss): The name of the loss function or the loss function itself.\n        self.metrics (list): The list of metrics.\n        self.char2index (dict, optional): The dictionary that maps characters to indices. Can be used for text generation.\n        self.index2char (list, optional): The list that maps indices to characters. Can be used for text generation.\n    \"\"\"\n    self.train_x = None\n    self.train_y = None\n    self.test_x = None\n    self.test_y = None\n\n    self.feature_shape = None\n    self.num_labels = None\n    self.output_activation = None\n    self.loss_function = None\n    self.metrics = None\n\n    self.input_size = None\n    self.output_size = None\n    self.sequence_length = None\n    self.sequential_output = None\n    self.output_activation = None\n    self.loss_function = None\n    self.metrics = None\n    self.char2index = None\n    self.index2char = None\n</code></pre>"},{"location":"api/datasets/data_template/#edgemark.models.datasets.data_template.DatasetSupervisorTemplate.load_dataset","title":"load_dataset","text":"<pre><code>load_dataset(**kwargs)\n</code></pre> <p>Loads the dataset and returns it in the format ((trainX, trainY), (testX, testY)). The data should be in numpy float32 and shuffled.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>((trainX, trainY), (testX, testY))</p> Source code in <code>edgemark/models/datasets/data_template.py</code> <pre><code>def load_dataset(**kwargs):\n    \"\"\"\n    Loads the dataset and returns it in the format ((trainX, trainY), (testX, testY)).\n    The data should be in numpy float32 and shuffled.\n\n    Returns:\n        tuple: ((trainX, trainY), (testX, testY))\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/platforms/EI/","title":"Edge Impulse","text":""},{"location":"api/platforms/EI/#edgemark.models.platforms.EI.EI_converter","title":"edgemark.models.platforms.EI.EI_converter","text":""},{"location":"api/platforms/EI/#edgemark.models.platforms.EI.EI_converter.convert_model","title":"convert_model","text":"<pre><code>convert_model(tflite_model_path, model_type, engine, api_key, project_id, save_dir)\n</code></pre> <p>Convert a TensorFlow Lite model to an Edge Impulse model.</p> <p>Parameters:</p> Name Type Description Default <code>tflite_model_path</code> <code>str</code> <p>The path to the TensorFlow Lite model.</p> required <code>model_type</code> <code>str</code> <p>The model type. Can be 'int8' or 'float32'.</p> required <code>engine</code> <code>str</code> <p>The engine to use. Can be 'tflite' or 'tflite-eon'.</p> required <code>api_key</code> <code>str</code> <p>The API key to use.</p> required <code>project_id</code> <code>str</code> <p>The project ID to use.</p> required <code>save_dir</code> <code>str</code> <p>The directory to save the Edge Impulse zipped C++ code.</p> required Source code in <code>edgemark/models/platforms/EI/EI_converter.py</code> <pre><code>def convert_model(tflite_model_path, model_type, engine, api_key, project_id, save_dir):\n    \"\"\"\n    Convert a TensorFlow Lite model to an Edge Impulse model.\n\n    Args:\n        tflite_model_path (str): The path to the TensorFlow Lite model.\n        model_type (str): The model type. Can be 'int8' or 'float32'.\n        engine (str): The engine to use. Can be 'tflite' or 'tflite-eon'.\n        api_key (str): The API key to use.\n        project_id (str): The project ID to use.\n        save_dir (str): The directory to save the Edge Impulse zipped C++ code.\n    \"\"\"\n    api_endpoint = \"https://studio.edgeimpulse.com/v1/api/{}\".format(project_id)\n\n    # get the output n_dims of the model\n    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n    interpreter.allocate_tensors()\n    output_details = interpreter.get_output_details()\n    assert len(output_details) == 1, \"The model should have only one output tensor\"\n    output_shape = output_details[0][\"shape\"]\n    assert len(output_shape) == 2, \"The output tensor should have 2 dimensions\"\n    n_dims = output_shape[1]\n    if n_dims == 1:\n        model_output_info = {\"modelType\": \"regression\"}\n    else:\n        model_output_info = {\n            \"modelType\": \"classification\",\n            \"labels\": [\"class {}\".format(i+1) for i in range(n_dims)]\n        }\n\n    # step 1: upload the model to Edge Impulse\n    with open(tflite_model_path, \"rb\") as f:\n        model_data = f.read()\n    model_data_base64 = base64.b64encode(model_data).decode(\"utf-8\")\n\n    url = api_endpoint + \"/jobs/deploy-pretrained-model\"\n    payload = {\n        \"modelFileType\": \"tflite\",\n        \"modelInfo\": {\n            \"input\": {\"inputType\": \"other\"},      # even if image, we don't want to have scaling or normalization, so we pretend it's 'other'\n            \"model\": model_output_info\n        },\n        \"modelFileBase64\": model_data_base64,\n        \"deploymentType\": \"zip\",\n        \"engine\": engine,\n        \"deployModelType\": model_type\n    }\n    headers = {\n        \"accept\": \"application/json\",\n        \"content-type\": \"application/json\",\n        \"x-api-key\": api_key\n    }\n\n    response = requests.post(url, json=payload, headers=headers, timeout=10)\n    response.raise_for_status()\n    response = response.json()\n    if response[\"success\"] is not True:\n        raise Exception(\"Failed to upload the model to Edge Impulse. The response is: {}\".format(response))\n    job_id = response[\"id\"]\n\n    # step 2: wait for the job to finish\n    tic = time.time()\n    while time.time() - tic &lt; 120:\n        url = api_endpoint + \"/jobs/{}/status\".format(job_id)\n        headers = {\n            \"accept\": \"application/json\",\n            \"x-api-key\": api_key\n        }\n        response = requests.get(url, headers=headers, timeout=10)\n        response.raise_for_status()\n        response = response.json()\n        if response[\"success\"] is not True:\n            raise Exception(\"Failed to get the job status. The response is: {}\".format(response))\n        if \"finished\" in response[\"job\"]:\n            break\n        time.sleep(5)\n    if response[\"job\"][\"finishedSuccessful\"] is not True:\n        raise Exception(\"The job failed. The response is: {}\".format(response))\n\n    # step 3: download the Edge Impulse zipped C++ code\n    url = api_endpoint + \"/deployment/download?type=zip&amp;modelType={}&amp;engine={}\".format(model_type, engine)\n    headers = {\n        \"accept\": \"application/zip\",\n        \"x-api-key\": api_key\n    }\n    response = requests.get(url, headers=headers, timeout=10)\n    response.raise_for_status()\n\n    os.makedirs(save_dir, exist_ok=True)\n    with open(os.path.join(save_dir, \"model.zip\"), \"wb\") as f:\n        f.write(response.content)\n\n    with zipfile.ZipFile(os.path.join(save_dir, \"model.zip\"), 'r') as zip_ref:\n        zip_ref.extractall(os.path.join(save_dir, \"model\"))\n</code></pre>"},{"location":"api/platforms/EI/#edgemark.models.platforms.EI.EI_converter.create_data_source_files","title":"create_data_source_files","text":"<pre><code>create_data_source_files(eqcheck_data_path, templates_dir, save_dir)\n</code></pre> <p>Create the data source files for the Edge Impulse model.</p> <p>Parameters:</p> Name Type Description Default <code>eqcheck_data_path</code> <code>str</code> <p>The path to the equality check data.</p> required <code>templates_dir</code> <code>str</code> <p>The directory containing the templates.</p> required <code>save_dir</code> <code>str</code> <p>The directory to save the data source files.</p> required Source code in <code>edgemark/models/platforms/EI/EI_converter.py</code> <pre><code>def create_data_source_files(eqcheck_data_path, templates_dir, save_dir):\n    \"\"\"\n    Create the data source files for the Edge Impulse model.\n\n    Args:\n        eqcheck_data_path (str): The path to the equality check data.\n        templates_dir (str): The directory containing the templates.\n        save_dir (str): The directory to save the data source files.\n    \"\"\"\n    def _np_to_c(array):\n        array = np.array(array)\n        array = array.reshape(-1)\n        c_array = \"{\" + \", \".join([str(x) for x in array]) + \"}\"\n        return c_array\n\n    data = np.load(eqcheck_data_path)\n    data_x = data['data_x']\n    data_y = data['data_y_pred']\n    data.close()\n\n    with open(os.path.join(templates_dir, 'data.h'), 'r') as f:\n        h_file = f.read()\n\n    with open(os.path.join(templates_dir, 'data.cpp'), 'r') as f:\n        cpp_file = f.read()\n\n    h_file = h_file.replace(\"{n_samples}\", str(data_x.shape[0]))\n    cpp_file = cpp_file.replace(\"{n_samples}\", str(data_x.shape[0]))\n\n    data_x_size = np.prod(data_x.shape[1:])\n    data_y_size = np.prod(data_y.shape[1:])\n    h_file = h_file.replace(\"{samples_x_size}\", str(data_x_size))\n    h_file = h_file.replace(\"{samples_y_size}\", str(data_y_size))\n    cpp_file = cpp_file.replace(\"{samples_x_size}\", str(data_x_size))\n    cpp_file = cpp_file.replace(\"{samples_y_size}\", str(data_y_size))\n\n    data_x_str = \"\\n\"\n    for i, sample_x in enumerate(data_x):\n        data_x_str += \"\\t\" + _np_to_c(sample_x)\n        if i &lt; len(data_x) - 1:\n            data_x_str += \",\"\n        data_x_str += \"\\n\"\n\n    data_y_str = \"\\n\"\n    for i, sample_y in enumerate(data_y):\n        data_y_str += \"\\t\" + _np_to_c(sample_y)\n        if i &lt; len(data_y) - 1:\n            data_y_str += \",\"\n        data_y_str += \"\\n\"\n\n    cpp_file = cpp_file.replace(\"{samples_x}\", data_x_str)\n    cpp_file = cpp_file.replace(\"{samples_y}\", data_y_str)\n\n    os.makedirs(save_dir, exist_ok=True)\n    with open(os.path.join(save_dir, 'data.h'), 'w') as f:\n        f.write(h_file)\n    with open(os.path.join(save_dir, 'data.cpp'), 'w') as f:\n        f.write(cpp_file)\n</code></pre>"},{"location":"api/platforms/EI/#edgemark.models.platforms.EI.EI_converter.main","title":"main","text":"<pre><code>main(cfg_path=config_file_path, **kwargs)\n</code></pre> <p>Convert TensorFlow Lite models to Edge Impulse models.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_path</code> <code>str</code> <p>The path to the configuration file. The configuration file that this path points to should contain the following keys:     - model_base_dir (str): A placeholder for the model base directory. This will be populated by the target directory.     - tflite_conversion_type (str): A placeholder for the TFLite conversion type. This will be populated by the conversion type.     - linkers_dir (str): Path to the directory where the tflite converted models list is loaded from and the EI converted models list will be saved.     - user_config (str): Path to the user configuration file. This file should contain the following keys:         - ei_api_key (str): The Edge Impulse API key.         - ei_project_id (str): The Edge Impulse project ID.     - tflite_model_path (str): The path to the TensorFlow Lite model.     - eqcheck_data_path (str): The path to the equality check data.     - data_templates_dir (str): Path to the directory containing the data templates.     - ei_save_dir (str): The directory to save the Edge Impulse files.</p> <code>config_file_path</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments to be passed to the configuration file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing the following keys for each target model: - dir (str): The directory of the target model. - flavors (list): A list of dictionaries containing the following keys for each optimization:     - flavor (str): The optimization flavor.     - result (str): The result of the conversion. It can be either \"success\" or \"failed\".     - error (str): The error message in case of failure.     - traceback (str): The traceback in case of failure.</p> Source code in <code>edgemark/models/platforms/EI/EI_converter.py</code> <pre><code>def main(cfg_path=config_file_path, **kwargs):\n    \"\"\"\n    Convert TensorFlow Lite models to Edge Impulse models.\n\n    Args:\n        cfg_path (str): The path to the configuration file.\n            The configuration file that this path points to should contain the following keys:\n                - model_base_dir (str): A placeholder for the model base directory. This will be populated by the target directory.\n                - tflite_conversion_type (str): A placeholder for the TFLite conversion type. This will be populated by the conversion type.\n                - linkers_dir (str): Path to the directory where the tflite converted models list is loaded from and the EI converted models list will be saved.\n                - user_config (str): Path to the user configuration file. This file should contain the following keys:\n                    - ei_api_key (str): The Edge Impulse API key.\n                    - ei_project_id (str): The Edge Impulse project ID.\n                - tflite_model_path (str): The path to the TensorFlow Lite model.\n                - eqcheck_data_path (str): The path to the equality check data.\n                - data_templates_dir (str): Path to the directory containing the data templates.\n                - ei_save_dir (str): The directory to save the Edge Impulse files.\n        **kwargs (dict): Keyword arguments to be passed to the configuration file.\n\n    Returns:\n        list: A list of dictionaries containing the following keys for each target model:\n            - dir (str): The directory of the target model.\n            - flavors (list): A list of dictionaries containing the following keys for each optimization:\n                - flavor (str): The optimization flavor.\n                - result (str): The result of the conversion. It can be either \"success\" or \"failed\".\n                - error (str): The error message in case of failure.\n                - traceback (str): The traceback in case of failure.\n    \"\"\"\n    cfg = OmegaConf.load(cfg_path, **kwargs)\n    cfg.update(OmegaConf.create(kwargs))\n    if os.path.exists(cfg.user_config):\n            cfg = OmegaConf.merge(cfg, OmegaConf.load(cfg.user_config))\n\n    targets = OmegaConf.load(os.path.join(cfg.linkers_dir, \"tflite_converted_models_list.yaml\"))\n\n    output = [{\"dir\": base_model[\"model_base_dir\"], \"flavors\": []} for base_model in targets]\n\n    converted_models_list = []\n    for i, base_model in enumerate(targets):\n        print(\"\\nConverting the model in: {} ({}/{})\".format(base_model[\"model_base_dir\"], i+1, len(targets)))\n\n        for model_flavor in base_model[\"conversions\"]:\n            if model_flavor in [\"basic\", \"q_full_int_only\"]:\n                txt = \"Model type: {} ...\".format(model_flavor)\n                txt += \" \" * (32 - len(txt))\n                print(txt, end=\" \", flush=True)\n                cfg.model_base_dir = base_model[\"model_base_dir\"]\n                cfg.tflite_conversion_type = model_flavor\n                model_type = \"int8\" if model_flavor == \"q_full_int_only\" else \"float32\"\n\n                try:\n                    convert_model(cfg.tflite_model_path, model_type, \"tflite-eon\", cfg.ei_api_key, cfg.ei_project_id, cfg.ei_save_dir)\n                    create_data_source_files(cfg.eqcheck_data_path, cfg.data_templates_dir, cfg.ei_save_dir)\n                    if os.path.exists(os.path.join(cfg.ei_save_dir, 'exception.txt')):\n                        os.remove(os.path.join(cfg.ei_save_dir, 'exception.txt'))\n                    converted_models_list.append(cfg.ei_save_dir)\n                    output[i][\"flavors\"].append({\n                        \"flavor\": model_flavor,\n                        \"result\": \"success\"\n                    })\n                    print(\"Done\")\n                except Exception as e:\n                    error_message = traceback.format_exc()\n                    _save_ei_exception(error_message, cfg.ei_save_dir, delete_assets=True)\n                    output[i][\"flavors\"].append({\n                        \"flavor\": model_flavor,\n                        \"result\": \"failed\",\n                        \"error\": type(e).__name__,\n                        \"traceback\": traceback.format_exc()\n                    })\n                    print(\"Failed\")\n\n    os.makedirs(cfg.linkers_dir, exist_ok=True)\n    with open(os.path.join(cfg.linkers_dir, 'ei_converted_models_list.yaml'), 'w') as f:\n        yaml.dump(converted_models_list, f, indent=4, sort_keys=False)\n\n    return output\n</code></pre>"},{"location":"api/platforms/eAI_Translator/","title":"eAI Translator","text":""},{"location":"api/platforms/eAI_Translator/#edgemark.models.platforms.eAI_Translator.eAI_Translator_converter","title":"edgemark.models.platforms.eAI_Translator.eAI_Translator_converter","text":""},{"location":"api/platforms/eAI_Translator/#edgemark.models.platforms.eAI_Translator.eAI_Translator_converter.create_data_source_files","title":"create_data_source_files","text":"<pre><code>create_data_source_files(tflite_model_path, eqcheck_data_path, templates_dir, save_dir)\n</code></pre> <p>Create the data source files for the eAI Translator model.</p> <p>Parameters:</p> Name Type Description Default <code>tflite_model_path</code> <code>str</code> <p>The path to the TensorFlow Lite model.</p> required <code>eqcheck_data_path</code> <code>str</code> <p>The path to the equality check data.</p> required <code>templates_dir</code> <code>str</code> <p>The directory containing the templates.</p> required <code>save_dir</code> <code>str</code> <p>The directory to save the data source files.</p> required Source code in <code>edgemark/models/platforms/eAI_Translator/eAI_Translator_converter.py</code> <pre><code>def create_data_source_files(tflite_model_path, eqcheck_data_path, templates_dir, save_dir):\n    \"\"\"\n    Create the data source files for the eAI Translator model.\n\n    Args:\n        tflite_model_path (str): The path to the TensorFlow Lite model.\n        eqcheck_data_path (str): The path to the equality check data.\n        templates_dir (str): The directory containing the templates.\n        save_dir (str): The directory to save the data source files.\n    \"\"\"\n    def _np_to_c(array):\n        array = np.array(array)\n        array = array.reshape(-1)\n        c_array = \"{\" + \", \".join([str(x) for x in array]) + \"}\"\n        return c_array\n\n    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    in_scale, in_zero_point = input_details[0]['quantization']\n    out_scale, out_zero_point = output_details[0]['quantization']\n    if input_details[0]['dtype'] is np.float32 and output_details[0]['dtype'] is np.float32:\n        in_out_dtype = \"float\"\n    elif input_details[0]['dtype'] is np.int16 and output_details[0]['dtype'] is np.int16:\n        in_out_dtype = \"int16\"\n    elif input_details[0]['dtype'] is np.int8 and output_details[0]['dtype'] is np.int8:\n        in_out_dtype = \"int8\"\n    else:\n        raise ValueError(\"Unknown input and output types: {} and {}\".format(input_details[0]['dtype'], output_details[0]['dtype']))\n\n    data = np.load(eqcheck_data_path)\n    data_x = data['data_x']\n    data_y = data['data_y_pred']\n    data.close()\n\n    with open(os.path.join(templates_dir, 'data.h'), 'r') as f:\n        h_file = f.read()\n\n    with open(os.path.join(templates_dir, 'data.c'), 'r') as f:\n        c_file = f.read()\n\n    h_file = h_file.replace(\"{n_samples}\", str(data_x.shape[0]))\n    c_file = c_file.replace(\"{n_samples}\", str(data_x.shape[0]))\n\n    data_x_size = np.prod(data_x.shape[1:])\n    data_y_size = np.prod(data_y.shape[1:])\n    h_file = h_file.replace(\"{samples_x_size}\", str(data_x_size))\n    h_file = h_file.replace(\"{samples_y_size}\", str(data_y_size))\n    c_file = c_file.replace(\"{samples_x_size}\", str(data_x_size))\n    c_file = c_file.replace(\"{samples_y_size}\", str(data_y_size))\n\n    if in_out_dtype == \"float\":\n        h_file = h_file.replace(\"{samples_x_dtype}\", \"float\")\n        h_file = h_file.replace(\"{samples_y_dtype}\", \"float\")\n        c_file = c_file.replace(\"{samples_x_dtype}\", \"float\")\n        c_file = c_file.replace(\"{samples_y_dtype}\", \"float\")\n    elif in_out_dtype == \"int8\":\n        h_file = h_file.replace(\"{samples_x_dtype}\", \"int8_t\")\n        h_file = h_file.replace(\"{samples_y_dtype}\", \"int8_t\")\n        c_file = c_file.replace(\"{samples_x_dtype}\", \"int8_t\")\n        c_file = c_file.replace(\"{samples_y_dtype}\", \"int8_t\")\n        data_x = (data_x / in_scale) + in_zero_point\n        data_y = (data_y / out_scale) + out_zero_point\n        data_x = np.clip(data_x, -128, 127)\n        data_y = np.clip(data_y, -128, 127)\n        data_x = data_x.astype(np.int8)\n        data_y = data_y.astype(np.int8)\n    elif in_out_dtype == \"int16\":\n        h_file = h_file.replace(\"{samples_x_dtype}\", \"int16_t\")\n        h_file = h_file.replace(\"{samples_y_dtype}\", \"int16_t\")\n        c_file = c_file.replace(\"{samples_x_dtype}\", \"int16_t\")\n        c_file = c_file.replace(\"{samples_y_dtype}\", \"int16_t\")\n        data_x = (data_x / in_scale) + in_zero_point\n        data_y = (data_y / out_scale) + out_zero_point\n        data_x = np.clip(data_x, -32768, 32767)\n        data_y = np.clip(data_y, -32768, 32767)\n        data_x = data_x.astype(np.int16)\n        data_y = data_y.astype(np.int16)\n    else:\n        raise ValueError(\"Unknown in_out_dtype: {}\".format(in_out_dtype))\n\n    data_x_str = \"\\n\"\n    for i, sample_x in enumerate(data_x):\n        data_x_str += \"\\t\" + _np_to_c(sample_x)\n        if i &lt; len(data_x) - 1:\n            data_x_str += \",\"\n        data_x_str += \"\\n\"\n\n    data_y_str = \"\\n\"\n    for i, sample_y in enumerate(data_y):\n        data_y_str += \"\\t\" + _np_to_c(sample_y)\n        if i &lt; len(data_y) - 1:\n            data_y_str += \",\"\n        data_y_str += \"\\n\"\n\n    c_file = c_file.replace(\"{samples_x}\", data_x_str)\n    c_file = c_file.replace(\"{samples_y}\", data_y_str)\n\n    os.makedirs(save_dir, exist_ok=True)\n    with open(os.path.join(save_dir, 'data.h'), 'w') as f:\n        f.write(h_file)\n    with open(os.path.join(save_dir, 'data.c'), 'w') as f:\n        f.write(c_file)\n</code></pre>"},{"location":"api/platforms/eAI_Translator/#edgemark.models.platforms.eAI_Translator.eAI_Translator_converter.main","title":"main","text":"<pre><code>main(cfg_path=config_file_path, **kwargs)\n</code></pre> <p>Create the data source files for the eAI Translator model.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_path</code> <code>str</code> <p>The path to the configuration file. The configuration file that this path points to should contain the following keys:     - model_base_dir (str): A placeholder for the model base directory. This will be populated by the target directory.     - tflite_conversion_type (str): A placeholder for the TFLite conversion type. This will be populated by the conversion type.     - linkers_dir (str): Path to the directory where the tflite converted models list is loaded from and the list of converted models is saved.     - tflite_model_path (str): The path to the TensorFlow Lite model.     - eqcheck_data_path (str): The path to the equality check data.     - data_templates_dir (str): Path to the directory containing the data templates.     - translator_save_dir (str): The directory to save the eAI Translator files.</p> <code>config_file_path</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments to be passed to the configuration file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing the following keys for each target model: - dir (str): The directory of the target model. - flavors (list): A list of dictionaries containing the following keys for each optimization:     - flavor (str): The optimization flavor.     - result (str): The result of the conversion. It can be either \"success\" or \"failed\".     - error (str): The error message in case of failure.     - traceback (str): The traceback in case of failure.</p> Source code in <code>edgemark/models/platforms/eAI_Translator/eAI_Translator_converter.py</code> <pre><code>def main(cfg_path=config_file_path, **kwargs):\n    \"\"\"\n    Create the data source files for the eAI Translator model.\n\n    Args:\n        cfg_path (str): The path to the configuration file.\n            The configuration file that this path points to should contain the following keys:\n                - model_base_dir (str): A placeholder for the model base directory. This will be populated by the target directory.\n                - tflite_conversion_type (str): A placeholder for the TFLite conversion type. This will be populated by the conversion type.\n                - linkers_dir (str): Path to the directory where the tflite converted models list is loaded from and the list of converted models is saved.\n                - tflite_model_path (str): The path to the TensorFlow Lite model.\n                - eqcheck_data_path (str): The path to the equality check data.\n                - data_templates_dir (str): Path to the directory containing the data templates.\n                - translator_save_dir (str): The directory to save the eAI Translator files.\n        **kwargs (dict): Keyword arguments to be passed to the configuration file.\n\n    Returns:\n        list: A list of dictionaries containing the following keys for each target model:\n            - dir (str): The directory of the target model.\n            - flavors (list): A list of dictionaries containing the following keys for each optimization:\n                - flavor (str): The optimization flavor.\n                - result (str): The result of the conversion. It can be either \"success\" or \"failed\".\n                - error (str): The error message in case of failure.\n                - traceback (str): The traceback in case of failure.\n    \"\"\"\n    cfg = OmegaConf.load(cfg_path, **kwargs)\n    cfg.update(OmegaConf.create(kwargs))\n\n    targets = OmegaConf.load(os.path.join(cfg.linkers_dir, \"tflite_converted_models_list.yaml\"))\n\n    output = [{\"dir\": base_model[\"model_base_dir\"], \"flavors\": []} for base_model in targets]\n\n    converted_models_list = []\n    for i, base_model in enumerate(targets):\n        print(\"\\Creating data files for the model in: {} ({}/{})\".format(base_model[\"model_base_dir\"], i+1, len(targets)))\n\n        for model_flavor in base_model[\"conversions\"]:\n            if model_flavor in [\"basic\", \"q_full_int_only\"]:\n                txt = \"Model type: {} ...\".format(model_flavor)\n                txt += \" \" * (32 - len(txt))\n                print(txt, end=\" \", flush=True)\n                cfg.model_base_dir = base_model[\"model_base_dir\"]\n                cfg.tflite_conversion_type = model_flavor\n\n                try:\n                    create_data_source_files(cfg.tflite_model_path, cfg.eqcheck_data_path, cfg.data_templates_dir, cfg.translator_save_dir)\n                    if os.path.exists(os.path.join(cfg.translator_save_dir, 'exception.txt')):\n                        os.remove(os.path.join(cfg.translator_save_dir, 'exception.txt'))\n                    converted_models_list.append(cfg.translator_save_dir)\n                    output[i][\"flavors\"].append({\n                        \"flavor\": model_flavor,\n                        \"result\": \"success\"\n                    })\n                    print(\"Done\")\n                except Exception as e:\n                    error_message = traceback.format_exc()\n                    _save_translator_exception(error_message, cfg.translator_save_dir, delete_assets=True)\n                    output[i][\"flavors\"].append({\n                        \"flavor\": model_flavor,\n                        \"result\": \"failed\",\n                        \"error\": type(e).__name__,\n                        \"traceback\": traceback.format_exc()\n                    })\n                    print(\"Failed\")\n\n    os.makedirs(cfg.linkers_dir, exist_ok=True)\n    with open(os.path.join(cfg.linkers_dir, 'translator_converted_models_list.yaml'), 'w') as f:\n        yaml.dump(converted_models_list, f, indent=4, sort_keys=False)\n\n    return output\n</code></pre>"},{"location":"api/platforms/ekkono/","title":"Ekkono","text":""},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model_generator","title":"edgemark.models.platforms.Ekkono.model_generator","text":""},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model_generator.main","title":"main","text":"<pre><code>main(cfg_path=config_file_path, **kwargs)\n</code></pre> <p>Generate, train, evaluate, and save models based on the given configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_path</code> <code>str</code> <p>The path to the configuration file containing the model generation parameters. The configuration file that this path points to should contain the following keys:     - time_tag (str): A placeholder for the time tag. This will be populated by the current time.     - linkers_dir (str): Path to the directory where the generated models list will be saved.     - target_models_dir (str): Path to the directory containing the target models configurations.     - datasets_dir (str): Path to the directory containing the datasets.     - model_save_dir (str): Path to the directory where the generated model will be saved.     - crystal_templates_dir (str): Path to the directory containing the crystal templates.     - wandb_online (bool): Flag to enable or disable the W&amp;B online mode.     - wandb_project_name (str): Name of the W&amp;B project.     - train_models (bool): Flag to enable or disable model training.     - evaluate_models (bool): Flag to enable or disable model evaluation.     - measure_execution_time (bool): Flag to enable or disable the measurement of execution time.     - n_eqcheck_data (int): Number of samples to be saved for equivalence check of the model on PC and MCU.</p> <code>config_file_path</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments to be passed to the configuration file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing the following keys for each target model: - name (str): Name of the target model configuration file. - result (str): Result of the model generation. It can be either \"success\" or \"failed\". - error (str): Error message in case of failure. - traceback (str): Traceback in case of failure.</p> Source code in <code>edgemark/models/platforms/Ekkono/model_generator.py</code> <pre><code>def main(cfg_path=config_file_path, **kwargs):\n    \"\"\"\n    Generate, train, evaluate, and save models based on the given configuration file.\n\n    Args:\n        cfg_path (str): The path to the configuration file containing the model generation parameters.\n            The configuration file that this path points to should contain the following keys:\n                - time_tag (str): A placeholder for the time tag. This will be populated by the current time.\n                - linkers_dir (str): Path to the directory where the generated models list will be saved.\n                - target_models_dir (str): Path to the directory containing the target models configurations.\n                - datasets_dir (str): Path to the directory containing the datasets.\n                - model_save_dir (str): Path to the directory where the generated model will be saved.\n                - crystal_templates_dir (str): Path to the directory containing the crystal templates.\n                - wandb_online (bool): Flag to enable or disable the W&amp;B online mode.\n                - wandb_project_name (str): Name of the W&amp;B project.\n                - train_models (bool): Flag to enable or disable model training.\n                - evaluate_models (bool): Flag to enable or disable model evaluation.\n                - measure_execution_time (bool): Flag to enable or disable the measurement of execution time.\n                - n_eqcheck_data (int): Number of samples to be saved for equivalence check of the model on PC and MCU.\n        **kwargs (dict): Keyword arguments to be passed to the configuration file.\n\n    Returns:\n        list: A list of dictionaries containing the following keys for each target model:\n            - name (str): Name of the target model configuration file.\n            - result (str): Result of the model generation. It can be either \"success\" or \"failed\".\n            - error (str): Error message in case of failure.\n            - traceback (str): Traceback in case of failure.\n    \"\"\"\n    cfg = OmegaConf.load(cfg_path, **kwargs)\n    cfg.update(OmegaConf.create(kwargs))\n\n    if not cfg.wandb_online:\n        os.environ['WANDB_MODE'] = 'offline'\n\n    target_files = find_target_files(cfg.target_models_dir)\n\n    output = [{\"name\": os.path.splitext(target_file)[0]} for target_file in target_files]\n\n    converted_models_list = []\n    for i, target_file in enumerate(target_files):\n        try:\n            model_cfg_path = os.path.join(cfg.target_models_dir, target_file)\n            model_cfg = OmegaConf.load(model_cfg_path)\n            cfg.time_tag = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n            if \"epochs\" in cfg:\n                model_cfg.epochs = cfg.epochs\n            if \"dataset\" in model_cfg:\n                model_cfg.dataset.path = os.path.join(cfg.datasets_dir, model_cfg.dataset.name, \"data.py\")\n\n            wandb_name = datetime.datetime.strptime(cfg.time_tag, \"%Y-%m-%d_%H-%M-%S\").strftime(\"%Y-%m-%d %H:%M:%S\")\n            wandb_dir = get_abs_path(cfg.model_save_dir)\n            os.makedirs(wandb_dir, exist_ok=True)\n            wandb.init(project=cfg.wandb_project_name, tags=[model_cfg.dataset.name, os.path.splitext(target_file)[0]], name=wandb_name, dir=wandb_dir)\n\n            title = \"Creating the model described in {} ({}/{})\".format(target_file, i+1, len(target_files))\n            print(\"\\n\")\n            print(\"=\"*80)\n            print(\"-\"*((80-len(title)-2)//2), end=\" \")\n            print(title, end=\" \")\n            print(\"-\"*((80-len(title)-2)//2))\n            print(\"=\"*80)\n\n            supervisor = ModelSupervisor(OmegaConf.to_container(model_cfg, resolve=True))\n\n            if cfg.train_models:\n                print(\"Training the model ...\", end=\" \", flush=True)\n                supervisor.train_model()\n                print(\"Done\\n\")\n\n            evaluation_result = None\n            if cfg.evaluate_models:\n                try:\n                    evaluation_result = supervisor.evaluate_model()\n                    for metric, value in evaluation_result.items():\n                        print(metric, \":\", value)\n                    print(\"\")\n                except Exception as e:\n                    print(\"Error in evaluating the model: {}\".format(e))\n                    print(\"Continuing without evaluation\")\n                    continue\n\n            print(\"Saving model to the directory: {} ...\".format(os.path.join(cfg.model_save_dir, \"edge\")), end=\" \", flush=True)\n            supervisor.save_model(os.path.join(cfg.model_save_dir, \"edge\"))\n            print(\"Done\\n\")\n            supervisor.log_model_to_wandb(os.path.join(cfg.model_save_dir, \"edge\"), os.path.splitext(target_file)[0].replace(\"/\", \"_\"))\n\n            print(\"Saving equality check data to the directory: {} ...\".format(os.path.join(cfg.model_save_dir, \"data\")), end=\" \", flush=True)\n            supervisor.save_eqcheck_data(cfg.n_eqcheck_data, os.path.join(cfg.model_save_dir, \"data\"))\n            print(\"Done\\n\")\n\n            if cfg.measure_execution_time:\n                print(\"Measuring execution time ...\")\n                execution_time = supervisor.measure_execution_time()\n                print(\"Average run time: {} ms\\n\".format(execution_time))\n\n            model_info = {\"Description\": \"\"}\n            model_info[\"setting_file\"] = target_file\n            model_info[\"trained\"] = cfg.train_models\n            model_info.update(supervisor.get_model_info())\n            if evaluation_result is not None:\n                for metric, value in evaluation_result.items():\n                    if not isinstance(metric, str):\n                        metric = str(metric)\n                    model_info[metric] = value\n            if cfg.measure_execution_time:\n                model_info[\"execution_time\"] = execution_time\n            model_info[\"wandb_name\"] = wandb_name\n\n            print(\"Saving the model info in the directory: {} ...\".format(cfg.model_save_dir), end=\" \", flush=True)\n            supervisor.save_model_info(model_info, cfg.model_save_dir)\n            print(\"Done\\n\")\n            wandb.config.update(model_info)\n\n            print(\"Saving the crystal files in the directory: {} ...\".format(os.path.join(cfg.model_save_dir, \"crystal\")), end=\" \", flush=True)\n            supervisor.fill_crystal_templates(os.path.join(cfg.model_save_dir, \"crystal\"), os.path.join(cfg.model_save_dir, \"data\"), cfg.crystal_templates_dir)\n            print(\"Done\\n\")\n\n            converted_models_list.append(os.path.join(cfg.model_save_dir, \"crystal\"))\n\n            wandb.finish()\n\n            output[i][\"result\"] = \"success\"\n\n        except Exception as e:\n            output[i][\"result\"] = \"failed\"\n            output[i][\"error\"] = type(e).__name__\n            output[i][\"traceback\"] = traceback.format_exc()\n            print(\"Error in generating the model:\")\n            print(traceback.format_exc())\n\n    os.makedirs(cfg.linkers_dir, exist_ok=True)\n    with open(os.path.join(cfg.linkers_dir, 'ekkono_converted_models_list.yaml'), 'w') as f:\n        yaml.dump(converted_models_list, f, indent=4, sort_keys=False)\n\n    return output\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model","title":"edgemark.models.platforms.Ekkono.model","text":""},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor","title":"ModelSupervisor","text":"Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>class ModelSupervisor:\n    def __init__(self, cfg=None):\n        \"\"\"\n        Initializes the class.\n        The following attributes should be set in the __init__ function:\n            self.model (ekkono.primer.Model): The model.\n            self.dataset (DatasetSupervisorTemplate): The dataset.\n\n        Args:\n            cfg (dict): The configuration of the model. Defaults to None.\n        \"\"\"\n\n        # default configs\n        self.ekkono_model_type = \"pretrained\"  # Ekkono model type. \"pretrained\" or \"incremental\". \"pretrained\" is not able to be trained on the end device, but \"incremental\" is.\n        self.denses_params = [16]       # each element is the number of neurons of a dense layer\n        self.activation = \"sigmoid\"     # the activation function of layers. It can be one of \"sigmoid\", \"tanh\", \"relu\" (which actually is leaky relu)\n        self.epochs = 50\n        self.batch_size = 32\n        self.dataset_info = {\n            \"name\": \"cifar10\",\n            \"path\": \"edgemark/models/datasets/cifar10/data.py\",\n            \"args\": {\n                \"flat_features\": True\n            }\n        }\n        self.random_seed = None\n\n        self.learning_rate = 1e-3\n        self.fine_tuning_learning_rate = 1e-4\n        self.fine_tuning_epochs = 5\n        self.fine_tuning_batch_size = 128\n\n        # update configs if provided\n        if cfg is not None:\n            self.set_configs(cfg)\n\n        # load corresponding dataset\n        spec = importlib.util.spec_from_file_location(\"imported_module\", self.dataset_info[\"path\"])\n        imported_module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(imported_module)\n\n        # load data\n        self.dataset = imported_module.DatasetSupervisor(**self.dataset_info[\"args\"])\n        assert len(self.dataset.feature_shape) == 1, \"The input shape must be 1-dimensional.\"\n        assert self.dataset.num_labels == 1, \"The output size should be 1.\"\n        assert len(self.dataset.train_x.shape) == 2, \"The train_x should be 2-dimensional.\"\n        assert len(self.dataset.train_y.shape) == 2, \"The train_y should be 2-dimensional.\"\n        assert self.dataset.train_y.shape[1] == 1, \"The output size should be 1.\"\n        assert self.dataset.output_activation == \"linear\", \"The output activation should be linear.\"\n        assert self.dataset.loss_function in [\"mean_squared_error\", \"mse\"], \"The loss function should be mean_squared_error.\"\n        for metric in self.dataset.metrics:\n            if metric not in [\"mean_squared_error\", \"mse\", \"rmse\", \"mean_absolute_error\", \"mae\"]:\n                print(\"Warning: The metric '{}' is not supported.\".format(metric))\n\n        (self.trainset, self.testset), target_name = self._create_ekkono_datasets(self.dataset)\n\n        # create the model\n        if self.ekkono_model_type == \"pretrained\":\n            self.model = self.create_pretrained_model(self.denses_params, self.activation, self.learning_rate, self.trainset.pipeline_template, target_name)\n        elif self.ekkono_model_type == \"incremental\":\n            self.model = self.create_incremental_model(self.denses_params, self.activation, self.epochs, self.batch_size, self.learning_rate, self.trainset, target_name)\n        else:\n            raise ValueError(\"The specified model type is not supported.\")\n\n\n    @staticmethod\n    def _create_ekkono_datasets(dataset):\n        \"\"\"\n        Creates Ekkono datasets based on the given dataset.\n\n        Args:\n            dataset (DatasetSupervisorTemplate): The dataset.\n\n        Returns:\n            tuple: ((ekkono.primer.Dataset, ekkono.primer.Dataset), str) which is ((trainset, testset), target_name)\n        \"\"\"\n        assert len(dataset.feature_shape) == 1, \"The input shape must be 1-dimensional.\"\n        assert dataset.num_labels == 1, \"The output size should be 1.\"\n        assert len(dataset.train_x.shape) == 2, \"The train_x should be 2-dimensional.\"\n        assert len(dataset.train_y.shape) == 2, \"The train_y should be 2-dimensional.\"\n        assert dataset.train_y.shape[1] == 1, \"The output size should be 1.\"\n\n        trainset_attributes = []\n        for i in range(dataset.feature_shape[0]):\n            trainset_attributes.append(primer.AttributeMeta('x_'+str(i)))\n        trainset_attributes.append(primer.AttributeMeta('y'))\n        trainset = primer.Dataset(trainset_attributes)\n\n        testset_attributes = []\n        for i in range(dataset.feature_shape[0]):\n            testset_attributes.append(primer.AttributeMeta('x_'+str(i)))\n        testset_attributes.append(primer.AttributeMeta('y'))\n        testset = primer.Dataset(testset_attributes)\n\n        for x, y in zip(dataset.train_x, dataset.train_y):\n            instance = [x_i.tolist() for x_i in x]\n            instance.append(y[0].tolist())\n            trainset.create_instance(instance)\n\n        for x, y in zip(dataset.test_x, dataset.test_y):\n            instance = [x_i.tolist() for x_i in x]\n            instance.append(y[0].tolist())\n            testset.create_instance(instance)\n\n        return (trainset, testset), \"y\"\n\n\n    def set_configs(self, cfg):\n        \"\"\"\n        Sets the configurations of the model.\n\n        Note: The changed configs won't affect the data, model, or any other loaded attributes.\n        In case you want to change them, you should call the corresponding functions.\n\n        Args:\n            cfg (dict): The configuration.\n        \"\"\"\n        if \"model_type\" in cfg:\n            if cfg[\"model_type\"] is not None and cfg[\"model_type\"] != \"CNN\":\n                raise ValueError(\"The model type should be 'CNN'.\")\n        if \"ekkono_model_type\" in cfg:\n            self.ekkono_model_type = cfg[\"ekkono_model_type\"]\n        if \"convs_params\" in cfg:\n            if cfg[\"convs_params\"] is not None and cfg[\"convs_params\"] != []:\n                raise ValueError(\"The model doesn't support convolutional layers.\")\n        if \"denses_params\" in cfg:\n            self.denses_params = cfg[\"denses_params\"]\n        if \"convs_dropout\" in cfg:\n            if cfg[\"convs_dropout\"] is not None and cfg[\"convs_dropout\"] != 0.00:\n                raise ValueError(\"Dropout is not supported in the model.\")\n        if \"denses_dropout\" in cfg:\n            if cfg[\"denses_dropout\"] is not None and cfg[\"denses_dropout\"] != 0.00:\n                raise ValueError(\"Dropout is not supported in the model.\")\n        if \"activation\" in cfg:\n            self.activation = cfg[\"activation\"]\n        if \"use_batchnorm\" in cfg:\n            if cfg[\"use_batchnorm\"] is not None and cfg[\"use_batchnorm\"] is not False:\n                raise ValueError(\"Batch normalization is not supported in the model.\")\n        if \"epochs\" in cfg:\n            self.epochs = cfg[\"epochs\"]\n        if \"batch_size\" in cfg:\n            self.batch_size = cfg[\"batch_size\"]\n        if \"dataset\" in cfg:\n            self.dataset_info = cfg[\"dataset\"]\n        if \"random_seed\" in cfg:\n            if cfg[\"random_seed\"] is not None:\n                print(\"Warning: Ekkono API doesn't let us to control its random operations. The random seed is ignored.\")\n        if \"learning_rate\" in cfg:\n            self.learning_rate = cfg[\"learning_rate\"]\n        if \"fine_tuning_learning_rate\" in cfg:\n            self.fine_tuning_learning_rate = cfg[\"fine_tuning_learning_rate\"]\n        if \"fine_tuning_epochs\" in cfg:\n            self.fine_tuning_epochs = cfg[\"fine_tuning_epochs\"]\n        if \"fine_tuning_batch_size\" in cfg:\n            self.fine_tuning_batch_size = cfg[\"fine_tuning_batch_size\"]\n\n\n    @staticmethod\n    def create_pretrained_model(denses_params, activation, learning_rate, pipeline_template, target_name):\n        \"\"\"\n        Creates the Ekkono pretrained model. The model is not able to be trained on the end device.\n\n        Args:\n            denses_params (list): Each element is the number of neurons of a dense layer (excluding the output layer which has one neuron).\n            activation (str): The activation function of the hidden layers. Can be one of \"sigmoid\", \"tanh\", \"relu\" (which actually is leaky relu).\n            pipeline_template (Ekkono.primer.PipelineTemplate): The pipeline template (usually taken from dataset).\n            target_name (str): The name of the target attribute.\n        \"\"\"\n        activation_function = None\n        if activation == \"sigmoid\":\n            activation_function = primer.MLPModelParams.ActivationFunction.SIGMOID\n        elif activation == \"tanh\":\n            activation_function = primer.MLPModelParams.ActivationFunction.TANH\n        elif activation == \"relu\":\n            activation_function = primer.MLPModelParams.ActivationFunction.LEAKYRELU\n        else:\n            raise ValueError(\"The specified activation function is not supported.\")\n\n        pipeline_template.add_target(target_name)\n\n        edge_model = primer.ModelFactory.create_mlp_model(\n            pipeline_template,\n            hidden_layers=denses_params,\n            optimizer= primer.MLPModelParams.Optimizer.ADAM,\n            start_learning_rate=learning_rate,\n            end_learning_rate=learning_rate,\n            activation_function=activation_function\n        )\n\n        return edge_model\n\n\n    @staticmethod\n    def create_incremental_model(denses_params, activation, epochs, batch_size, learning_rate, trainset, target_name):\n        \"\"\"\n        Creates the Ekkono incremental model. The model is able to be trained on the end device.\n\n        Args:\n            denses_params (list): Each element is the number of neurons of a dense layer (excluding the output layer which has one neuron).\n            activation (str): The activation function of the hidden layers. Can be one of \"sigmoid\", \"tanh\", \"relu\" (which actually is leaky relu).\n            epochs (int): The number of epochs for training.\n            batch_size (int): The batch size for training.\n            learning_rate (float): The learning rate for training.\n            trainset (DatasetSupervisorTemplate): The training dataset.\n            target_name (str): The name of the target attribute.\n        \"\"\"\n        activation_function = None\n        if activation == \"sigmoid\":\n            activation_function = primer.MLPModelParams.ActivationFunction.SIGMOID\n        elif activation == \"tanh\":\n            activation_function = primer.MLPModelParams.ActivationFunction.TANH\n        elif activation == \"relu\":\n            activation_function = primer.MLPModelParams.ActivationFunction.LEAKYRELU\n        else:\n            raise ValueError(\"The specified activation function is not supported.\")\n\n        pipeline_template = trainset.pipeline_template\n        pipeline_template.add_target(target_name)\n\n        edge_model = primer.ModelFactory.create_incremental_mlp_model(\n            pipeline_template,\n            hidden_layers=denses_params,                        # Crystal Tunable\n            batch_size=batch_size,                              # Crystal Tunable\n            iterations_per_batch=epochs,                        # Crystal Fixed - Edge can have an epoch greater than 1 but for Crystal it is fixed to 1\n            optimizer= primer.MLPModelParams.Optimizer.ADAM,    # Crystal Fixed - Edge can use different optimizers but Crystal uses GRADIENTDESCENT\n            start_learning_rate=learning_rate,                  # Crystal Tunable\n            end_learning_rate=learning_rate,                    # Crystal Tunable\n            decay_instances=5000,                               # Crystal Tunable\n            activation_function=activation_function,            # Crystal Tunable\n            attribute_stats=pipeline_template.instantiate(trainset).get_attribute_stats()\n        )\n\n        return edge_model\n\n\n    # Optional function: Whether you implement this function or not depends on your application.\n    def train_model(self):\n        \"\"\"\n        Trains the model.\n        \"\"\"\n        primer.ModelTrainer.train(self.model, self.trainset, batch_size=self.batch_size, epochs=self.epochs)\n\n\n    # Optional function: Whether you implement this function or not depends on your application.\n    def evaluate_model(self):\n        \"\"\"\n        Evaluates the model.\n\n        Returns:\n            dict: The evaluation metrics.\n        \"\"\"\n        train_pred_tuples = primer.ModelTester.get_prediction_tuples(self.model, self.model.target_attributes[0], self.trainset)\n        train_mae = primer.ModelTester.calculate_mae(train_pred_tuples)\n        train_rmse = primer.ModelTester.calculate_rmse(train_pred_tuples)\n\n        test_pred_tuples = primer.ModelTester.get_prediction_tuples(self.model, self.model.target_attributes[0], self.testset)\n        test_mae = primer.ModelTester.calculate_mae(test_pred_tuples)\n        test_rmse = primer.ModelTester.calculate_rmse(test_pred_tuples)\n\n        output = {\n            \"train_mae\": train_mae,\n            \"train_rmse\": train_rmse,\n            \"test_mae\": test_mae,\n            \"test_rmse\": test_rmse\n        }\n        return output\n\n\n    def get_model_info(self):\n        \"\"\"\n        Returns the model info.\n\n        Returns:\n            dict: The model info.\n        \"\"\"\n        model_info = {\n            \"ekkono_model_type\": self.ekkono_model_type,\n            \"denses_params\": self.denses_params,\n            \"activation\": self.activation,\n            \"epochs\": self.epochs,\n            \"batch_size\": self.batch_size,\n            \"learning_rate\": self.learning_rate,\n            \"fine_tuning_epochs\": self.fine_tuning_epochs,\n            \"fine_tuning_batch_size\": self.fine_tuning_batch_size,\n            \"fine_tuning_learning_rate\": self.fine_tuning_learning_rate,\n            \"dataset\": self.dataset_info,\n            \"random_seed\": self.random_seed\n        }\n        return model_info\n\n\n    def _estimate_arena_size(self, verbose=False):\n        \"\"\"\n        Estimates the size of the arena for the TFLM model.\n\n        Returns:\n            int: The size of the arena in bytes.\n        \"\"\"\n        arena_size = primer.ModelInspector.crystal_dynamic_memory_usage(self.model)\n\n        if verbose:\n            print(\"Edge memory usage: {:.2f} kB\".format(self.model.memory_usage / 1024))\n            print(\"Crystal file size: {:.2f} kB\".format(primer.ModelInspector.crystal_model_size(self.model) / 1024))\n            print(\"Crystal RAM requirement: {:.2f} kB\".format(arena_size / 1024))\n\n        return arena_size\n\n\n    def measure_execution_time(self):\n        \"\"\"\n        Measures the execution time of the model.\n\n        Returns:\n            float: The execution time in ms.\n        \"\"\"\n        rng = np.random.RandomState(42)\n        sample_idx = rng.randint(0, self.dataset.train_x.shape[0])\n        x = np.array(self.dataset.train_x[sample_idx])\n\n        crystal_model_data = primer.Converter.convert(self.model, primer.Converter.ConversionTarget.CRYSTAL)\n        if self.ekkono_model_type == \"pretrained\":\n            crystal_model = crystal.load_predictive_model(crystal_model_data)\n        elif self.ekkono_model_type == \"incremental\":\n            crystal_model = crystal.load_incremental_predictive_model(crystal_model_data)\n\n        # warm up\n        tic = time.time()\n        for i in range(10000):\n            crystal_model.predict(x)\n        toc = time.time()\n        itr = int(10 * 10000 / (toc - tic))\n\n        # run the test\n        tic = time.time()\n        for i in range(itr):\n            crystal_model.predict(x)\n        toc = time.time()\n        execution_time = (toc-tic)/itr*1000     # in ms\n\n        return execution_time\n\n\n    def save_eqcheck_data(self, n_samples, save_dir):\n        \"\"\"\n        Saves the eqcheck data as {\"data_x\", \"data_y_pred\"}.\n\n        The data_x has shape (samples, *input_shape) and data_y_pred has shape (samples, *output_shape).\n\n        Args:\n            n_samples (int): The number of samples to be saved\n            save_dir (str): The directory where the data should be saved\n        \"\"\"\n        data_x = self.dataset.train_x[:n_samples]\n        data_y_pred = []\n        for i in range(len(data_x)):\n            instance = data_x[i].tolist() + [0]  # the last element is a placeholder for the output\n            data_y_pred.append(self.model.predict(instance))\n        data_y_pred = np.array(data_y_pred)\n\n        os.makedirs(save_dir, exist_ok=True)\n        np.savez(os.path.join(save_dir, 'eqcheck_data.npz'), data_x=data_x, data_y_pred=data_y_pred)\n\n\n    # Optional function\n    @staticmethod\n    def load_eqcheck_data(load_dir):\n        \"\"\"\n        Loads the eqcheck data.\n\n        Args:\n            load_dir (str): The directory where the eqcheck data is stored.\n\n        Returns:\n            tuple: (data_x, data_y_pred)\n        \"\"\"\n        eqcheck_data = np.load(os.path.join(load_dir, 'eqcheck_data.npz'))\n        data_x = eqcheck_data['data_x']\n        data_y_pred = eqcheck_data['data_y_pred']\n        return data_x, data_y_pred\n\n\n    def save_model(self, save_dir):\n        \"\"\"\n        Saves the model.\n\n        Args:\n            save_dir (str): The directory where the model should be saved in.\n        \"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n        self.model.save(os.path.join(save_dir, \"model.edge\"))\n\n\n    def load_model(self, load_dir):\n        \"\"\"\n        Loads the model.\n\n        Args:\n            load_dir (str): The parent directory where the SavedModel format is stored in.\n        \"\"\"\n        if self.ekkono_model_type == \"pretrained\":\n            self.model = primer.PredictiveModel.load(os.path.join(load_dir, \"model.edge\"))\n        elif self.ekkono_model_type == \"incremental\":\n            self.model = primer.IncrementalPredictiveModel.load(os.path.join(load_dir, \"model.edge\"))\n\n\n    @staticmethod\n    def log_model_to_wandb(model_dir, model_save_name):\n        \"\"\"\n        Logs the model to wandb.\n\n        Args:\n            model_dir (str): The directory where the model is stored.\n            model_save_name (str): The name that will be assigned to the model artifact.\n        \"\"\"\n        model_path = os.path.join(model_dir, \"model.edge\")\n        model_artifact = wandb.Artifact(model_save_name, type=\"model\")\n        model_artifact.add_file(model_path)\n        wandb.log_artifact(model_artifact)\n\n\n    @staticmethod\n    def save_model_info(model_info, save_dir):\n        \"\"\"\n        Saves the model info.\n\n        Args:\n            model_info (dict): The model info.\n            save_dir (str): The directory where the model info should be saved.\n        \"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n        yaml.Dumper.ignore_aliases = lambda *args : True\n        with open(os.path.join(save_dir, \"model_info.yaml\"), 'w') as f:\n            yaml.dump(model_info, f, indent=4, sort_keys=False)\n\n\n    def fill_crystal_templates(self, save_dir, eqcheck_data_dir, templates_dir):\n        \"\"\"\n        Taken from the main.h/c and data.h/c available in the templates_dir, fills their\n        placeholders with the appropriate data and saves them to the save_dir.\n\n        Args:\n            save_dir (str): The directory where the filled files should be saved.\n            eqcheck_data_dir (str): The directory where the eqcheck data is stored.\n            templates_dir (str): The directory where the templates are stored.\n        \"\"\"\n        def _np_to_c(array):\n            if array.ndim == 0:\n                return str(array.item())\n            c_array = \"{\" + \", \".join(_np_to_c(subarray) for subarray in array) + \"}\"\n            return c_array\n\n        # create model files\n        with open(os.path.join(templates_dir, 'model.h'), 'r') as f:\n            h_file = f.read()\n        with open(os.path.join(templates_dir, 'model.c'), 'r') as f:\n            c_file = f.read()\n\n        h_file = h_file.replace(\"{model_type}\", self.ekkono_model_type.upper())\n        h_file = h_file.replace(\"{arena_size}\", str(self._estimate_arena_size()))\n        h_file = h_file.replace(\"{input_size}\", str(self.dataset.feature_shape[0]))\n\n        crystal_model_data = primer.Converter.convert(self.model, primer.Converter.ConversionTarget.CRYSTAL)\n\n        c_file = c_file.replace(\"{model_data_size}\", str(len(crystal_model_data)))\n\n        model_data_str = \"\"\n        for i, byte in enumerate(crystal_model_data):\n            if i % 16 == 0:\n                model_data_str += \"\\n\\t\"\n            model_data_str += \"0x{:02x}, \".format(byte)\n        model_data_str = model_data_str[:-2] + \"\\n\"\n\n        c_file = c_file.replace(\"{model_data}\", model_data_str)\n\n        os.makedirs(save_dir, exist_ok=True)\n        with open(os.path.join(save_dir, 'model.h'), 'w') as f:\n            f.write(h_file)\n        with open(os.path.join(save_dir, 'model.c'), 'w') as f:\n            f.write(c_file)\n\n        # create data files\n        data = np.load(os.path.join(eqcheck_data_dir, 'eqcheck_data.npz'))\n        data_x = data['data_x']\n        data_y = data['data_y_pred']\n        data.close()\n        assert len(data_x.shape) == 2, \"The data_x should be 2-dimensional.\"\n        assert len(data_y.shape) == 2, \"The data_y should be 2-dimensional.\"\n        assert len(data_x) == len(data_y), \"The number of samples in data_x and data_y should be the same.\"\n        assert data_y.shape[1] == 1, \"The output size should be 1.\"\n\n        with open(os.path.join(templates_dir, 'data.h'), 'r') as f:\n            h_file = f.read()\n        with open(os.path.join(templates_dir, 'data.c'), 'r') as f:\n            c_file = f.read()\n\n        h_file = h_file.replace(\"{n_samples}\", str(data_x.shape[0]))\n        h_file = h_file.replace(\"{n_features}\", str(data_x.shape[1]))\n        c_file = c_file.replace(\"{n_features}\", str(data_x.shape[1]))\n\n        data_x_str = \"\\n\"\n        for i, sample_x in enumerate(data_x):\n            data_x_str += \"\\t\" + _np_to_c(sample_x)\n            if i &lt; len(data_x) - 1:\n                data_x_str += \",\"\n            data_x_str += \"\\n\"\n\n        data_y_str = \"\\n\"\n        for i, sample_y in enumerate(data_y):\n            data_y_str += \"\\t\" + _np_to_c(sample_y)\n            if i &lt; len(data_y) - 1:\n                data_y_str += \",\"\n            data_y_str += \"\\n\"\n\n        c_file = c_file.replace(\"{samples_x}\", data_x_str)\n        c_file = c_file.replace(\"{samples_y}\", data_y_str)\n\n        os.makedirs(save_dir, exist_ok=True)\n        with open(os.path.join(save_dir, 'data.h'), 'w') as f:\n            f.write(h_file)\n        with open(os.path.join(save_dir, 'data.c'), 'w') as f:\n            f.write(c_file)\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.__init__","title":"__init__","text":"<pre><code>__init__(cfg=None)\n</code></pre> <p>Initializes the class. The following attributes should be set in the init function:     self.model (ekkono.primer.Model): The model.     self.dataset (DatasetSupervisorTemplate): The dataset.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>The configuration of the model. Defaults to None.</p> <code>None</code> Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def __init__(self, cfg=None):\n    \"\"\"\n    Initializes the class.\n    The following attributes should be set in the __init__ function:\n        self.model (ekkono.primer.Model): The model.\n        self.dataset (DatasetSupervisorTemplate): The dataset.\n\n    Args:\n        cfg (dict): The configuration of the model. Defaults to None.\n    \"\"\"\n\n    # default configs\n    self.ekkono_model_type = \"pretrained\"  # Ekkono model type. \"pretrained\" or \"incremental\". \"pretrained\" is not able to be trained on the end device, but \"incremental\" is.\n    self.denses_params = [16]       # each element is the number of neurons of a dense layer\n    self.activation = \"sigmoid\"     # the activation function of layers. It can be one of \"sigmoid\", \"tanh\", \"relu\" (which actually is leaky relu)\n    self.epochs = 50\n    self.batch_size = 32\n    self.dataset_info = {\n        \"name\": \"cifar10\",\n        \"path\": \"edgemark/models/datasets/cifar10/data.py\",\n        \"args\": {\n            \"flat_features\": True\n        }\n    }\n    self.random_seed = None\n\n    self.learning_rate = 1e-3\n    self.fine_tuning_learning_rate = 1e-4\n    self.fine_tuning_epochs = 5\n    self.fine_tuning_batch_size = 128\n\n    # update configs if provided\n    if cfg is not None:\n        self.set_configs(cfg)\n\n    # load corresponding dataset\n    spec = importlib.util.spec_from_file_location(\"imported_module\", self.dataset_info[\"path\"])\n    imported_module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(imported_module)\n\n    # load data\n    self.dataset = imported_module.DatasetSupervisor(**self.dataset_info[\"args\"])\n    assert len(self.dataset.feature_shape) == 1, \"The input shape must be 1-dimensional.\"\n    assert self.dataset.num_labels == 1, \"The output size should be 1.\"\n    assert len(self.dataset.train_x.shape) == 2, \"The train_x should be 2-dimensional.\"\n    assert len(self.dataset.train_y.shape) == 2, \"The train_y should be 2-dimensional.\"\n    assert self.dataset.train_y.shape[1] == 1, \"The output size should be 1.\"\n    assert self.dataset.output_activation == \"linear\", \"The output activation should be linear.\"\n    assert self.dataset.loss_function in [\"mean_squared_error\", \"mse\"], \"The loss function should be mean_squared_error.\"\n    for metric in self.dataset.metrics:\n        if metric not in [\"mean_squared_error\", \"mse\", \"rmse\", \"mean_absolute_error\", \"mae\"]:\n            print(\"Warning: The metric '{}' is not supported.\".format(metric))\n\n    (self.trainset, self.testset), target_name = self._create_ekkono_datasets(self.dataset)\n\n    # create the model\n    if self.ekkono_model_type == \"pretrained\":\n        self.model = self.create_pretrained_model(self.denses_params, self.activation, self.learning_rate, self.trainset.pipeline_template, target_name)\n    elif self.ekkono_model_type == \"incremental\":\n        self.model = self.create_incremental_model(self.denses_params, self.activation, self.epochs, self.batch_size, self.learning_rate, self.trainset, target_name)\n    else:\n        raise ValueError(\"The specified model type is not supported.\")\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.create_incremental_model","title":"create_incremental_model  <code>staticmethod</code>","text":"<pre><code>create_incremental_model(denses_params, activation, epochs, batch_size, learning_rate, trainset, target_name)\n</code></pre> <p>Creates the Ekkono incremental model. The model is able to be trained on the end device.</p> <p>Parameters:</p> Name Type Description Default <code>denses_params</code> <code>list</code> <p>Each element is the number of neurons of a dense layer (excluding the output layer which has one neuron).</p> required <code>activation</code> <code>str</code> <p>The activation function of the hidden layers. Can be one of \"sigmoid\", \"tanh\", \"relu\" (which actually is leaky relu).</p> required <code>epochs</code> <code>int</code> <p>The number of epochs for training.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate for training.</p> required <code>trainset</code> <code>DatasetSupervisorTemplate</code> <p>The training dataset.</p> required <code>target_name</code> <code>str</code> <p>The name of the target attribute.</p> required Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>@staticmethod\ndef create_incremental_model(denses_params, activation, epochs, batch_size, learning_rate, trainset, target_name):\n    \"\"\"\n    Creates the Ekkono incremental model. The model is able to be trained on the end device.\n\n    Args:\n        denses_params (list): Each element is the number of neurons of a dense layer (excluding the output layer which has one neuron).\n        activation (str): The activation function of the hidden layers. Can be one of \"sigmoid\", \"tanh\", \"relu\" (which actually is leaky relu).\n        epochs (int): The number of epochs for training.\n        batch_size (int): The batch size for training.\n        learning_rate (float): The learning rate for training.\n        trainset (DatasetSupervisorTemplate): The training dataset.\n        target_name (str): The name of the target attribute.\n    \"\"\"\n    activation_function = None\n    if activation == \"sigmoid\":\n        activation_function = primer.MLPModelParams.ActivationFunction.SIGMOID\n    elif activation == \"tanh\":\n        activation_function = primer.MLPModelParams.ActivationFunction.TANH\n    elif activation == \"relu\":\n        activation_function = primer.MLPModelParams.ActivationFunction.LEAKYRELU\n    else:\n        raise ValueError(\"The specified activation function is not supported.\")\n\n    pipeline_template = trainset.pipeline_template\n    pipeline_template.add_target(target_name)\n\n    edge_model = primer.ModelFactory.create_incremental_mlp_model(\n        pipeline_template,\n        hidden_layers=denses_params,                        # Crystal Tunable\n        batch_size=batch_size,                              # Crystal Tunable\n        iterations_per_batch=epochs,                        # Crystal Fixed - Edge can have an epoch greater than 1 but for Crystal it is fixed to 1\n        optimizer= primer.MLPModelParams.Optimizer.ADAM,    # Crystal Fixed - Edge can use different optimizers but Crystal uses GRADIENTDESCENT\n        start_learning_rate=learning_rate,                  # Crystal Tunable\n        end_learning_rate=learning_rate,                    # Crystal Tunable\n        decay_instances=5000,                               # Crystal Tunable\n        activation_function=activation_function,            # Crystal Tunable\n        attribute_stats=pipeline_template.instantiate(trainset).get_attribute_stats()\n    )\n\n    return edge_model\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.create_pretrained_model","title":"create_pretrained_model  <code>staticmethod</code>","text":"<pre><code>create_pretrained_model(denses_params, activation, learning_rate, pipeline_template, target_name)\n</code></pre> <p>Creates the Ekkono pretrained model. The model is not able to be trained on the end device.</p> <p>Parameters:</p> Name Type Description Default <code>denses_params</code> <code>list</code> <p>Each element is the number of neurons of a dense layer (excluding the output layer which has one neuron).</p> required <code>activation</code> <code>str</code> <p>The activation function of the hidden layers. Can be one of \"sigmoid\", \"tanh\", \"relu\" (which actually is leaky relu).</p> required <code>pipeline_template</code> <code>PipelineTemplate</code> <p>The pipeline template (usually taken from dataset).</p> required <code>target_name</code> <code>str</code> <p>The name of the target attribute.</p> required Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>@staticmethod\ndef create_pretrained_model(denses_params, activation, learning_rate, pipeline_template, target_name):\n    \"\"\"\n    Creates the Ekkono pretrained model. The model is not able to be trained on the end device.\n\n    Args:\n        denses_params (list): Each element is the number of neurons of a dense layer (excluding the output layer which has one neuron).\n        activation (str): The activation function of the hidden layers. Can be one of \"sigmoid\", \"tanh\", \"relu\" (which actually is leaky relu).\n        pipeline_template (Ekkono.primer.PipelineTemplate): The pipeline template (usually taken from dataset).\n        target_name (str): The name of the target attribute.\n    \"\"\"\n    activation_function = None\n    if activation == \"sigmoid\":\n        activation_function = primer.MLPModelParams.ActivationFunction.SIGMOID\n    elif activation == \"tanh\":\n        activation_function = primer.MLPModelParams.ActivationFunction.TANH\n    elif activation == \"relu\":\n        activation_function = primer.MLPModelParams.ActivationFunction.LEAKYRELU\n    else:\n        raise ValueError(\"The specified activation function is not supported.\")\n\n    pipeline_template.add_target(target_name)\n\n    edge_model = primer.ModelFactory.create_mlp_model(\n        pipeline_template,\n        hidden_layers=denses_params,\n        optimizer= primer.MLPModelParams.Optimizer.ADAM,\n        start_learning_rate=learning_rate,\n        end_learning_rate=learning_rate,\n        activation_function=activation_function\n    )\n\n    return edge_model\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model()\n</code></pre> <p>Evaluates the model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The evaluation metrics.</p> Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def evaluate_model(self):\n    \"\"\"\n    Evaluates the model.\n\n    Returns:\n        dict: The evaluation metrics.\n    \"\"\"\n    train_pred_tuples = primer.ModelTester.get_prediction_tuples(self.model, self.model.target_attributes[0], self.trainset)\n    train_mae = primer.ModelTester.calculate_mae(train_pred_tuples)\n    train_rmse = primer.ModelTester.calculate_rmse(train_pred_tuples)\n\n    test_pred_tuples = primer.ModelTester.get_prediction_tuples(self.model, self.model.target_attributes[0], self.testset)\n    test_mae = primer.ModelTester.calculate_mae(test_pred_tuples)\n    test_rmse = primer.ModelTester.calculate_rmse(test_pred_tuples)\n\n    output = {\n        \"train_mae\": train_mae,\n        \"train_rmse\": train_rmse,\n        \"test_mae\": test_mae,\n        \"test_rmse\": test_rmse\n    }\n    return output\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.fill_crystal_templates","title":"fill_crystal_templates","text":"<pre><code>fill_crystal_templates(save_dir, eqcheck_data_dir, templates_dir)\n</code></pre> <p>Taken from the main.h/c and data.h/c available in the templates_dir, fills their placeholders with the appropriate data and saves them to the save_dir.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>The directory where the filled files should be saved.</p> required <code>eqcheck_data_dir</code> <code>str</code> <p>The directory where the eqcheck data is stored.</p> required <code>templates_dir</code> <code>str</code> <p>The directory where the templates are stored.</p> required Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def fill_crystal_templates(self, save_dir, eqcheck_data_dir, templates_dir):\n    \"\"\"\n    Taken from the main.h/c and data.h/c available in the templates_dir, fills their\n    placeholders with the appropriate data and saves them to the save_dir.\n\n    Args:\n        save_dir (str): The directory where the filled files should be saved.\n        eqcheck_data_dir (str): The directory where the eqcheck data is stored.\n        templates_dir (str): The directory where the templates are stored.\n    \"\"\"\n    def _np_to_c(array):\n        if array.ndim == 0:\n            return str(array.item())\n        c_array = \"{\" + \", \".join(_np_to_c(subarray) for subarray in array) + \"}\"\n        return c_array\n\n    # create model files\n    with open(os.path.join(templates_dir, 'model.h'), 'r') as f:\n        h_file = f.read()\n    with open(os.path.join(templates_dir, 'model.c'), 'r') as f:\n        c_file = f.read()\n\n    h_file = h_file.replace(\"{model_type}\", self.ekkono_model_type.upper())\n    h_file = h_file.replace(\"{arena_size}\", str(self._estimate_arena_size()))\n    h_file = h_file.replace(\"{input_size}\", str(self.dataset.feature_shape[0]))\n\n    crystal_model_data = primer.Converter.convert(self.model, primer.Converter.ConversionTarget.CRYSTAL)\n\n    c_file = c_file.replace(\"{model_data_size}\", str(len(crystal_model_data)))\n\n    model_data_str = \"\"\n    for i, byte in enumerate(crystal_model_data):\n        if i % 16 == 0:\n            model_data_str += \"\\n\\t\"\n        model_data_str += \"0x{:02x}, \".format(byte)\n    model_data_str = model_data_str[:-2] + \"\\n\"\n\n    c_file = c_file.replace(\"{model_data}\", model_data_str)\n\n    os.makedirs(save_dir, exist_ok=True)\n    with open(os.path.join(save_dir, 'model.h'), 'w') as f:\n        f.write(h_file)\n    with open(os.path.join(save_dir, 'model.c'), 'w') as f:\n        f.write(c_file)\n\n    # create data files\n    data = np.load(os.path.join(eqcheck_data_dir, 'eqcheck_data.npz'))\n    data_x = data['data_x']\n    data_y = data['data_y_pred']\n    data.close()\n    assert len(data_x.shape) == 2, \"The data_x should be 2-dimensional.\"\n    assert len(data_y.shape) == 2, \"The data_y should be 2-dimensional.\"\n    assert len(data_x) == len(data_y), \"The number of samples in data_x and data_y should be the same.\"\n    assert data_y.shape[1] == 1, \"The output size should be 1.\"\n\n    with open(os.path.join(templates_dir, 'data.h'), 'r') as f:\n        h_file = f.read()\n    with open(os.path.join(templates_dir, 'data.c'), 'r') as f:\n        c_file = f.read()\n\n    h_file = h_file.replace(\"{n_samples}\", str(data_x.shape[0]))\n    h_file = h_file.replace(\"{n_features}\", str(data_x.shape[1]))\n    c_file = c_file.replace(\"{n_features}\", str(data_x.shape[1]))\n\n    data_x_str = \"\\n\"\n    for i, sample_x in enumerate(data_x):\n        data_x_str += \"\\t\" + _np_to_c(sample_x)\n        if i &lt; len(data_x) - 1:\n            data_x_str += \",\"\n        data_x_str += \"\\n\"\n\n    data_y_str = \"\\n\"\n    for i, sample_y in enumerate(data_y):\n        data_y_str += \"\\t\" + _np_to_c(sample_y)\n        if i &lt; len(data_y) - 1:\n            data_y_str += \",\"\n        data_y_str += \"\\n\"\n\n    c_file = c_file.replace(\"{samples_x}\", data_x_str)\n    c_file = c_file.replace(\"{samples_y}\", data_y_str)\n\n    os.makedirs(save_dir, exist_ok=True)\n    with open(os.path.join(save_dir, 'data.h'), 'w') as f:\n        f.write(h_file)\n    with open(os.path.join(save_dir, 'data.c'), 'w') as f:\n        f.write(c_file)\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.get_model_info","title":"get_model_info","text":"<pre><code>get_model_info()\n</code></pre> <p>Returns the model info.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The model info.</p> Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def get_model_info(self):\n    \"\"\"\n    Returns the model info.\n\n    Returns:\n        dict: The model info.\n    \"\"\"\n    model_info = {\n        \"ekkono_model_type\": self.ekkono_model_type,\n        \"denses_params\": self.denses_params,\n        \"activation\": self.activation,\n        \"epochs\": self.epochs,\n        \"batch_size\": self.batch_size,\n        \"learning_rate\": self.learning_rate,\n        \"fine_tuning_epochs\": self.fine_tuning_epochs,\n        \"fine_tuning_batch_size\": self.fine_tuning_batch_size,\n        \"fine_tuning_learning_rate\": self.fine_tuning_learning_rate,\n        \"dataset\": self.dataset_info,\n        \"random_seed\": self.random_seed\n    }\n    return model_info\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.load_eqcheck_data","title":"load_eqcheck_data  <code>staticmethod</code>","text":"<pre><code>load_eqcheck_data(load_dir)\n</code></pre> <p>Loads the eqcheck data.</p> <p>Parameters:</p> Name Type Description Default <code>load_dir</code> <code>str</code> <p>The directory where the eqcheck data is stored.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(data_x, data_y_pred)</p> Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>@staticmethod\ndef load_eqcheck_data(load_dir):\n    \"\"\"\n    Loads the eqcheck data.\n\n    Args:\n        load_dir (str): The directory where the eqcheck data is stored.\n\n    Returns:\n        tuple: (data_x, data_y_pred)\n    \"\"\"\n    eqcheck_data = np.load(os.path.join(load_dir, 'eqcheck_data.npz'))\n    data_x = eqcheck_data['data_x']\n    data_y_pred = eqcheck_data['data_y_pred']\n    return data_x, data_y_pred\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.load_model","title":"load_model","text":"<pre><code>load_model(load_dir)\n</code></pre> <p>Loads the model.</p> <p>Parameters:</p> Name Type Description Default <code>load_dir</code> <code>str</code> <p>The parent directory where the SavedModel format is stored in.</p> required Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def load_model(self, load_dir):\n    \"\"\"\n    Loads the model.\n\n    Args:\n        load_dir (str): The parent directory where the SavedModel format is stored in.\n    \"\"\"\n    if self.ekkono_model_type == \"pretrained\":\n        self.model = primer.PredictiveModel.load(os.path.join(load_dir, \"model.edge\"))\n    elif self.ekkono_model_type == \"incremental\":\n        self.model = primer.IncrementalPredictiveModel.load(os.path.join(load_dir, \"model.edge\"))\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.log_model_to_wandb","title":"log_model_to_wandb  <code>staticmethod</code>","text":"<pre><code>log_model_to_wandb(model_dir, model_save_name)\n</code></pre> <p>Logs the model to wandb.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>The directory where the model is stored.</p> required <code>model_save_name</code> <code>str</code> <p>The name that will be assigned to the model artifact.</p> required Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>@staticmethod\ndef log_model_to_wandb(model_dir, model_save_name):\n    \"\"\"\n    Logs the model to wandb.\n\n    Args:\n        model_dir (str): The directory where the model is stored.\n        model_save_name (str): The name that will be assigned to the model artifact.\n    \"\"\"\n    model_path = os.path.join(model_dir, \"model.edge\")\n    model_artifact = wandb.Artifact(model_save_name, type=\"model\")\n    model_artifact.add_file(model_path)\n    wandb.log_artifact(model_artifact)\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.measure_execution_time","title":"measure_execution_time","text":"<pre><code>measure_execution_time()\n</code></pre> <p>Measures the execution time of the model.</p> <p>Returns:</p> Type Description <code>float</code> <p>The execution time in ms.</p> Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def measure_execution_time(self):\n    \"\"\"\n    Measures the execution time of the model.\n\n    Returns:\n        float: The execution time in ms.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    sample_idx = rng.randint(0, self.dataset.train_x.shape[0])\n    x = np.array(self.dataset.train_x[sample_idx])\n\n    crystal_model_data = primer.Converter.convert(self.model, primer.Converter.ConversionTarget.CRYSTAL)\n    if self.ekkono_model_type == \"pretrained\":\n        crystal_model = crystal.load_predictive_model(crystal_model_data)\n    elif self.ekkono_model_type == \"incremental\":\n        crystal_model = crystal.load_incremental_predictive_model(crystal_model_data)\n\n    # warm up\n    tic = time.time()\n    for i in range(10000):\n        crystal_model.predict(x)\n    toc = time.time()\n    itr = int(10 * 10000 / (toc - tic))\n\n    # run the test\n    tic = time.time()\n    for i in range(itr):\n        crystal_model.predict(x)\n    toc = time.time()\n    execution_time = (toc-tic)/itr*1000     # in ms\n\n    return execution_time\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.save_eqcheck_data","title":"save_eqcheck_data","text":"<pre><code>save_eqcheck_data(n_samples, save_dir)\n</code></pre> <p>Saves the eqcheck data as {\"data_x\", \"data_y_pred\"}.</p> <p>The data_x has shape (samples, input_shape) and data_y_pred has shape (samples, output_shape).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to be saved</p> required <code>save_dir</code> <code>str</code> <p>The directory where the data should be saved</p> required Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def save_eqcheck_data(self, n_samples, save_dir):\n    \"\"\"\n    Saves the eqcheck data as {\"data_x\", \"data_y_pred\"}.\n\n    The data_x has shape (samples, *input_shape) and data_y_pred has shape (samples, *output_shape).\n\n    Args:\n        n_samples (int): The number of samples to be saved\n        save_dir (str): The directory where the data should be saved\n    \"\"\"\n    data_x = self.dataset.train_x[:n_samples]\n    data_y_pred = []\n    for i in range(len(data_x)):\n        instance = data_x[i].tolist() + [0]  # the last element is a placeholder for the output\n        data_y_pred.append(self.model.predict(instance))\n    data_y_pred = np.array(data_y_pred)\n\n    os.makedirs(save_dir, exist_ok=True)\n    np.savez(os.path.join(save_dir, 'eqcheck_data.npz'), data_x=data_x, data_y_pred=data_y_pred)\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.save_model","title":"save_model","text":"<pre><code>save_model(save_dir)\n</code></pre> <p>Saves the model.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>The directory where the model should be saved in.</p> required Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def save_model(self, save_dir):\n    \"\"\"\n    Saves the model.\n\n    Args:\n        save_dir (str): The directory where the model should be saved in.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    self.model.save(os.path.join(save_dir, \"model.edge\"))\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.save_model_info","title":"save_model_info  <code>staticmethod</code>","text":"<pre><code>save_model_info(model_info, save_dir)\n</code></pre> <p>Saves the model info.</p> <p>Parameters:</p> Name Type Description Default <code>model_info</code> <code>dict</code> <p>The model info.</p> required <code>save_dir</code> <code>str</code> <p>The directory where the model info should be saved.</p> required Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>@staticmethod\ndef save_model_info(model_info, save_dir):\n    \"\"\"\n    Saves the model info.\n\n    Args:\n        model_info (dict): The model info.\n        save_dir (str): The directory where the model info should be saved.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    yaml.Dumper.ignore_aliases = lambda *args : True\n    with open(os.path.join(save_dir, \"model_info.yaml\"), 'w') as f:\n        yaml.dump(model_info, f, indent=4, sort_keys=False)\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.set_configs","title":"set_configs","text":"<pre><code>set_configs(cfg)\n</code></pre> <p>Sets the configurations of the model.</p> <p>Note: The changed configs won't affect the data, model, or any other loaded attributes. In case you want to change them, you should call the corresponding functions.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>The configuration.</p> required Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def set_configs(self, cfg):\n    \"\"\"\n    Sets the configurations of the model.\n\n    Note: The changed configs won't affect the data, model, or any other loaded attributes.\n    In case you want to change them, you should call the corresponding functions.\n\n    Args:\n        cfg (dict): The configuration.\n    \"\"\"\n    if \"model_type\" in cfg:\n        if cfg[\"model_type\"] is not None and cfg[\"model_type\"] != \"CNN\":\n            raise ValueError(\"The model type should be 'CNN'.\")\n    if \"ekkono_model_type\" in cfg:\n        self.ekkono_model_type = cfg[\"ekkono_model_type\"]\n    if \"convs_params\" in cfg:\n        if cfg[\"convs_params\"] is not None and cfg[\"convs_params\"] != []:\n            raise ValueError(\"The model doesn't support convolutional layers.\")\n    if \"denses_params\" in cfg:\n        self.denses_params = cfg[\"denses_params\"]\n    if \"convs_dropout\" in cfg:\n        if cfg[\"convs_dropout\"] is not None and cfg[\"convs_dropout\"] != 0.00:\n            raise ValueError(\"Dropout is not supported in the model.\")\n    if \"denses_dropout\" in cfg:\n        if cfg[\"denses_dropout\"] is not None and cfg[\"denses_dropout\"] != 0.00:\n            raise ValueError(\"Dropout is not supported in the model.\")\n    if \"activation\" in cfg:\n        self.activation = cfg[\"activation\"]\n    if \"use_batchnorm\" in cfg:\n        if cfg[\"use_batchnorm\"] is not None and cfg[\"use_batchnorm\"] is not False:\n            raise ValueError(\"Batch normalization is not supported in the model.\")\n    if \"epochs\" in cfg:\n        self.epochs = cfg[\"epochs\"]\n    if \"batch_size\" in cfg:\n        self.batch_size = cfg[\"batch_size\"]\n    if \"dataset\" in cfg:\n        self.dataset_info = cfg[\"dataset\"]\n    if \"random_seed\" in cfg:\n        if cfg[\"random_seed\"] is not None:\n            print(\"Warning: Ekkono API doesn't let us to control its random operations. The random seed is ignored.\")\n    if \"learning_rate\" in cfg:\n        self.learning_rate = cfg[\"learning_rate\"]\n    if \"fine_tuning_learning_rate\" in cfg:\n        self.fine_tuning_learning_rate = cfg[\"fine_tuning_learning_rate\"]\n    if \"fine_tuning_epochs\" in cfg:\n        self.fine_tuning_epochs = cfg[\"fine_tuning_epochs\"]\n    if \"fine_tuning_batch_size\" in cfg:\n        self.fine_tuning_batch_size = cfg[\"fine_tuning_batch_size\"]\n</code></pre>"},{"location":"api/platforms/ekkono/#edgemark.models.platforms.Ekkono.model.ModelSupervisor.train_model","title":"train_model","text":"<pre><code>train_model()\n</code></pre> <p>Trains the model.</p> Source code in <code>edgemark/models/platforms/Ekkono/model.py</code> <pre><code>def train_model(self):\n    \"\"\"\n    Trains the model.\n    \"\"\"\n    primer.ModelTrainer.train(self.model, self.trainset, batch_size=self.batch_size, epochs=self.epochs)\n</code></pre>"},{"location":"api/platforms/tensorflow/","title":"TensorFlow","text":""},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_generator","title":"edgemark.models.platforms.TensorFlow.model_generator","text":""},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_generator.main","title":"main","text":"<pre><code>main(cfg_path=config_file_path, **kwargs)\n</code></pre> <p>Generate, train, evaluate, and save models based on the given configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_path</code> <code>str</code> <p>The path to the configuration file containing the model generation parameters. The configuration file that this path points to should contain the following keys:     - model_type (str): A placeholder for the model type. This will be populated by the target model configuration.     - time_tag (str): A placeholder for the time tag. This will be populated by the current time.     - target_models_dir (str): Path to the directory containing the target models configurations.     - datasets_dir (str): Path to the directory containing the datasets.     - linkers_dir (str): Path to the directory where the generated models list will be saved.     - model_path (str): Path to the model file.     - model_save_dir (str): Path to the directory where the generated model will be saved.     - data_save_dir (str): Path to the directory where the representative and equality check data will be saved.     - TFLM_info_save_path (str): Path to the file where the TFLM info will be saved.     - wandb_online (bool): Flag to enable or disable the W&amp;B online mode.     - wandb_project_name (str): Name of the W&amp;B project.     - train_models (bool): Flag to enable or disable model training.     - evaluate_models (bool): Flag to enable or disable model evaluation.     - measure_execution_time (bool): Flag to enable or disable the measurement of execution time.     - epochs (int): Number of epochs for training the model. If specified, it will override the number of epochs in the model configuration.     - n_representative_data (int): Number of samples to be saved for TFLite conversion.     - n_eqcheck_data (int): Number of samples to be saved for equivalence check of the model on PC and MCU.</p> <code>config_file_path</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments to be passed to the configuration file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing the following keys for each target model: - name (str): Name of the target model configuration file. - result (str): Result of the model generation. It can be either \"success\" or \"failed\". - error (str): Error message in case of failure. - traceback (str): Traceback in case of failure.</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_generator.py</code> <pre><code>def main(cfg_path=config_file_path, **kwargs):\n    \"\"\"\n    Generate, train, evaluate, and save models based on the given configuration file.\n\n    Args:\n        cfg_path (str): The path to the configuration file containing the model generation parameters.\n            The configuration file that this path points to should contain the following keys:\n                - model_type (str): A placeholder for the model type. This will be populated by the target model configuration.\n                - time_tag (str): A placeholder for the time tag. This will be populated by the current time.\n                - target_models_dir (str): Path to the directory containing the target models configurations.\n                - datasets_dir (str): Path to the directory containing the datasets.\n                - linkers_dir (str): Path to the directory where the generated models list will be saved.\n                - model_path (str): Path to the model file.\n                - model_save_dir (str): Path to the directory where the generated model will be saved.\n                - data_save_dir (str): Path to the directory where the representative and equality check data will be saved.\n                - TFLM_info_save_path (str): Path to the file where the TFLM info will be saved.\n                - wandb_online (bool): Flag to enable or disable the W&amp;B online mode.\n                - wandb_project_name (str): Name of the W&amp;B project.\n                - train_models (bool): Flag to enable or disable model training.\n                - evaluate_models (bool): Flag to enable or disable model evaluation.\n                - measure_execution_time (bool): Flag to enable or disable the measurement of execution time.\n                - epochs (int): Number of epochs for training the model. If specified, it will override the number of epochs in the model configuration.\n                - n_representative_data (int): Number of samples to be saved for TFLite conversion.\n                - n_eqcheck_data (int): Number of samples to be saved for equivalence check of the model on PC and MCU.\n        **kwargs (dict): Keyword arguments to be passed to the configuration file.\n\n    Returns:\n        list: A list of dictionaries containing the following keys for each target model:\n            - name (str): Name of the target model configuration file.\n            - result (str): Result of the model generation. It can be either \"success\" or \"failed\".\n            - error (str): Error message in case of failure.\n            - traceback (str): Traceback in case of failure.\n    \"\"\"\n    cfg = OmegaConf.load(cfg_path)\n    cfg.update(OmegaConf.create(kwargs))\n\n    if not cfg.wandb_online:\n        os.environ['WANDB_MODE'] = 'offline'\n    models_list = []\n\n    target_files = find_target_files(cfg.target_models_dir)\n\n    output = [{\"name\": os.path.splitext(target_file)[0]} for target_file in target_files]\n\n    for i, target_file in enumerate(target_files):\n        try:\n            model_cfg_path = os.path.join(cfg.target_models_dir, target_file)\n            model_cfg = OmegaConf.load(model_cfg_path)\n            cfg.model_type = model_cfg.model_type\n            cfg.time_tag = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n            if \"epochs\" in cfg:\n                model_cfg.epochs = cfg.epochs\n            if \"dataset\" in model_cfg:\n                model_cfg.dataset.path = os.path.join(cfg.datasets_dir, model_cfg.dataset.name, \"data.py\")\n\n            wandb_name = datetime.datetime.strptime(cfg.time_tag, \"%Y-%m-%d_%H-%M-%S\").strftime(\"%Y-%m-%d %H:%M:%S\")\n            wandb_dir = get_abs_path(os.path.join(cfg.model_save_dir, 'tf'))\n            os.makedirs(wandb_dir, exist_ok=True)\n            wandb.init(project=cfg.wandb_project_name, group=model_cfg.model_type, tags=[model_cfg.model_type, model_cfg.dataset.name, os.path.splitext(target_file)[0]], name=wandb_name, dir=wandb_dir)\n\n            spec = importlib.util.spec_from_file_location(\"imported_module\", cfg.model_path)\n            imported_module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(imported_module)\n\n            title = \"Creating the {} model described in {} ({}/{})\".format(model_cfg.model_type, target_file, i+1, len(target_files))\n            print(\"\\n\")\n            print(\"=\"*80)\n            print(\"-\"*((80-len(title)-2)//2), end=\" \")\n            print(title, end=\" \")\n            print(\"-\"*((80-len(title)-2)//2))\n            print(\"=\"*80)\n\n            supervisor = imported_module.ModelSupervisor(OmegaConf.to_container(model_cfg, resolve=True))\n\n            print(\"Saving representative data to the directory: {} ...\".format(cfg.data_save_dir), end=\" \", flush=True)\n            supervisor.save_representative_data(cfg.n_representative_data, cfg.data_save_dir)\n            print(\"Done\\n\")\n\n            try:\n                supervisor.compile_model(fine_tuning=False)\n            except Exception as e:\n                print(\"Error in compiling the model: {}\".format(e))\n                print(\"Continuing without compilation\")\n\n            # print(\"Model summary:\")\n            # supervisor.model.summary()\n            # print(\"\")\n            total_params, trainable_params, non_trainable_params = supervisor.get_params_count()\n            MACs = supervisor.get_FLOPs() // 2\n\n            if cfg.train_models:\n                print(\"Training the model ...\")\n                tensorboard_log_dir = os.path.join(cfg.model_save_dir, 'tf/logs')\n                best_weights_dir = os.path.join(cfg.model_save_dir, 'tf/weights/weights_best')\n                supervisor.train_model(fine_tuning=False, tensorboard_log_dir=tensorboard_log_dir, best_weights_dir=best_weights_dir, use_wandb=True)\n                print(\"\")\n\n            evaluation_result = None\n            if cfg.evaluate_models:\n                try:\n                    evaluation_result = supervisor.evaluate_model()\n                    for metric, value in evaluation_result.items():\n                        print(metric, \":\", value)\n                    print(\"\")\n                except Exception as e:\n                    print(\"Error in evaluating the model: {}\".format(e))\n                    print(\"Continuing without evaluation\")\n\n            print(\"Saving model and weights to the directory: {} ...\".format(cfg.model_save_dir), end=\" \", flush=True)\n            supervisor.save_model(os.path.join(cfg.model_save_dir, \"tf/model\"))\n            supervisor.save_weights(os.path.join(cfg.model_save_dir, 'tf/weights/weights_last'))\n            print(\"Done\\n\")\n            supervisor.log_model_to_wandb(os.path.join(cfg.model_save_dir, \"tf/model\"), os.path.splitext(target_file)[0].replace(\"/\", \"_\"))\n\n            print(\"Saving equality check data to the directory: {} ...\".format(cfg.data_save_dir), end=\" \", flush=True)\n            supervisor.save_eqcheck_data(cfg.n_eqcheck_data, cfg.data_save_dir)\n            print(\"Done\\n\")\n\n            if cfg.measure_execution_time:\n                print(\"Measuring execution time ...\")\n                execution_time = supervisor.measure_execution_time()\n                print(\"Average run time: {} ms\\n\".format(execution_time))\n\n            model_info = {\"Description\": \"\"}\n            model_info[\"setting_file\"] = target_file\n            model_info[\"model_type\"] = model_cfg.model_type\n            model_info[\"trained\"] = cfg.train_models\n            model_info.update(supervisor.get_model_info())\n            model_info[\"total_params\"] = total_params\n            model_info[\"trainable_params\"] = trainable_params\n            model_info[\"non_trainable_params\"] = non_trainable_params\n            model_info[\"MACs\"] = MACs\n            if evaluation_result is not None:\n                for metric, value in evaluation_result.items():\n                    if not isinstance(metric, str):\n                        metric = str(metric)\n                    model_info[metric] = value\n            if cfg.measure_execution_time:\n                model_info[\"execution_time\"] = execution_time\n            model_info[\"wandb_name\"] = wandb_name\n\n            print(\"Saving the model info in the directory: {} ...\".format(cfg.model_save_dir), end=\" \", flush=True)\n            supervisor.save_model_info(model_info, cfg.model_save_dir)\n            print(\"Done\\n\")\n            wandb.config.update(model_info)\n\n            try:\n                print(\"Saving the TFLM info in: {} ...\".format(cfg.TFLM_info_save_path), end=\" \", flush=True)\n                supervisor.save_TFLM_info(cfg.TFLM_info_save_path)\n                print(\"Done\\n\")\n            except Exception as e:\n                print(\"Error in saving the TFLM info: {}\".format(e))\n                print(\"TFLM info will not be saved. Please fix this issue if you want to use the TFLM converter later.\")\n\n            models_list.append(cfg.model_save_dir)\n\n            wandb.finish()\n\n            output[i][\"result\"] = \"success\"\n\n        except Exception as e:\n            output[i][\"result\"] = \"failed\"\n            output[i][\"error\"] = type(e).__name__\n            output[i][\"traceback\"] = traceback.format_exc()\n            print(\"Error in generating the model:\")\n            print(traceback.format_exc())\n\n    print(\"Saving the generated models list in the directory: {} ...\".format(cfg.linkers_dir), end=\" \", flush=True)\n    save_models_list(models_list, cfg.linkers_dir)\n    print(\"Done\\n\")\n\n    return output\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_generator.save_models_list","title":"save_models_list","text":"<pre><code>save_models_list(models_list, save_dir)\n</code></pre> <p>Saves the list of generated models.</p> <p>Parameters:</p> Name Type Description Default <code>models_list</code> <code>list</code> <p>The list of generated models.</p> required <code>save_dir</code> <code>str</code> <p>The directory where the models list should be saved.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_generator.py</code> <pre><code>def save_models_list(models_list, save_dir):\n    \"\"\"\n    Saves the list of generated models.\n\n    Args:\n        models_list (list): The list of generated models.\n        save_dir (str): The directory where the models list should be saved.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    with open(os.path.join(save_dir, \"tf_generated_models_list.yaml\"), 'w') as f:\n        yaml.dump(models_list, f, indent=4, sort_keys=False)\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template","title":"edgemark.models.platforms.TensorFlow.model_template","text":"<p>This module contains a template class that other models should inherit from.</p>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate","title":"ModelSupervisorTemplate","text":"<p>This class is a template for TensorFlow models. In order to create a new model, you should inherit from this class and implement its abstract functions.</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>class ModelSupervisorTemplate:\n    \"\"\"\n    This class is a template for TensorFlow models. In order to create a new model,\n    you should inherit from this class and implement its abstract functions.\n    \"\"\"\n\n    def __init__(self, cfg=None):\n        \"\"\"\n        Initializes the class.\n        The following attributes should be set in the __init__ function:\n            self.model (tf.keras.Model): The model.\n            self.dataset (DatasetSupervisorTemplate): The dataset.\n\n        Args:\n            cfg (dict): The configurations of the model. Defaults to None.\n        \"\"\"\n        self.model = None\n        self.dataset = None\n\n\n    def set_configs(self, cfg):\n        \"\"\"\n        Sets the configs from the given dictionary.\n\n        Note: The changed configs won't affect the data, model, or any other loaded attributes.\n        In case you want to change them, you should call the corresponding functions.\n\n        Args:\n            cfg (dict): The configurations.\n        \"\"\"\n        raise NotImplementedError\n\n\n    # Optional function: Whether you implement this function or not depends on your application.\n    def compile_model(self, fine_tuning=False):\n        \"\"\"\n        Compiles the model.\n\n        Args:\n            fine_tuning (bool, optional): If True, the model will be compiled for fine-tuning. Defaults to False.\n        \"\"\"\n        raise NotImplementedError\n\n\n    # Optional function: Whether you implement this function or not depends on your application.\n    def train_model(self, fine_tuning=False, tensorboard_log_dir=None, best_weights_dir=None, use_wandb=False):\n        \"\"\"\n        Trains the model.\n\n        Args:\n            fine_tuning (bool, optional): If True, the model will be trained for fine-tuning. Defaults to False.\n            tensorboard_log_dir (str, optional): The directory where the logs should be saved. If None, the logs won't be saved. Defaults to None.\n            best_weights_dir (str, optional): The directory where the best weights should be saved. If None, the best weights won't be saved. Defaults to None.\n            use_wandb (bool, optional): If True, the training progress will be logged to W&amp;B. Defaults to False.\n\n        Returns:\n            Optional[tf.keras.callbacks.History]: The training history or None.\n        \"\"\"\n        raise NotImplementedError\n\n\n    # Optional function: Whether you implement this function or not depends on your application.\n    def evaluate_model(self):\n        \"\"\"\n        Evaluates the model.\n\n        Returns:\n            dict: The evaluation metrics.\n        \"\"\"\n        raise NotImplementedError\n\n\n    def get_model_info(self):\n        \"\"\"\n        Returns the model info that can be anything important, including its configuration.\n\n        Returns:\n            dict: The model info.\n        \"\"\"\n        raise NotImplementedError\n\n\n    def get_params_count(self):\n        \"\"\"\n        Returns the number of parameters in the model.\n\n        Returns:\n            list[int]: The total number of parameters, the number of trainable parameters, and the number of non-trainable parameters.\n        \"\"\"\n        total_params = 0\n        trainable_params = 0\n        non_trainable_params = 0\n\n        for layer in self.model.variables:\n            total_params += np.prod(layer.shape)\n\n        for layer in self.model.trainable_variables:\n            trainable_params += np.prod(layer.shape)\n\n        non_trainable_params = total_params - trainable_params\n\n        return int(total_params), int(trainable_params), int(non_trainable_params)\n\n\n    def get_FLOPs(self):\n        \"\"\"\n        Returns the number of FLOPs of the model.\n\n        Returns:\n            int: The number of FLOPs.\n        \"\"\"\n        input_signature = [\n            tf.TensorSpec(\n                shape=(1, *params.shape[1:]),\n                dtype=params.dtype,\n                name=params.name\n            ) for params in self.model.inputs\n        ]\n        forward_graph = tf.function(self.model, input_signature).get_concrete_function().graph\n        options = option_builder.ProfileOptionBuilder.float_operation()\n        options['output'] = 'none'\n        graph_info = model_analyzer.profile(forward_graph, options=options)\n\n        FLOPs = graph_info.total_float_ops\n\n        return FLOPs\n\n\n    def measure_execution_time(self):\n        \"\"\"\n        Measures the execution time of the model.\n        The process starts by a warm-up phase for 100 iterations, then the execution time is measured for ~10 seconds.\n\n        Returns:\n            float: The execution time in ms.\n        \"\"\"\n        rng = np.random.RandomState(42)\n        sample_idx = rng.randint(0, self.dataset.train_x.shape[0])\n        x = np.array([self.dataset.train_x[sample_idx]])\n\n        # warm up\n        tic = time.time()\n        for i in range(100):\n            self.model(x, training=False)\n        toc = time.time()\n        itr = int(10 * 100 / (toc - tic))\n\n        # run the test\n        tic = time.time()\n        for i in range(itr):\n            self.model(x, training=False)\n        toc = time.time()\n        execution_time = (toc-tic)/itr*1000     # in ms\n\n        return execution_time\n\n\n    def save_representative_data(self, n_samples, save_dir):\n        \"\"\"\n        Saves the representative data with shape (samples, *input_shape).\n\n        Args:\n            n_samples (int): The number of samples to be saved.\n            save_dir (str): The directory where the data should be saved.\n        \"\"\"\n        data_x = self.dataset.train_x[:n_samples]\n        os.makedirs(save_dir, exist_ok=True)\n        np.save(os.path.join(save_dir, 'representative_data.npy'), data_x)\n\n\n    # Optional function\n    @staticmethod\n    def load_representative_data(load_dir):\n        \"\"\"\n        Loads the representative data.\n\n        Args:\n            load_dir (str): The directory where the representative data is stored.\n\n        Returns:\n            numpy.ndarray: The representative data.\n        \"\"\"\n        representative_data = np.load(os.path.join(load_dir, 'representative_data.npy'))\n        return representative_data\n\n\n    def save_eqcheck_data(self, n_samples, save_dir):\n        \"\"\"\n        Saves the eqcheck data as {\"data_x\", \"data_y_pred\"}.\n\n        The data_x has shape (samples, *input_shape) and data_y_pred has shape (samples, *output_shape).\n\n        Args:\n            n_samples (int): The number of samples to be saved\n            save_dir (str): The directory where the data should be saved\n        \"\"\"\n        # sanity check (useful for when deploying the model using TFLM)\n        for data_dim_size, model_dim_size in zip(self.dataset.train_x.shape[1:], self.model.inputs[0].shape[1:]):\n            if data_dim_size != model_dim_size and model_dim_size is not None:\n                raise ValueError(\"The shape of the train_x doesn't match the input shape of the model.\")\n\n        data_x = self.dataset.train_x[:n_samples]\n        data_y_pred = self.model.predict(data_x, verbose=0)\n\n        np.savez(os.path.join(save_dir, 'eqcheck_data.npz'), data_x=data_x, data_y_pred=data_y_pred)\n\n\n    # Optional function\n    @staticmethod\n    def load_eqcheck_data(load_dir):\n        \"\"\"\n        Loads the eqcheck data.\n\n        Args:\n            load_dir (str): The directory where the eqcheck data is stored.\n\n        Returns:\n            tuple: (data_x, data_y_pred)\n        \"\"\"\n        eqcheck_data = np.load(os.path.join(load_dir, 'eqcheck_data.npz'))\n        data_x = eqcheck_data['data_x']\n        data_y_pred = eqcheck_data['data_y_pred']\n        return data_x, data_y_pred\n\n\n    def save_model(self, save_dir):\n        \"\"\"\n        Saves the model in two formats: Keras and SavedModel.\n\n        Args:\n            save_dir (str): The directory where the model should be saved in.\n        \"\"\"\n        # save the model as a Keras format\n        os.makedirs(os.path.join(save_dir, \"keras_format\"), exist_ok=True)\n        self.model.save(os.path.join(save_dir, \"keras_format/model.keras\"))\n\n        # save the model as a SavedModel format\n        os.makedirs(os.path.join(save_dir, \"saved_model_format\"), exist_ok=True)\n        self.model.save(os.path.join(save_dir, \"saved_model_format\"))\n\n\n    def load_model(self, load_dir):\n        \"\"\"\n        Loads the model in the SavedModel format.\n\n        Args:\n            load_dir (str): The parent directory where the SavedModel format is stored in.\n        \"\"\"\n        self.model = tf.keras.models.load_model(os.path.join(load_dir, \"saved_model_format\"))\n\n\n    @staticmethod\n    def log_model_to_wandb(model_dir, model_save_name):\n        \"\"\"\n        Logs the model to W&amp;B.\n\n        Args:\n            model_dir (str): The directory where the model is stored.\n            model_save_name (str): The name that will be assigned to the model artifact.\n        \"\"\"\n        model_path = os.path.join(model_dir, \"keras_format/model.keras\")\n        model_artifact = wandb.Artifact(model_save_name, type=\"model\")\n        model_artifact.add_file(model_path)\n        wandb.log_artifact(model_artifact)\n\n\n    def save_weights(self, save_dir):\n        \"\"\"\n        Saves the model weights.\n\n        Args:\n            save_dir (str): The directory where the model weights should be saved.\n        \"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n        self.model.save_weights(os.path.join(save_dir, \"weights\"))\n\n\n    def load_weights(self, load_dir):\n        \"\"\"\n        Loads the model weights.\n\n        Args:\n            load_dir (str): The directory where the model weights are stored.\n        \"\"\"\n        self.model.load_weights(os.path.join(load_dir, \"weights\"))\n\n\n    @staticmethod\n    def save_model_info(model_info, save_dir):\n        \"\"\"\n        Saves the model info.\n\n        Args:\n            model_info (dict): The model info.\n            save_dir (str): The directory where the model info should be saved.\n        \"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n        yaml.Dumper.ignore_aliases = lambda *args : True\n        with open(os.path.join(save_dir, \"model_info.yaml\"), 'w') as f:\n            yaml.dump(model_info, f, indent=4, sort_keys=False)\n\n\n    def save_TFLM_info(self, save_path):\n        \"\"\"\n        Saves the information required by the TFLM converter as a YAML file.\n        This is to help TFLM converter in a later stage.\n\n        Args:\n            save_path (str): The YAML file path where the TFLM info should be saved.\n        \"\"\"\n        TFLM_info = {}\n\n        arena_size_base = self._estimate_arena_size()\n        TFLM_info[\"arena_size\"] = {\n            \"32bit\": arena_size_base,\n            \"16bit\": arena_size_base//2,\n            \"8bit\": arena_size_base//4\n        }\n\n        TFLM_info[\"input_dims\"] = [int(dim) for dim in self.model.inputs[0].shape[1:]]\n        TFLM_info[\"output_dims\"] = [int(dim) for dim in self.model.outputs[0].shape[1:]]\n\n        TFLM_info[\"op_resolver_funcs\"] = None\n        try:\n            TFLM_info[\"op_resolver_funcs\"] = self._get_op_resolver_funcs()\n        except NotImplementedError:\n            print(\"The model doesn't have an implementation for the _get_op_resolver_funcs function. The placeholders should be filled manually.\")\n\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        with open(save_path, 'w') as f:\n            yaml.dump(TFLM_info, f, indent=4, sort_keys=False)\n\n\n    def _get_training_callbacks(self, tensorboard_log_dir=None, best_weights_dir=None, use_wandb=False):\n        \"\"\"\n        Returns the training callbacks.\n\n        Args:\n            tensorboard_log_dir (str, optional): The directory where the logs should be saved. If None, the logs won't be saved. Defaults to None.\n            best_weights_dir (str, optional): The directory where the best weights should be saved. If None, the best weights won't be saved. Defaults to None.\n            use_wandb (bool, optional): If True, the training progress will be logged to wandb. Defaults to False.\n        \"\"\"\n        callbacks = []\n\n        if tensorboard_log_dir is not None:\n            callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=tensorboard_log_dir, histogram_freq=1))\n\n        if best_weights_dir is not None:\n            best_weights_path = os.path.join(best_weights_dir, \"weights\")\n            callbacks.append(tf.keras.callbacks.ModelCheckpoint(best_weights_path, save_best_only=True, save_weights_only=True, verbose=0))\n\n        if use_wandb:\n            callbacks.append(WandbCallback(save_model=False, log_weights=True, compute_flops=True))\n\n        return callbacks\n\n\n    def _estimate_arena_size(self):\n        \"\"\"\n        Estimates the size of the arena for the TFLM model.\n        This is to help the TFLM converter in a later stage.\n        Note: Depending on your model architecture, you may need to override this function.\n\n        Returns:\n            int: The size of the arena in bytes.\n        \"\"\"\n\n        # Note: Assuming a Sequential model. Also, assuming that TFLM is wise to do in-place operations if possible.\n\n        def _mul_dims(dims):\n            output = 1\n            for dim in dims:\n                output *= dim\n            return output\n        arena_size = 0\n        layer_1_size = _mul_dims(self.model.layers[0].input_shape[1:])\n\n        for layer in self.model.layers:\n            if isinstance(layer, tf.keras.layers.InputLayer):\n                continue\n\n            elif isinstance(layer, tf.keras.layers.Dense):\n                layer_2_size = _mul_dims(layer.output_shape[1:])\n\n            elif isinstance(layer, tf.keras.layers.Conv2D):\n                layer_2_size = _mul_dims(layer.output_shape[1:])\n\n            elif isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n                layer_2_size = _mul_dims(layer.output_shape[1:])\n\n            elif isinstance(layer, tf.keras.layers.MaxPooling2D):\n                layer_2_size = _mul_dims(layer.output_shape[1:])\n\n            elif isinstance(layer, tf.keras.layers.AveragePooling2D):\n                layer_2_size = _mul_dims(layer.output_shape[1:])\n\n            elif isinstance(layer, tf.keras.layers.GlobalAveragePooling2D):\n                layer_2_size = _mul_dims(layer.output_shape[1:])\n\n            elif isinstance(layer, tf.keras.layers.ZeroPadding2D):\n                layer_2_size = _mul_dims(layer.output_shape[1:])\n\n            elif isinstance(layer, tf.keras.layers.Flatten):\n                continue\n\n            elif isinstance(layer, tf.keras.layers.Add):\n                continue\n\n            elif isinstance(layer, tf.keras.layers.BatchNormalization):\n                continue\n\n            elif isinstance(layer, tf.keras.layers.Activation):\n                continue\n\n            elif isinstance(layer, tf.keras.layers.Dropout):\n                continue\n\n            elif isinstance(layer, tf.keras.layers.ReLU):\n                continue\n\n            elif isinstance(layer, tf.keras.layers.Softmax):\n                continue\n\n            else:\n                raise ValueError(\"Unknown layer type: {}\".format(layer))\n\n            if layer_1_size + layer_2_size &gt; arena_size:\n                arena_size = layer_1_size + layer_2_size\n            layer_1_size = layer_2_size\n\n        arena_size = arena_size * 4     # 4 bytes for each float32\n        return arena_size\n\n\n    def _get_op_resolver_funcs(self):\n        \"\"\"\n        Returns the operators needed to run the TFLM model.\n        This is to help the TFLM converter in a later stage.\n        Possible strings in the output can be found here:\n            https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/micro_mutable_op_resolver.h\n\n        Returns:\n            list[str]: The op resolvers to be called by the C++ code.\n\n        Example:\n            &gt;&gt;&gt; get_op_resolver_funcs()\n            [\"AddFullyConnected()\", \"AddRelu()\", \"AddSoftmax()\"]\n        \"\"\"\n        output = []\n        for layer in self.model.layers:\n            # TODO: check if registration is needed for non-float32 operations (e.g., AddConv2D(tflite::Register_CONV_2D_INT8()))\n            if isinstance(layer, tf.keras.layers.InputLayer):\n                pass\n\n            elif isinstance(layer, tf.keras.layers.Dense):\n                if \"AddFullyConnected()\" not in output:\n                    output.append(\"AddFullyConnected()\")\n\n            elif isinstance(layer, tf.keras.layers.Conv2D):\n                if \"AddConv2D()\" not in output:\n                    output.append(\"AddConv2D()\")\n\n            elif isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n                if \"AddDepthwiseConv2D()\" not in output:\n                    output.append(\"AddDepthwiseConv2D()\")\n\n            elif isinstance(layer, tf.keras.layers.MaxPooling2D):\n                if \"AddMaxPool2D()\" not in output:\n                    output.append(\"AddMaxPool2D()\")\n\n            elif isinstance(layer, tf.keras.layers.AveragePooling2D):\n                if \"AddAveragePool2D()\" not in output:\n                    output.append(\"AddAveragePool2D()\")\n\n            elif isinstance(layer, tf.keras.layers.GlobalAveragePooling2D):\n                if \"AddMean()\" not in output:\n                    output.append(\"AddMean()\")\n\n            elif isinstance(layer, tf.keras.layers.ZeroPadding2D):\n                if \"AddPad()\" not in output:\n                    output.append(\"AddPad()\")\n\n            elif isinstance(layer, tf.keras.layers.Flatten):\n                if \"AddReshape()\" not in output:\n                    output.append(\"AddReshape()\")\n\n            elif isinstance(layer, tf.keras.layers.Add):\n                if \"AddAdd()\" not in output:\n                    output.append(\"AddAdd()\")\n\n            elif isinstance(layer, tf.keras.layers.BatchNormalization):\n                pass\n\n            elif isinstance(layer, tf.keras.layers.Embedding):\n                if \"AddCast()\" not in output:\n                    output.append(\"AddCast()\")\n                if \"AddGather()\" not in output:\n                    output.append(\"AddGather()\")\n\n            elif isinstance(layer, tf.keras.layers.SimpleRNN):\n                if \"AddReshape()\" not in output:\n                    output.append(\"AddReshape()\")\n                if \"AddFullyConnected()\" not in output:\n                    output.append(\"AddFullyConnected()\")\n                if \"AddAdd()\" not in output:\n                    output.append(\"AddAdd()\")\n                if \"AddTanh()\" not in output:\n                    output.append(\"AddTanh()\")\n                if \"AddPack()\" not in output:\n                    output.append(\"AddPack()\")\n                if \"AddUnpack()\" not in output:\n                    output.append(\"AddUnpack()\")\n                if \"AddQuantize()\" not in output:       # needed for int8_only quantization\n                    output.append(\"AddQuantize()\")\n                if \"AddDequantize()\" not in output:     # needed for int8_only quantization\n                    output.append(\"AddDequantize()\")\n\n            elif isinstance(layer, tf.keras.layers.LSTM):\n                if \"AddReshape()\" not in output:\n                    output.append(\"AddReshape()\")\n                if \"AddFullyConnected()\" not in output:\n                    output.append(\"AddFullyConnected()\")\n                if \"AddAdd()\" not in output:\n                    output.append(\"AddAdd()\")\n                if \"AddTanh()\" not in output:\n                    output.append(\"AddTanh()\")\n                if \"AddPack()\" not in output:\n                    output.append(\"AddPack()\")\n                if \"AddUnpack()\" not in output:\n                    output.append(\"AddUnpack()\")\n                if \"AddSplit()\" not in output:\n                    output.append(\"AddSplit()\")\n                if \"AddLogistic()\" not in output:\n                    output.append(\"AddLogistic()\")\n                if \"AddMul()\" not in output:\n                    output.append(\"AddMul()\")\n                if \"AddQuantize()\" not in output:       # needed for int8_only quantization\n                    output.append(\"AddQuantize()\")\n                if \"AddDequantize()\" not in output:     # needed for int8_only quantization\n                    output.append(\"AddDequantize()\")\n\n            elif isinstance(layer, tf.keras.layers.GRU):\n                if \"AddReshape()\" not in output:\n                    output.append(\"AddReshape()\")\n                if \"AddFullyConnected()\" not in output:\n                    output.append(\"AddFullyConnected()\")\n                if \"AddAdd()\" not in output:\n                    output.append(\"AddAdd()\")\n                if \"AddTanh()\" not in output:\n                    output.append(\"AddTanh()\")\n                if \"AddPack()\" not in output:\n                    output.append(\"AddPack()\")\n                if \"AddUnpack()\" not in output:\n                    output.append(\"AddUnpack()\")\n                if \"AddSplit()\" not in output:\n                    output.append(\"AddSplit()\")\n                if \"AddLogistic()\" not in output:\n                    output.append(\"AddLogistic()\")\n                if \"AddMul()\" not in output:\n                    output.append(\"AddMul()\")\n                if \"AddSub()\" not in output:\n                    output.append(\"AddSub()\")\n                if \"AddSplitV()\" not in output:\n                    output.append(\"AddSplitV()\")\n\n\n            elif isinstance(layer, tf.keras.layers.Activation):\n                pass\n\n            elif isinstance(layer, tf.keras.layers.Dropout):\n                pass\n\n            elif isinstance(layer, tf.keras.layers.ReLU):\n                if \"AddRelu()\" not in output:\n                    output.append(\"AddRelu()\")\n\n            elif isinstance(layer, tf.keras.layers.Softmax):\n                if \"AddSoftmax()\" not in output:\n                    output.append(\"AddSoftmax()\")\n\n            else:\n                raise ValueError(\"Unknown layer type: {}\".format(layer))\n\n            try:\n                if layer.activation is tf.keras.activations.sigmoid:\n                    if \"AddLogistic()\" not in output:\n                        output.append(\"AddLogistic()\")\n\n                elif layer.activation is tf.keras.activations.tanh:\n                    if \"AddTanh()\" not in output:\n                        output.append(\"AddTanh()\")\n\n                elif layer.activation is tf.keras.activations.relu:\n                    if \"AddRelu()\" not in output:\n                        output.append(\"AddRelu()\")\n\n                elif layer.activation is tf.keras.activations.softmax:\n                    if \"AddSoftmax()\" not in output:\n                        output.append(\"AddSoftmax()\")\n\n            except Exception as _:\n                pass\n\n        return output\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.__init__","title":"__init__","text":"<pre><code>__init__(cfg=None)\n</code></pre> <p>Initializes the class. The following attributes should be set in the init function:     self.model (tf.keras.Model): The model.     self.dataset (DatasetSupervisorTemplate): The dataset.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>The configurations of the model. Defaults to None.</p> <code>None</code> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def __init__(self, cfg=None):\n    \"\"\"\n    Initializes the class.\n    The following attributes should be set in the __init__ function:\n        self.model (tf.keras.Model): The model.\n        self.dataset (DatasetSupervisorTemplate): The dataset.\n\n    Args:\n        cfg (dict): The configurations of the model. Defaults to None.\n    \"\"\"\n    self.model = None\n    self.dataset = None\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.compile_model","title":"compile_model","text":"<pre><code>compile_model(fine_tuning=False)\n</code></pre> <p>Compiles the model.</p> <p>Parameters:</p> Name Type Description Default <code>fine_tuning</code> <code>bool</code> <p>If True, the model will be compiled for fine-tuning. Defaults to False.</p> <code>False</code> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def compile_model(self, fine_tuning=False):\n    \"\"\"\n    Compiles the model.\n\n    Args:\n        fine_tuning (bool, optional): If True, the model will be compiled for fine-tuning. Defaults to False.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model()\n</code></pre> <p>Evaluates the model.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The evaluation metrics.</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def evaluate_model(self):\n    \"\"\"\n    Evaluates the model.\n\n    Returns:\n        dict: The evaluation metrics.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.get_FLOPs","title":"get_FLOPs","text":"<pre><code>get_FLOPs()\n</code></pre> <p>Returns the number of FLOPs of the model.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of FLOPs.</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def get_FLOPs(self):\n    \"\"\"\n    Returns the number of FLOPs of the model.\n\n    Returns:\n        int: The number of FLOPs.\n    \"\"\"\n    input_signature = [\n        tf.TensorSpec(\n            shape=(1, *params.shape[1:]),\n            dtype=params.dtype,\n            name=params.name\n        ) for params in self.model.inputs\n    ]\n    forward_graph = tf.function(self.model, input_signature).get_concrete_function().graph\n    options = option_builder.ProfileOptionBuilder.float_operation()\n    options['output'] = 'none'\n    graph_info = model_analyzer.profile(forward_graph, options=options)\n\n    FLOPs = graph_info.total_float_ops\n\n    return FLOPs\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.get_model_info","title":"get_model_info","text":"<pre><code>get_model_info()\n</code></pre> <p>Returns the model info that can be anything important, including its configuration.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The model info.</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def get_model_info(self):\n    \"\"\"\n    Returns the model info that can be anything important, including its configuration.\n\n    Returns:\n        dict: The model info.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.get_params_count","title":"get_params_count","text":"<pre><code>get_params_count()\n</code></pre> <p>Returns the number of parameters in the model.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>The total number of parameters, the number of trainable parameters, and the number of non-trainable parameters.</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def get_params_count(self):\n    \"\"\"\n    Returns the number of parameters in the model.\n\n    Returns:\n        list[int]: The total number of parameters, the number of trainable parameters, and the number of non-trainable parameters.\n    \"\"\"\n    total_params = 0\n    trainable_params = 0\n    non_trainable_params = 0\n\n    for layer in self.model.variables:\n        total_params += np.prod(layer.shape)\n\n    for layer in self.model.trainable_variables:\n        trainable_params += np.prod(layer.shape)\n\n    non_trainable_params = total_params - trainable_params\n\n    return int(total_params), int(trainable_params), int(non_trainable_params)\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.load_eqcheck_data","title":"load_eqcheck_data  <code>staticmethod</code>","text":"<pre><code>load_eqcheck_data(load_dir)\n</code></pre> <p>Loads the eqcheck data.</p> <p>Parameters:</p> Name Type Description Default <code>load_dir</code> <code>str</code> <p>The directory where the eqcheck data is stored.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(data_x, data_y_pred)</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>@staticmethod\ndef load_eqcheck_data(load_dir):\n    \"\"\"\n    Loads the eqcheck data.\n\n    Args:\n        load_dir (str): The directory where the eqcheck data is stored.\n\n    Returns:\n        tuple: (data_x, data_y_pred)\n    \"\"\"\n    eqcheck_data = np.load(os.path.join(load_dir, 'eqcheck_data.npz'))\n    data_x = eqcheck_data['data_x']\n    data_y_pred = eqcheck_data['data_y_pred']\n    return data_x, data_y_pred\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.load_model","title":"load_model","text":"<pre><code>load_model(load_dir)\n</code></pre> <p>Loads the model in the SavedModel format.</p> <p>Parameters:</p> Name Type Description Default <code>load_dir</code> <code>str</code> <p>The parent directory where the SavedModel format is stored in.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def load_model(self, load_dir):\n    \"\"\"\n    Loads the model in the SavedModel format.\n\n    Args:\n        load_dir (str): The parent directory where the SavedModel format is stored in.\n    \"\"\"\n    self.model = tf.keras.models.load_model(os.path.join(load_dir, \"saved_model_format\"))\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.load_representative_data","title":"load_representative_data  <code>staticmethod</code>","text":"<pre><code>load_representative_data(load_dir)\n</code></pre> <p>Loads the representative data.</p> <p>Parameters:</p> Name Type Description Default <code>load_dir</code> <code>str</code> <p>The directory where the representative data is stored.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The representative data.</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>@staticmethod\ndef load_representative_data(load_dir):\n    \"\"\"\n    Loads the representative data.\n\n    Args:\n        load_dir (str): The directory where the representative data is stored.\n\n    Returns:\n        numpy.ndarray: The representative data.\n    \"\"\"\n    representative_data = np.load(os.path.join(load_dir, 'representative_data.npy'))\n    return representative_data\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.load_weights","title":"load_weights","text":"<pre><code>load_weights(load_dir)\n</code></pre> <p>Loads the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>load_dir</code> <code>str</code> <p>The directory where the model weights are stored.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def load_weights(self, load_dir):\n    \"\"\"\n    Loads the model weights.\n\n    Args:\n        load_dir (str): The directory where the model weights are stored.\n    \"\"\"\n    self.model.load_weights(os.path.join(load_dir, \"weights\"))\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.log_model_to_wandb","title":"log_model_to_wandb  <code>staticmethod</code>","text":"<pre><code>log_model_to_wandb(model_dir, model_save_name)\n</code></pre> <p>Logs the model to W&amp;B.</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>The directory where the model is stored.</p> required <code>model_save_name</code> <code>str</code> <p>The name that will be assigned to the model artifact.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>@staticmethod\ndef log_model_to_wandb(model_dir, model_save_name):\n    \"\"\"\n    Logs the model to W&amp;B.\n\n    Args:\n        model_dir (str): The directory where the model is stored.\n        model_save_name (str): The name that will be assigned to the model artifact.\n    \"\"\"\n    model_path = os.path.join(model_dir, \"keras_format/model.keras\")\n    model_artifact = wandb.Artifact(model_save_name, type=\"model\")\n    model_artifact.add_file(model_path)\n    wandb.log_artifact(model_artifact)\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.measure_execution_time","title":"measure_execution_time","text":"<pre><code>measure_execution_time()\n</code></pre> <p>Measures the execution time of the model. The process starts by a warm-up phase for 100 iterations, then the execution time is measured for ~10 seconds.</p> <p>Returns:</p> Type Description <code>float</code> <p>The execution time in ms.</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def measure_execution_time(self):\n    \"\"\"\n    Measures the execution time of the model.\n    The process starts by a warm-up phase for 100 iterations, then the execution time is measured for ~10 seconds.\n\n    Returns:\n        float: The execution time in ms.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    sample_idx = rng.randint(0, self.dataset.train_x.shape[0])\n    x = np.array([self.dataset.train_x[sample_idx]])\n\n    # warm up\n    tic = time.time()\n    for i in range(100):\n        self.model(x, training=False)\n    toc = time.time()\n    itr = int(10 * 100 / (toc - tic))\n\n    # run the test\n    tic = time.time()\n    for i in range(itr):\n        self.model(x, training=False)\n    toc = time.time()\n    execution_time = (toc-tic)/itr*1000     # in ms\n\n    return execution_time\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.save_TFLM_info","title":"save_TFLM_info","text":"<pre><code>save_TFLM_info(save_path)\n</code></pre> <p>Saves the information required by the TFLM converter as a YAML file. This is to help TFLM converter in a later stage.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>The YAML file path where the TFLM info should be saved.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def save_TFLM_info(self, save_path):\n    \"\"\"\n    Saves the information required by the TFLM converter as a YAML file.\n    This is to help TFLM converter in a later stage.\n\n    Args:\n        save_path (str): The YAML file path where the TFLM info should be saved.\n    \"\"\"\n    TFLM_info = {}\n\n    arena_size_base = self._estimate_arena_size()\n    TFLM_info[\"arena_size\"] = {\n        \"32bit\": arena_size_base,\n        \"16bit\": arena_size_base//2,\n        \"8bit\": arena_size_base//4\n    }\n\n    TFLM_info[\"input_dims\"] = [int(dim) for dim in self.model.inputs[0].shape[1:]]\n    TFLM_info[\"output_dims\"] = [int(dim) for dim in self.model.outputs[0].shape[1:]]\n\n    TFLM_info[\"op_resolver_funcs\"] = None\n    try:\n        TFLM_info[\"op_resolver_funcs\"] = self._get_op_resolver_funcs()\n    except NotImplementedError:\n        print(\"The model doesn't have an implementation for the _get_op_resolver_funcs function. The placeholders should be filled manually.\")\n\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    with open(save_path, 'w') as f:\n        yaml.dump(TFLM_info, f, indent=4, sort_keys=False)\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.save_eqcheck_data","title":"save_eqcheck_data","text":"<pre><code>save_eqcheck_data(n_samples, save_dir)\n</code></pre> <p>Saves the eqcheck data as {\"data_x\", \"data_y_pred\"}.</p> <p>The data_x has shape (samples, input_shape) and data_y_pred has shape (samples, output_shape).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to be saved</p> required <code>save_dir</code> <code>str</code> <p>The directory where the data should be saved</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def save_eqcheck_data(self, n_samples, save_dir):\n    \"\"\"\n    Saves the eqcheck data as {\"data_x\", \"data_y_pred\"}.\n\n    The data_x has shape (samples, *input_shape) and data_y_pred has shape (samples, *output_shape).\n\n    Args:\n        n_samples (int): The number of samples to be saved\n        save_dir (str): The directory where the data should be saved\n    \"\"\"\n    # sanity check (useful for when deploying the model using TFLM)\n    for data_dim_size, model_dim_size in zip(self.dataset.train_x.shape[1:], self.model.inputs[0].shape[1:]):\n        if data_dim_size != model_dim_size and model_dim_size is not None:\n            raise ValueError(\"The shape of the train_x doesn't match the input shape of the model.\")\n\n    data_x = self.dataset.train_x[:n_samples]\n    data_y_pred = self.model.predict(data_x, verbose=0)\n\n    np.savez(os.path.join(save_dir, 'eqcheck_data.npz'), data_x=data_x, data_y_pred=data_y_pred)\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.save_model","title":"save_model","text":"<pre><code>save_model(save_dir)\n</code></pre> <p>Saves the model in two formats: Keras and SavedModel.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>The directory where the model should be saved in.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def save_model(self, save_dir):\n    \"\"\"\n    Saves the model in two formats: Keras and SavedModel.\n\n    Args:\n        save_dir (str): The directory where the model should be saved in.\n    \"\"\"\n    # save the model as a Keras format\n    os.makedirs(os.path.join(save_dir, \"keras_format\"), exist_ok=True)\n    self.model.save(os.path.join(save_dir, \"keras_format/model.keras\"))\n\n    # save the model as a SavedModel format\n    os.makedirs(os.path.join(save_dir, \"saved_model_format\"), exist_ok=True)\n    self.model.save(os.path.join(save_dir, \"saved_model_format\"))\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.save_model_info","title":"save_model_info  <code>staticmethod</code>","text":"<pre><code>save_model_info(model_info, save_dir)\n</code></pre> <p>Saves the model info.</p> <p>Parameters:</p> Name Type Description Default <code>model_info</code> <code>dict</code> <p>The model info.</p> required <code>save_dir</code> <code>str</code> <p>The directory where the model info should be saved.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>@staticmethod\ndef save_model_info(model_info, save_dir):\n    \"\"\"\n    Saves the model info.\n\n    Args:\n        model_info (dict): The model info.\n        save_dir (str): The directory where the model info should be saved.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    yaml.Dumper.ignore_aliases = lambda *args : True\n    with open(os.path.join(save_dir, \"model_info.yaml\"), 'w') as f:\n        yaml.dump(model_info, f, indent=4, sort_keys=False)\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.save_representative_data","title":"save_representative_data","text":"<pre><code>save_representative_data(n_samples, save_dir)\n</code></pre> <p>Saves the representative data with shape (samples, *input_shape).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to be saved.</p> required <code>save_dir</code> <code>str</code> <p>The directory where the data should be saved.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def save_representative_data(self, n_samples, save_dir):\n    \"\"\"\n    Saves the representative data with shape (samples, *input_shape).\n\n    Args:\n        n_samples (int): The number of samples to be saved.\n        save_dir (str): The directory where the data should be saved.\n    \"\"\"\n    data_x = self.dataset.train_x[:n_samples]\n    os.makedirs(save_dir, exist_ok=True)\n    np.save(os.path.join(save_dir, 'representative_data.npy'), data_x)\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.save_weights","title":"save_weights","text":"<pre><code>save_weights(save_dir)\n</code></pre> <p>Saves the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>The directory where the model weights should be saved.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def save_weights(self, save_dir):\n    \"\"\"\n    Saves the model weights.\n\n    Args:\n        save_dir (str): The directory where the model weights should be saved.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    self.model.save_weights(os.path.join(save_dir, \"weights\"))\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.set_configs","title":"set_configs","text":"<pre><code>set_configs(cfg)\n</code></pre> <p>Sets the configs from the given dictionary.</p> <p>Note: The changed configs won't affect the data, model, or any other loaded attributes. In case you want to change them, you should call the corresponding functions.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>The configurations.</p> required Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def set_configs(self, cfg):\n    \"\"\"\n    Sets the configs from the given dictionary.\n\n    Note: The changed configs won't affect the data, model, or any other loaded attributes.\n    In case you want to change them, you should call the corresponding functions.\n\n    Args:\n        cfg (dict): The configurations.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/platforms/tensorflow/#edgemark.models.platforms.TensorFlow.model_template.ModelSupervisorTemplate.train_model","title":"train_model","text":"<pre><code>train_model(fine_tuning=False, tensorboard_log_dir=None, best_weights_dir=None, use_wandb=False)\n</code></pre> <p>Trains the model.</p> <p>Parameters:</p> Name Type Description Default <code>fine_tuning</code> <code>bool</code> <p>If True, the model will be trained for fine-tuning. Defaults to False.</p> <code>False</code> <code>tensorboard_log_dir</code> <code>str</code> <p>The directory where the logs should be saved. If None, the logs won't be saved. Defaults to None.</p> <code>None</code> <code>best_weights_dir</code> <code>str</code> <p>The directory where the best weights should be saved. If None, the best weights won't be saved. Defaults to None.</p> <code>None</code> <code>use_wandb</code> <code>bool</code> <p>If True, the training progress will be logged to W&amp;B. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[History]</code> <p>The training history or None.</p> Source code in <code>edgemark/models/platforms/TensorFlow/model_template.py</code> <pre><code>def train_model(self, fine_tuning=False, tensorboard_log_dir=None, best_weights_dir=None, use_wandb=False):\n    \"\"\"\n    Trains the model.\n\n    Args:\n        fine_tuning (bool, optional): If True, the model will be trained for fine-tuning. Defaults to False.\n        tensorboard_log_dir (str, optional): The directory where the logs should be saved. If None, the logs won't be saved. Defaults to None.\n        best_weights_dir (str, optional): The directory where the best weights should be saved. If None, the best weights won't be saved. Defaults to None.\n        use_wandb (bool, optional): If True, the training progress will be logged to W&amp;B. Defaults to False.\n\n    Returns:\n        Optional[tf.keras.callbacks.History]: The training history or None.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/platforms/tflite/","title":"TFLite","text":""},{"location":"api/platforms/tflite/#edgemark.models.platforms.TFLite.TFLite_converter","title":"edgemark.models.platforms.TFLite.TFLite_converter","text":""},{"location":"api/platforms/tflite/#edgemark.models.platforms.TFLite.TFLite_converter.basic_convert","title":"basic_convert","text":"<pre><code>basic_convert(model_path)\n</code></pre> <p>Convert the model without any optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the TensorFlow model.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The TFLite model.</p> Source code in <code>edgemark/models/platforms/TFLite/TFLite_converter.py</code> <pre><code>def basic_convert(model_path):\n    \"\"\"\n    Convert the model without any optimizations.\n\n    Args:\n        model_path (str): The path to the TensorFlow model.\n\n    Returns:\n        bytes: The TFLite model.\n    \"\"\"\n    converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n    tflite_quant_model = converter.convert()\n    return tflite_quant_model\n</code></pre>"},{"location":"api/platforms/tflite/#edgemark.models.platforms.TFLite.TFLite_converter.cluster","title":"cluster","text":"<pre><code>cluster(model_path, n_clusters)\n</code></pre> <p>Cluster the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the TensorFlow model.</p> required <code>n_clusters</code> <code>int</code> <p>The number of clusters.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>The clustered model.</p> Source code in <code>edgemark/models/platforms/TFLite/TFLite_converter.py</code> <pre><code>def cluster(model_path, n_clusters):\n    \"\"\"\n    Cluster the model.\n\n    Args:\n        model_path (str): The path to the TensorFlow model.\n        n_clusters (int): The number of clusters.\n\n    Returns:\n        tf.keras.Model: The clustered model.\n    \"\"\"\n    model = tf.keras.models.load_model(model_path)\n\n    clustered_model = tfmot.clustering.keras.cluster_weights(model, number_of_clusters=n_clusters)\n\n    stripped_model = tfmot.clustering.keras.strip_clustering(clustered_model)\n\n    return stripped_model\n</code></pre>"},{"location":"api/platforms/tflite/#edgemark.models.platforms.TFLite.TFLite_converter.main","title":"main","text":"<pre><code>main(cfg_path=config_file_path, **kwargs)\n</code></pre> <p>Convert the TensorFlow models to TFLite models with the specified optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_path</code> <code>str</code> <p>The path to the configuration file. The configuration file that this path points to should contain the following keys:     - model_base_dir (str): A placeholder for the model base directory. This will be populated by the target directory.     - linkers_dir (str): Path to the directory where the generated models list is loaded from and the converted models list will be saved.     - tf_model_path (str): Path to the TensorFlow model.     - representative_data_path (str): Path to the representative data for quantization. The file should be a numpy file.     - tflite_save_dir (str): Path to the directory to save the converted models and their assets.     - conversion_timeout (float): The timeout for each conversion in seconds.     - optimizations (list): The optimizations to apply during conversion. In each element/group, the optimizations should be separated by '+'.</p> <code>config_file_path</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments to be passed to the configuration file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing the following keys for each target model: - dir (str): The directory of the target model. - flavors (list): A list of dictionaries containing the following keys for each optimization:     - flavor (str): The optimization flavor.     - result (str): The result of the conversion. It can be either \"success\" or \"failed\".     - error (str): The error message in case of failure.     - traceback (str): The traceback in case of failure. Either this or 'exception_file' will be present.     - exception_file (str): The path to the exception file in case of failure. Either this or 'traceback' will be present.</p> Source code in <code>edgemark/models/platforms/TFLite/TFLite_converter.py</code> <pre><code>def main(cfg_path=config_file_path, **kwargs):\n    \"\"\"\n    Convert the TensorFlow models to TFLite models with the specified optimizations.\n\n    Args:\n        cfg_path (str): The path to the configuration file.\n            The configuration file that this path points to should contain the following keys:\n                - model_base_dir (str): A placeholder for the model base directory. This will be populated by the target directory.\n                - linkers_dir (str): Path to the directory where the generated models list is loaded from and the converted models list will be saved.\n                - tf_model_path (str): Path to the TensorFlow model.\n                - representative_data_path (str): Path to the representative data for quantization. The file should be a numpy file.\n                - tflite_save_dir (str): Path to the directory to save the converted models and their assets.\n                - conversion_timeout (float): The timeout for each conversion in seconds.\n                - optimizations (list): The optimizations to apply during conversion. In each element/group, the optimizations should be separated by '+'.\n        **kwargs (dict): Keyword arguments to be passed to the configuration file.\n\n    Returns:\n        list: A list of dictionaries containing the following keys for each target model:\n            - dir (str): The directory of the target model.\n            - flavors (list): A list of dictionaries containing the following keys for each optimization:\n                - flavor (str): The optimization flavor.\n                - result (str): The result of the conversion. It can be either \"success\" or \"failed\".\n                - error (str): The error message in case of failure.\n                - traceback (str): The traceback in case of failure. Either this or 'exception_file' will be present.\n                - exception_file (str): The path to the exception file in case of failure. Either this or 'traceback' will be present.\n    \"\"\"\n    cfg = OmegaConf.load(cfg_path, **kwargs)\n    cfg.update(OmegaConf.create(kwargs))\n\n    targets = OmegaConf.load(os.path.join(cfg.linkers_dir, \"tf_generated_models_list.yaml\"))\n    converted_models_list = []\n    output = [{\"dir\": target_dir, \"flavors\": []} for target_dir in targets]\n\n    for i, target_dir in enumerate(targets):\n        cfg.model_base_dir = target_dir\n        print(\"Converting the model in: {} ({}/{})\".format(cfg.tf_model_path, i+1, len(targets)))\n        print(\"Saving to: {}\".format(cfg.tflite_save_dir))\n\n        conversions_list = []\n        model_path = cfg.tf_model_path\n        saving_root = cfg.tflite_save_dir\n        opt_groups = cfg.optimizations\n        representative_data_path = cfg.representative_data_path\n        representative_data = None\n\n        finishers = [\"basic\", \"q_dynamic\", \"q_full_int\", \"q_full_int_only\", \"q_16x8\", \"q_16x8_int_only\", \"q_float16\"]\n\n        # sanity checks\n        for opt_group in opt_groups:\n            try:\n                opts = [opt.strip() for opt in opt_group.split('+')]\n\n                if \"q_full_int\" in opts or \"q_full_int_only\" in opts or \"q_16x8\" in opts or \"q_16x8_int_only\" in opts:\n                    if not os.path.exists(representative_data_path):\n                        raise FileNotFoundError(\"representative_dataset_path is required for full_int, full_int_only, 16x8, and 16x8_int_only optimizations, but {} does not exist.\".format(representative_data_path))\n                    representative_data = np.load(representative_data_path)\n\n                for opt in opts[:-1]:\n                    if opt in finishers:\n                        raise ValueError(\"The {} optimization should be the last one in its group ({}).\".format(opt, opt_group))\n\n            except Exception as e:\n                output[i][\"flavors\"].append({\n                    \"flavor\": opt_group,\n                    \"result\": \"failed\",\n                    \"error\": type(e).__name__,\n                    \"traceback\": traceback.format_exc()\n                })\n                print(\"Error:\")\n                print(traceback.format_exc())\n                opt_groups.remove(opt_group)\n\n        progress_bar = tqdm(total=len(opt_groups), desc=\"TFLite conversion\", colour='green', bar_format=progress_bar_format, leave=True)\n        errors = []\n        convertion_messages = \"\"\n\n        for opt_group in opt_groups:\n            try:\n                opts = [opt.strip() for opt in opt_group.split('+')]\n\n                if opts[-1] not in finishers:\n                    opts.append(\"basic\")\n\n                conversion_funcs = []\n                for i, opt in enumerate(opts):\n                    if opt == \"basic\":\n                        conversion_funcs.append({\"func\": basic_convert, \"args\": ()})\n                    elif opt == \"q_dynamic\":\n                        conversion_funcs.append({\"func\": quantize, \"args\": ()})\n                    elif opt == \"q_full_int\":\n                        conversion_funcs.append({\"func\": quantize, \"args\": (representative_data,)})\n                    elif opt == \"q_full_int_only\":\n                        conversion_funcs.append({\"func\": quantize, \"args\": (representative_data,), \"kwargs\": {\"int_only\": True}})\n                    elif opt == \"q_16x8\":\n                        conversion_funcs.append({\"func\": quantize, \"args\": (representative_data,), \"kwargs\": {\"a16_w8\": True}})\n                    elif opt == \"q_16x8_int_only\":\n                        conversion_funcs.append({\"func\": quantize, \"args\": (representative_data,), \"kwargs\": {\"a16_w8\": True, \"int_only\": True}})\n                    elif opt == \"q_float16\":\n                        conversion_funcs.append({\"func\": quantize, \"args\": (), \"kwargs\": {\"float16\": True}})\n                    elif opt.startswith(\"p_\") and opt[2:].isdigit():\n                        target_sparsity = float(opt[2:]) / 100\n                        assert 0 &lt;= target_sparsity &lt;= 1, \"Sparsity should be between 0 and 1.\"\n                        conversion_funcs.append({\"func\": prune, \"args\": (target_sparsity,)})\n                    elif opt.startswith(\"c_\") and opt[2:].isdigit():\n                        n_clusters = int(opt[2:])\n                        assert n_clusters &gt; 0, \"Number of clusters should be greater than 0.\"\n                        conversion_funcs.append({\"func\": cluster, \"args\": (n_clusters,)})\n                    else:\n                        raise ValueError(\"Unknown optimization: {}\".format(opt))\n\n                progress_bar.set_postfix_str(\"{}\".format(opt_group))\n                save_dir = os.path.join(saving_root, opt_group)\n                convert_and_save_args = (save_dir, model_path, conversion_funcs)\n\n                process_success, process_result = _run_as_process(cfg.conversion_timeout, _run_in_silence, _convert_and_save, *convert_and_save_args)\n\n                if process_success:\n                    convertion_success, _ = process_result.result\n                else:\n                    convertion_success = False\n                    failure_report = \"Process failed.\\nResult: {}\\nException: {}\\nTimed out: {}\\n\".format(process_result.result, process_result.exception, process_result.timed_out)\n                    if process_result.result is None and process_result.exception is None and process_result.timed_out is False:\n                        failure_report += \"Possible crash of the process\\n\"\n                    _save_tflite_exception(failure_report, os.path.join(saving_root, opt_group), delete_model=True)\n\n                if convertion_success:\n                    conversions_list.append(opt_group)\n                    _, message = process_result.result\n                    convertion_messages += \"=\"*80 + \"\\n\" + \"{}:\\n\".format(opt_group) + message + \"\\n\\n\"\n\n                    output[i][\"flavors\"].append({\n                        \"flavor\": opt_group,\n                        \"result\": \"success\"\n                    })\n\n                else:\n                    convertion_messages += \"=\"*80 + \"\\n\" + \"{}:\\n\".format(opt_group) + \"Conversion failed\\n\\n\"\n                    errors.append(opt_group)\n\n                    output[i][\"flavors\"].append({\n                        \"flavor\": opt_group,\n                        \"result\": \"failed\",\n                        \"error\": \"Conversion failed\",\n                        \"exception_file\": os.path.join(saving_root, opt_group, 'exception.txt')\n                    })\n\n                progress_bar.update(1)\n\n            except Exception as e:\n                errors.append(opt_group)\n                convertion_messages += \"=\"*80 + \"\\n\" + \"{}:\\n\".format(opt_group) + traceback.format_exc() + \"\\n\\n\"\n\n                output[i][\"flavors\"].append({\n                    \"flavor\": opt_group,\n                    \"result\": \"failed\",\n                    \"error\": type(e).__name__,\n                    \"traceback\": traceback.format_exc()\n                })\n\n                progress_bar.update(1)\n\n        progress_bar.set_postfix_str(\"Done\")\n        progress_bar.close()\n\n        converted_models_element = {\n            \"model_base_dir\": cfg.model_base_dir,\n            \"conversions\": conversions_list\n        }\n        converted_models_list.append(converted_models_element)\n\n        # Save the conversion messages\n        with open(os.path.join(saving_root, 'conversion_messages.txt'), 'w') as f:\n            f.write(convertion_messages)\n\n        if len(errors) &gt; 0:\n            print(\"Errors:\")\n            for error in errors:\n                print(\"    \" + error)\n            print(\"Please check the exception files for details.\")\n\n        print()\n\n    # Save the list of converted models\n    os.makedirs(cfg.linkers_dir, exist_ok=True)\n    with open(os.path.join(cfg.linkers_dir, 'tflite_converted_models_list.yaml'), 'w') as f:\n        yaml.dump(converted_models_list, f, indent=4, sort_keys=False)\n\n    return output\n</code></pre>"},{"location":"api/platforms/tflite/#edgemark.models.platforms.TFLite.TFLite_converter.prune","title":"prune","text":"<pre><code>prune(model_path, target_sparsity)\n</code></pre> <p>Prune the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the TensorFlow model.</p> required <code>target_sparsity</code> <code>float</code> <p>The target sparsity.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>The pruned model.</p> Source code in <code>edgemark/models/platforms/TFLite/TFLite_converter.py</code> <pre><code>def prune(model_path, target_sparsity):\n    \"\"\"\n    Prune the model.\n\n    Args:\n        model_path (str): The path to the TensorFlow model.\n        target_sparsity (float): The target sparsity.\n\n    Returns:\n        tf.keras.Model: The pruned model.\n    \"\"\"\n    model = tf.keras.models.load_model(model_path)\n\n    pruning_schedule = tfmot.sparsity.keras.ConstantSparsity(target_sparsity=target_sparsity, begin_step=0, frequency=1)\n    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)\n\n    train_x = np.random.rand(1, *model.input.shape[1:])\n    train_y = np.random.rand(1, *model.output.shape[1:])\n\n    callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n    opt = tf.keras.optimizers.SGD(learning_rate=0)\n\n    model_for_pruning.compile(optimizer=opt, loss='mse')\n    model_for_pruning.fit(train_x, train_y, epochs=1, callbacks=callbacks, verbose=0)\n\n    stripped_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n\n    return stripped_model\n</code></pre>"},{"location":"api/platforms/tflite/#edgemark.models.platforms.TFLite.TFLite_converter.quantize","title":"quantize","text":"<pre><code>quantize(model_path, representative_data=None, int_only=False, a16_w8=False, float16=False)\n</code></pre> <p>Quantize the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the TensorFlow model.</p> required <code>representative_data</code> <code>ndarray</code> <p>The representative data for quantization.</p> <code>None</code> <code>int_only</code> <code>bool</code> <p>Whether to force the model to use integer quantization only.</p> <code>False</code> <code>a16_w8</code> <code>bool</code> <p>Whether to quantize the model to use 16-bit activations and 8-bit weights.</p> <code>False</code> <code>float16</code> <code>bool</code> <p>Whether to quantize the model to use 16-bit floating point numbers.</p> <code>False</code> <p>Returns:</p> Type Description <code>bytes</code> <p>The TFLite model.</p> Source code in <code>edgemark/models/platforms/TFLite/TFLite_converter.py</code> <pre><code>def quantize(model_path, representative_data=None, int_only=False, a16_w8=False, float16=False):\n    \"\"\"\n    Quantize the model.\n\n    Args:\n        model_path (str): The path to the TensorFlow model.\n        representative_data (numpy.ndarray): The representative data for quantization.\n        int_only (bool): Whether to force the model to use integer quantization only.\n        a16_w8 (bool): Whether to quantize the model to use 16-bit activations and 8-bit weights.\n        float16 (bool): Whether to quantize the model to use 16-bit floating point numbers.\n\n    Returns:\n        bytes: The TFLite model.\n    \"\"\"\n    # sanity check\n    if float16 and ((representative_data is not None) or int_only or a16_w8):\n        print(\"Warning: float16 came with int_only, a16_w8, or representative_data. Are you sure you want to do this?\")\n    if int_only and (representative_data is None):\n        print(\"Warning: int_only requires representative_data. You will receive an error.\")\n    if a16_w8 and representative_data is None and int_only is False and float16 is False:\n        print(\"Warning: Just setting a16_w8 is possible but not common. Are you sure you want to do this?\")\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    if representative_data is not None:\n        def representative_dataset():\n            for sample in representative_data:\n                    sample = np.expand_dims(sample, axis=0)     # batch_size = 1\n                    yield [sample]      # set sample as the first (and only) input of the model\n        converter.representative_dataset = representative_dataset\n\n    if int_only:\n        if a16_w8:\n            converter.inference_input_type = tf.int16\n            converter.inference_output_type = tf.int16\n        else:\n            converter.inference_input_type = tf.int8\n            converter.inference_output_type = tf.int8\n\n    if a16_w8:\n        if int_only:\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\n        else:\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n    else:\n        if int_only:\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        else:\n            pass\n\n    if float16:\n        converter.target_spec.supported_types = [tf.float16]\n\n    tflite_quant_model = converter.convert()\n    return tflite_quant_model\n</code></pre>"},{"location":"api/platforms/tflm/","title":"TFLM","text":""},{"location":"api/platforms/tflm/#edgemark.models.platforms.TFLM.TFLM_converter","title":"edgemark.models.platforms.TFLM.TFLM_converter","text":""},{"location":"api/platforms/tflm/#edgemark.models.platforms.TFLM.TFLM_converter.create_tflm_source_files","title":"create_tflm_source_files","text":"<pre><code>create_tflm_source_files(tflite_model_path, eqcheck_data_path, tflm_info_path, templates_dir, save_dir)\n</code></pre> <p>Create the TFLM model and data source files.</p> <p>Parameters:</p> Name Type Description Default <code>tflite_model_path</code> <code>str</code> <p>The path to the TensorFlow Lite model.</p> required <code>eqcheck_data_path</code> <code>str</code> <p>The path to the data file for the equivalence check. The file should be a numpy file with 'data_x' and 'data_y_pred' arrays.</p> required <code>tflm_info_path</code> <code>str</code> <p>The path to the TFLM info file. This should be a YAML file containing the following keys: - input_dims (list): The input dimensions of the model. - output_dims (list): The output dimensions of the model. - arena_size (dict): The arena sizes for different data types. The keys should be '8bit', '16bit', and '32bit'. - op_resolver_funcs (list): The list of strings. These should be the operator resolver functions to be added.</p> required <code>templates_dir</code> <code>str</code> <p>The directory containing the template files for the model and data source files.</p> required <code>save_dir</code> <code>str</code> <p>The directory to save the source files.</p> required Source code in <code>edgemark/models/platforms/TFLM/TFLM_converter.py</code> <pre><code>def create_tflm_source_files(tflite_model_path, eqcheck_data_path, tflm_info_path, templates_dir, save_dir):\n    \"\"\"\n    Create the TFLM model and data source files.\n\n    Args:\n        tflite_model_path (str): The path to the TensorFlow Lite model.\n        eqcheck_data_path (str): The path to the data file for the equivalence check. The file should be a numpy file with 'data_x' and 'data_y_pred' arrays.\n        tflm_info_path (str): The path to the TFLM info file. This should be a YAML file containing the following keys:\n            - input_dims (list): The input dimensions of the model.\n            - output_dims (list): The output dimensions of the model.\n            - arena_size (dict): The arena sizes for different data types. The keys should be '8bit', '16bit', and '32bit'.\n            - op_resolver_funcs (list): The list of strings. These should be the operator resolver functions to be added.\n        templates_dir (str): The directory containing the template files for the model and data source files.\n        save_dir (str): The directory to save the source files.\n    \"\"\"\n    def _np_to_c(array):\n        if array.ndim == 0:\n            return str(array.item())\n        c_array = \"{\" + \", \".join(_np_to_c(subarray) for subarray in array) + \"}\"\n        return c_array\n\n    tflm_info = OmegaConf.load(tflm_info_path)\n    tflite_model = open(tflite_model_path, \"rb\").read()\n\n    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    in_scale, in_zero_point = input_details[0]['quantization']\n    out_scale, out_zero_point = output_details[0]['quantization']\n    if input_details[0]['dtype'] is np.float32 and output_details[0]['dtype'] is np.float32:\n        in_out_dtype = \"float\"\n    elif input_details[0]['dtype'] is np.int16 and output_details[0]['dtype'] is np.int16:\n        in_out_dtype = \"int16\"\n    elif input_details[0]['dtype'] is np.int8 and output_details[0]['dtype'] is np.int8:\n        in_out_dtype = \"int8\"\n    else:\n        raise ValueError(\"Unknown input and output types: {} and {}\".format(input_details[0]['dtype'], output_details[0]['dtype']))\n\n    if in_out_dtype == \"float\" and tflm_info.op_resolver_funcs is not None:\n        for tensor in interpreter.get_tensor_details():\n            if tensor['dtype'] not in [np.float32, np.int32]:       # not the basic quantization either\n                tflm_info.op_resolver_funcs += [\"AddQuantize()\", \"AddDequantize()\"]\n                break\n\n    # create model files\n    with open(os.path.join(templates_dir, 'model.h'), 'r') as f:\n        h_file = f.read()\n\n    with open(os.path.join(templates_dir, 'model.cpp'), 'r') as f:\n        cpp_file = f.read()\n\n    if in_out_dtype == \"float\":\n        h_file = h_file.replace(\"{input_dtype}\", \"float\")\n        h_file = h_file.replace(\"{output_dtype}\", \"float\")\n        h_file = h_file.replace(\"{arena_size}\", str(tflm_info.arena_size[\"32bit\"] + 10240))     # adding 10kB for safety\n    elif in_out_dtype == \"int8\":\n        h_file = h_file.replace(\"{input_dtype}\", \"int8_t\")\n        h_file = h_file.replace(\"{output_dtype}\", \"int8_t\")\n        h_file = h_file.replace(\"{arena_size}\", str(tflm_info.arena_size[\"8bit\"] + 10240))\n    elif in_out_dtype == \"int16\":\n        h_file = h_file.replace(\"{input_dtype}\", \"int16_t\")\n        h_file = h_file.replace(\"{output_dtype}\", \"int16_t\")\n        h_file = h_file.replace(\"{arena_size}\", str(tflm_info.arena_size[\"16bit\"] + 10240))\n    else:\n        raise ValueError(\"Unknown in_out_dtype: {}\".format(in_out_dtype))\n\n    h_file = h_file.replace(\"{input_n_dims}\", str(len(tflm_info.input_dims)))\n    h_file = h_file.replace(\"{output_n_dims}\", str(len(tflm_info.output_dims)))\n\n    input_dims_size_str = \"\"\n    for i, dim in enumerate(tflm_info.input_dims):\n        if i &gt; 0:\n            input_dims_size_str += \"\\n\"\n        input_dims_size_str += \"#define INPUT_DIM_{i}_SIZE {dim}\".format(i=i, dim=dim)\n    h_file = h_file.replace(\"{input_dims_size}\", input_dims_size_str)\n\n    output_dims_size_str = \"\"\n    for i, dim in enumerate(tflm_info.output_dims):\n        if i &gt; 0:\n            output_dims_size_str += \"\\n\"\n        output_dims_size_str += \"#define OUTPUT_DIM_{i}_SIZE {dim}\".format(i=i, dim=dim)\n    h_file = h_file.replace(\"{output_dims_size}\", output_dims_size_str)\n\n    if tflm_info.op_resolver_funcs is not None:\n        h_file = h_file.replace(\"{n_operators}\", str(len(tflm_info.op_resolver_funcs)))\n\n    cpp_file = cpp_file.replace(\"{model_data_size}\", str(len(tflite_model)))\n\n    model_data_str = \"\"\n    for i, byte in enumerate(tflite_model):\n        if i % 16 == 0:\n            model_data_str += \"\\n\\t\"\n        model_data_str += \"0x{:02x}, \".format(byte)\n    model_data_str = model_data_str[:-2] + \"\\n\"\n\n    cpp_file = cpp_file.replace(\"{model_data}\", model_data_str)\n\n    if tflm_info.op_resolver_funcs is not None:\n        op_resolver_content_str = \"\"\n        op_resolver_content_str += \"\\n\\tTfLiteStatus status;\\n\"\n        for i, op_resolver_func in enumerate(tflm_info.op_resolver_funcs):\n            op_resolver_content_str += \"\\n\"\n            op_resolver_content_str += \"\\tstatus = resolver_ptr-&gt;{func};\\n\".format(func=op_resolver_func)\n            op_resolver_content_str += \"\\tif (status != kTfLiteOk) {\\n\"\n            op_resolver_content_str += \"\\t\\tprintf(\\\"Failed to add {func}\\\");\\n\".format(func=op_resolver_func)\n            op_resolver_content_str += \"\\t\\treturn status;\\n\"\n            op_resolver_content_str += \"\\t}\\n\"\n        op_resolver_content_str += \"\\n\"\n        op_resolver_content_str += \"\\treturn kTfLiteOk;\\n\"\n        cpp_file = cpp_file.replace(\"{op_resolver_content}\", op_resolver_content_str)\n\n    os.makedirs(save_dir, exist_ok=True)\n    with open(os.path.join(save_dir, 'model.h'), 'w') as f:\n        f.write(h_file)\n    with open(os.path.join(save_dir, 'model.cpp'), 'w') as f:\n        f.write(cpp_file)\n\n    # create data files\n    data = np.load(eqcheck_data_path)\n    data_x = data['data_x']\n    data_y = data['data_y_pred']\n    data.close()\n\n    with open(os.path.join(templates_dir, 'data.h'), 'r') as f:\n        h_file = f.read()\n\n    with open(os.path.join(templates_dir, 'data.cpp'), 'r') as f:\n        cpp_file = f.read()\n\n    h_file = h_file.replace(\"{n_samples}\", str(data_x.shape[0]))\n\n    data_x_shape_str = \"\"\n    for dim in data_x.shape:\n        data_x_shape_str += \"[{}]\".format(dim)\n    data_y_shape_str = \"\"\n    for dim in data_y.shape:\n        data_y_shape_str += \"[{}]\".format(dim)\n    h_file = h_file.replace(\"{samples_x_shape}\", data_x_shape_str)\n    h_file = h_file.replace(\"{samples_y_shape}\", data_y_shape_str)\n    cpp_file = cpp_file.replace(\"{samples_x_shape}\", data_x_shape_str)\n    cpp_file = cpp_file.replace(\"{samples_y_shape}\", data_y_shape_str)\n\n    if in_out_dtype == \"float\":\n        h_file = h_file.replace(\"{input_dtype}\", \"float\")\n        h_file = h_file.replace(\"{output_dtype}\", \"float\")\n        cpp_file = cpp_file.replace(\"{input_dtype}\", \"float\")\n        cpp_file = cpp_file.replace(\"{output_dtype}\", \"float\")\n    elif in_out_dtype == \"int8\":\n        h_file = h_file.replace(\"{input_dtype}\", \"int8_t\")\n        h_file = h_file.replace(\"{output_dtype}\", \"int8_t\")\n        cpp_file = cpp_file.replace(\"{input_dtype}\", \"int8_t\")\n        cpp_file = cpp_file.replace(\"{output_dtype}\", \"int8_t\")\n        data_x = (data_x / in_scale) + in_zero_point\n        data_y = (data_y / out_scale) + out_zero_point\n        data_x = np.clip(data_x, -128, 127)\n        data_y = np.clip(data_y, -128, 127)\n        data_x = data_x.astype(np.int8)\n        data_y = data_y.astype(np.int8)\n    elif in_out_dtype == \"int16\":\n        h_file = h_file.replace(\"{input_dtype}\", \"int16_t\")\n        h_file = h_file.replace(\"{output_dtype}\", \"int16_t\")\n        cpp_file = cpp_file.replace(\"{input_dtype}\", \"int16_t\")\n        cpp_file = cpp_file.replace(\"{output_dtype}\", \"int16_t\")\n        data_x = (data_x / in_scale) + in_zero_point\n        data_y = (data_y / out_scale) + out_zero_point\n        data_x = np.clip(data_x, -32768, 32767)\n        data_y = np.clip(data_y, -32768, 32767)\n        data_x = data_x.astype(np.int16)\n        data_y = data_y.astype(np.int16)\n    else:\n        raise ValueError(\"Unknown in_out_dtype: {}\".format(in_out_dtype))\n\n    data_x_str = \"\\n\"\n    for i, sample_x in enumerate(data_x):\n        data_x_str += \"\\t\" + _np_to_c(sample_x)\n        if i &lt; len(data_x) - 1:\n            data_x_str += \",\"\n        data_x_str += \"\\n\"\n\n    data_y_str = \"\\n\"\n    for i, sample_y in enumerate(data_y):\n        data_y_str += \"\\t\" + _np_to_c(sample_y)\n        if i &lt; len(data_y) - 1:\n            data_y_str += \",\"\n        data_y_str += \"\\n\"\n\n    cpp_file = cpp_file.replace(\"{samples_x}\", data_x_str)\n    cpp_file = cpp_file.replace(\"{samples_y}\", data_y_str)\n\n    os.makedirs(save_dir, exist_ok=True)\n    with open(os.path.join(save_dir, 'data.h'), 'w') as f:\n        f.write(h_file)\n    with open(os.path.join(save_dir, 'data.cpp'), 'w') as f:\n        f.write(cpp_file)\n</code></pre>"},{"location":"api/platforms/tflm/#edgemark.models.platforms.TFLM.TFLM_converter.main","title":"main","text":"<pre><code>main(cfg_path=config_file_path, **kwargs)\n</code></pre> <p>Convert TensorFlow Lite models to TFLM models.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_path</code> <code>str</code> <p>The path to the configuration file. The configuration file that this path points to should contain the following keys:     - model_base_dir (str): A placeholder for the model base directory. This will be populated by the target directory.     - tflite_conversion_type (str): A placeholder for the TFLite conversion type. This will be populated by the conversion type.     - linkers_dir (str): Path to the directory where the tflite converted models list is loaded from and the TFLM converted models list will be saved.     - tflite_model_path (str): The path to the TensorFlow Lite model.     - eqcheck_data_path (str): The path to the equality check data.     - tflm_info_path (str): The path to the TFLM info file.     - tflm_templates_dir (str): Path to the directory containing the TFLM model and data source templates.</p> <code>config_file_path</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments to be passed to the configuration file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing the following keys for each target model: - dir (str): The directory of the target model. - flavors (list): A list of dictionaries containing the following keys for each optimization:     - flavor (str): The optimization flavor.     - result (str): The result of the conversion. It can be either \"success\" or \"failed\".     - error (str): The error message in case of failure.     - traceback (str): The traceback in case of failure.</p> Source code in <code>edgemark/models/platforms/TFLM/TFLM_converter.py</code> <pre><code>def main(cfg_path=config_file_path, **kwargs):\n    \"\"\"\n    Convert TensorFlow Lite models to TFLM models.\n\n    Args:\n        cfg_path (str): The path to the configuration file.\n            The configuration file that this path points to should contain the following keys:\n                - model_base_dir (str): A placeholder for the model base directory. This will be populated by the target directory.\n                - tflite_conversion_type (str): A placeholder for the TFLite conversion type. This will be populated by the conversion type.\n                - linkers_dir (str): Path to the directory where the tflite converted models list is loaded from and the TFLM converted models list will be saved.\n                - tflite_model_path (str): The path to the TensorFlow Lite model.\n                - eqcheck_data_path (str): The path to the equality check data.\n                - tflm_info_path (str): The path to the TFLM info file.\n                - tflm_templates_dir (str): Path to the directory containing the TFLM model and data source templates.\n        **kwargs (dict): Keyword arguments to be passed to the configuration file.\n\n    Returns:\n        list: A list of dictionaries containing the following keys for each target model:\n            - dir (str): The directory of the target model.\n            - flavors (list): A list of dictionaries containing the following keys for each optimization:\n                - flavor (str): The optimization flavor.\n                - result (str): The result of the conversion. It can be either \"success\" or \"failed\".\n                - error (str): The error message in case of failure.\n                - traceback (str): The traceback in case of failure.\n    \"\"\"\n    cfg = OmegaConf.load(cfg_path, **kwargs)\n    cfg.update(OmegaConf.create(kwargs))\n\n    targets = OmegaConf.load(os.path.join(cfg.linkers_dir, \"tflite_converted_models_list.yaml\"))\n\n    output = [{\"dir\": base_model[\"model_base_dir\"], \"flavors\": []} for base_model in targets]\n\n    converted_models_list = []\n    for i, base_model in enumerate(targets):\n        print(\"\\nConverting the model in: {} ({}/{})\".format(base_model[\"model_base_dir\"], i+1, len(targets)))\n\n        for model_flavor in base_model[\"conversions\"]:\n            if model_flavor != \"q_float_16\":\n                txt = \"Model type: {} ...\".format(model_flavor)\n                txt += \" \" * (32 - len(txt))\n                print(txt, end=\" \", flush=True)\n                cfg.model_base_dir = base_model[\"model_base_dir\"]\n                cfg.tflite_conversion_type = model_flavor\n\n                try:\n                    create_tflm_source_files(cfg.tflite_model_path, cfg.eqcheck_data_path, cfg.tflm_info_path, cfg.tflm_templates_dir, cfg.tflm_save_dir)\n                    if os.path.exists(os.path.join(cfg.tflm_save_dir, 'exception.txt')):\n                        os.remove(os.path.join(cfg.tflm_save_dir, 'exception.txt'))\n                    converted_models_list.append(cfg.tflm_save_dir)\n                    output[i][\"flavors\"].append({\n                        \"flavor\": model_flavor,\n                        \"result\": \"success\"\n                    })\n                    print(\"Done\")\n                except Exception as e:\n                    error_message = traceback.format_exc()\n                    _save_tflm_exception(error_message, cfg.tflm_save_dir, delete_assets=True)\n                    output[i][\"flavors\"].append({\n                        \"flavor\": model_flavor,\n                        \"result\": \"failed\",\n                        \"error\": type(e).__name__,\n                        \"traceback\": traceback.format_exc()\n                    })\n                    print(\"Failed\")\n\n    os.makedirs(cfg.linkers_dir, exist_ok=True)\n    with open(os.path.join(cfg.linkers_dir, 'tflm_converted_models_list.yaml'), 'w') as f:\n        yaml.dump(converted_models_list, f, indent=4, sort_keys=False)\n\n    return output\n</code></pre>"},{"location":"api/utils/result_plotter/","title":"Result Plotter","text":""},{"location":"api/utils/result_plotter/#edgemark.models.utils.result_plotter","title":"edgemark.models.utils.result_plotter","text":""},{"location":"api/utils/result_plotter/#edgemark.models.utils.result_plotter.collect_data","title":"collect_data","text":"<pre><code>collect_data(data_file_path)\n</code></pre> <p>Collects the data from an excel (xlsx) file. The data related to inactive sheets, models, and model types will be excluded.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_path</code> <code>str</code> <p>The path to the excel file.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the data and the models metadata. - data (dict): The data collected from the excel file. It will be in the following format:     {plot_name: {\"color\", \"marker\", \"data_points\": [{\"model_name\", \"flash\", \"ram\", \"exe\": {\"n_tests\", \"average\", \"std\"}, \"error\": {\"n_tests\", \"average\", \"std\"}}]}} - models_metadata (dict): The metadata of the models. It will be in the following format:     {model_name: {\"display_name\", \"parameters\", \"MACs\"}}</p> Source code in <code>edgemark/models/utils/result_plotter.py</code> <pre><code>def collect_data(data_file_path):\n    \"\"\"\n    Collects the data from an excel (xlsx) file.\n    The data related to inactive sheets, models, and model types will be excluded.\n\n    Args:\n        data_file_path (str): The path to the excel file.\n\n    Returns:\n        tuple: A tuple containing the data and the models metadata.\n            - data (dict): The data collected from the excel file. It will be in the following format:\n                {plot_name: {\"color\", \"marker\", \"data_points\": [{\"model_name\", \"flash\", \"ram\", \"exe\": {\"n_tests\", \"average\", \"std\"}, \"error\": {\"n_tests\", \"average\", \"std\"}}]}}\n            - models_metadata (dict): The metadata of the models. It will be in the following format:\n                {model_name: {\"display_name\", \"parameters\", \"MACs\"}}\n    \"\"\"\n    def _resolve_merged_cells(file_path):\n        wb = openpyxl.load_workbook(file_path)\n        xls = pd.read_excel(file_path, sheet_name=None)\n\n        resolved_dfs = {}\n        for sheet_name, df in xls.items():\n            sheet = wb[sheet_name]\n\n            for merged_cell_range in sheet.merged_cells.ranges:\n                row_min, col_min, row_max, col_max = merged_cell_range.min_row, merged_cell_range.min_col, merged_cell_range.max_row, merged_cell_range.max_col\n                merged_value = sheet.cell(row=row_min, column=col_min).value\n\n                if merged_value is None or row_min &lt;= 2:    # skip empty rows and header row (first two rows)\n                    continue\n\n                for row in range(row_min, row_max + 1):\n                    for col in range(col_min, col_max + 1):\n                        # convert to zero-based indexing for pandas DataFrame\n                        # also, first row is header row and not counted in pandas DataFrame\n                        r = row - 2\n                        c = col - 1\n                        if pd.isna(df.iat[r, c]):\n                            df.iat[r, c] = merged_value\n\n            resolved_dfs[sheet_name] = df\n\n        return resolved_dfs\n\n    def _replace_nan_with_none(value):\n        return None if pd.isna(value) else value\n\n    sheets = _resolve_merged_cells(data_file_path)\n\n    # collect the sheets metadata\n    sheets_metadata = {}    # {sheet_name: {active, display_name, color}}\n    for sheet_name, sheet in sheets.items():\n        if sheet_name.lower() == \"metadata - sheets\":\n            if \"Sheet Name\" not in sheet.columns or \"Active\" not in sheet.columns or \"Display Name\" not in sheet.columns or \"Color\" not in sheet.columns:\n                raise ValueError(\"The metadata sheet should contain the columns 'Sheet Name', 'Active', 'Display Name', and 'Color'.\")\n\n            for _, row in sheet.iterrows():\n                if _replace_nan_with_none(row[\"Sheet Name\"]) is not None:\n                    sheets_metadata[row[\"Sheet Name\"]] = {\n                        \"active\": _replace_nan_with_none(row[\"Active\"]),\n                        \"display_name\": _replace_nan_with_none(row[\"Display Name\"]),\n                        \"color\": _replace_nan_with_none(row[\"Color\"])\n                    }\n\n    # collect the models metadata\n    models_metadata = {}    # {model_name: {display_name, parameters, MACs}}\n    for sheet_name, sheet in sheets.items():\n        if sheet_name.lower() == \"metadata - models\":\n            if \"Model Name\" not in sheet.columns or \"Active\" not in sheet.columns or \"Display Name\" not in sheet.columns or \"Parameters\" not in sheet.columns or \"MACs\" not in sheet.columns:\n                raise ValueError(\"The metadata sheet should contain the columns 'Model Name', 'Active', 'Display Name', 'Parameters', and 'MACs'.\")\n\n            for _, row in sheet.iterrows():\n                if _replace_nan_with_none(row[\"Model Name\"]) is not None:\n                    models_metadata[row[\"Model Name\"]] = {\n                        \"active\": _replace_nan_with_none(row[\"Active\"]),\n                        \"display_name\": _replace_nan_with_none(row[\"Display Name\"]),\n                        \"parameters\": _replace_nan_with_none(row[\"Parameters\"]),\n                        \"MACs\": _replace_nan_with_none(row[\"MACs\"])\n                    }\n\n    # collect the model types metadata\n    model_types_metadata = {}   # {model_type: {active, display_name, marker}}\n    for sheet_name, sheet in sheets.items():\n        if sheet_name.lower() == \"metadata - model types\":\n            if \"Model Type\" not in sheet.columns or \"Active\" not in sheet.columns or \"Display Name\" not in sheet.columns or \"Marker\" not in sheet.columns:\n                raise ValueError(\"The metadata sheet should contain the columns 'Model Type', 'Active', 'Display Name', and 'Marker'.\")\n\n            for _, row in sheet.iterrows():\n                if _replace_nan_with_none(row[\"Model Type\"]) is not None:\n                    model_types_metadata[row[\"Model Type\"]] = {\n                        \"active\": _replace_nan_with_none(row[\"Active\"]),\n                        \"display_name\": _replace_nan_with_none(row[\"Display Name\"]),\n                        \"marker\": _replace_nan_with_none(row[\"Marker\"])\n                    }\n\n    # collect the data\n    plots = {}   # {plot_name: {model_name, flash, ram, exe: {n_tests, average, std}, error: {n_tests, average, std}}}\n    for sheet_name, sheet in sheets.items():\n        if sheet_name.lower() == \"metadata - sheets\" or sheet_name.lower() == \"metadata - models\" or sheet_name.lower() == \"metadata - model types\":\n            continue\n        if sheet_name in sheets_metadata and sheets_metadata[sheet_name][\"active\"] is not None and sheets_metadata[sheet_name][\"active\"] == 0:\n            continue\n\n        # check if the sheet contains the required columns\n        if (\"Model Name\" not in sheet.columns or\n            \"Flash (kB)\" not in sheet.columns or\n            \"RAM (kB)\" not in sheet.columns or\n            \"Execution Time (ms)\" not in sheet.columns or\n            \"Error (MAE)\" not in sheet.columns):\n            raise ValueError(\"The data sheets should contain the columns 'Model Name', 'Flash (kB)', 'RAM (kB)', 'Execution Time (ms)', and 'Error (MAE)'.\")\n\n        # find the index of the column\n        columns = {}\n        columns[\"Model Name\"] = sheet.columns.get_loc(\"Model Name\")\n        columns[\"Flash (kB)\"] = sheet.columns.get_loc(\"Flash (kB)\")\n        columns[\"RAM (kB)\"] = sheet.columns.get_loc(\"RAM (kB)\")\n        columns[\"Execution Time (ms) - n tests\"] = sheet.columns.get_loc(\"Execution Time (ms)\")\n        columns[\"Execution Time (ms) - average\"] = columns[\"Execution Time (ms) - n tests\"] + 1\n        columns[\"Execution Time (ms) - std\"] = columns[\"Execution Time (ms) - n tests\"] + 2\n        columns[\"Error (MAE) - n tests\"] = sheet.columns.get_loc(\"Error (MAE)\")\n        columns[\"Error (MAE) - average\"] = columns[\"Error (MAE) - n tests\"] + 1\n        columns[\"Error (MAE) - std\"] = columns[\"Error (MAE) - n tests\"] + 2\n\n        # check if the second row contains the required columns\n        if sheet.iloc[0, columns[\"Execution Time (ms) - n tests\"]] != \"n tests\":\n            raise ValueError(\"The Execution Time (ms) column should contain 'n tests' in the second row.\")\n        if sheet.iloc[0, columns[\"Execution Time (ms) - average\"]] != \"average\":\n            raise ValueError(\"The Execution Time (ms) column should contain 'average' in the second row.\")\n        if sheet.iloc[0, columns[\"Execution Time (ms) - std\"]] != \"std\":\n            raise ValueError(\"The Execution Time (ms) column should contain 'std' in the second row.\")\n        if sheet.iloc[0, columns[\"Error (MAE) - n tests\"]] != \"n tests\":\n            raise ValueError(\"The Error (MAE) column should contain 'n tests' in the second row.\")\n        if sheet.iloc[0, columns[\"Error (MAE) - average\"]] != \"average\":\n            raise ValueError(\"The Error (MAE) column should contain 'average' in the second row.\")\n        if sheet.iloc[0, columns[\"Error (MAE) - std\"]] != \"std\":\n            raise ValueError(\"The Error (MAE) column should contain 'std' in the second row.\")\n\n        # data collection loop\n        for i, row in sheet.iterrows():\n            if i == 0:   # skip the second row as its a part of the column names\n                continue\n\n            model_name = _replace_nan_with_none(row[\"Model Name\"])\n            if model_name is not None:\n                model_type = _replace_nan_with_none(row[\"Model Type\"])\n                plot_name = sheet_name\n                if sheet_name in sheets_metadata and sheets_metadata[sheet_name][\"display_name\"] is not None:\n                    plot_name = sheets_metadata[sheet_name][\"display_name\"]\n\n                if (model_name in models_metadata and\n                    models_metadata[model_name][\"active\"] is not None and\n                    models_metadata[model_name][\"active\"] == 0):\n                    continue\n                if (model_type is not None and\n                    model_type in model_types_metadata and\n                    model_types_metadata[model_type][\"active\"] is not None and\n                    model_types_metadata[model_type][\"active\"] == 0):\n                    continue\n\n                if model_type is not None:\n                    if model_type in model_types_metadata and model_types_metadata[model_type][\"display_name\"] is not None:\n                            plot_name += \" [{}]\".format(model_types_metadata[model_type][\"display_name\"])\n                    else:\n                        plot_name += \" [{}]\".format(model_type)\n\n                if plot_name not in plots:\n                    color = None\n                    if sheet_name in sheets_metadata and sheets_metadata[sheet_name][\"color\"] is not None:\n                        color = sheets_metadata[sheet_name][\"color\"]\n                    marker = 'o'\n                    if model_type is not None and model_type in model_types_metadata and model_types_metadata[model_type][\"marker\"] is not None:\n                        marker = model_types_metadata[model_type][\"marker\"]\n                    plots[plot_name] = {\n                        \"color\": color,\n                        \"marker\": marker,\n                        \"data_points\": []\n                    }\n                plots[plot_name][\"data_points\"].append({\n                    \"model_name\": model_name,\n                    \"flash\": _replace_nan_with_none(row[\"Flash (kB)\"]),\n                    \"ram\": _replace_nan_with_none(row[\"RAM (kB)\"]),\n                    \"exe\": {\n                        \"n_tests\": _replace_nan_with_none(row.iloc[columns[\"Execution Time (ms) - n tests\"]]),\n                        \"average\": _replace_nan_with_none(row.iloc[columns[\"Execution Time (ms) - average\"]]),\n                        \"std\": _replace_nan_with_none(row.iloc[columns[\"Execution Time (ms) - std\"]])\n                    },\n                    \"error\": {\n                        \"n_tests\": _replace_nan_with_none(row.iloc[columns[\"Error (MAE) - n tests\"]]),\n                        \"average\": _replace_nan_with_none(row.iloc[columns[\"Error (MAE) - average\"]]),\n                        \"std\": _replace_nan_with_none(row.iloc[columns[\"Error (MAE) - std\"]])\n                    }\n                })\n\n    return plots, models_metadata\n</code></pre>"},{"location":"api/utils/result_plotter/#edgemark.models.utils.result_plotter.fill_plot_gaps","title":"fill_plot_gaps","text":"<pre><code>fill_plot_gaps(ax, line, opacity=0.5)\n</code></pre> <p>This function fills the gap caused by the None values with a dashed line.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes where the line is plotted.</p> required <code>line</code> <code>Line2D</code> <p>The line to fill the gap for.</p> required <code>opacity</code> <code>float</code> <p>The opacity of the dashed line.</p> <code>0.5</code> Source code in <code>edgemark/models/utils/result_plotter.py</code> <pre><code>def fill_plot_gaps(ax, line, opacity=0.5):\n    \"\"\"\n    This function fills the gap caused by the None values with a dashed line.\n\n    Args:\n        ax (matplotlib.axes.Axes): The axes where the line is plotted.\n        line (matplotlib.lines.Line2D): The line to fill the gap for.\n        opacity (float): The opacity of the dashed line.\n    \"\"\"\n    # fill the gap caused by the None values\n    x = line.get_xdata()\n    y = line.get_ydata()\n    for i in range(len(y) - 1):\n        if y[i] is not None and y[i+1] is None:\n            for j in range(i+1, len(y)):\n                if y[j] is not None:\n                    ax.plot([x[i], x[j]], [y[i], y[j]], color=line.get_color(), linestyle='dashed', alpha=opacity)\n                    break\n</code></pre>"},{"location":"api/utils/result_plotter/#edgemark.models.utils.result_plotter.main","title":"main","text":"<pre><code>main(data_file_path, save_dir)\n</code></pre> <p>This function plots the results of the benchmarking test.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_path</code> <code>str</code> <p>The path to the excel file containing the benchmarking results.</p> required <code>save_dir</code> <code>str</code> <p>The directory where the plots will be saved.</p> required Source code in <code>edgemark/models/utils/result_plotter.py</code> <pre><code>def main(data_file_path, save_dir):\n    \"\"\"\n    This function plots the results of the benchmarking test.\n\n    Args:\n        data_file_path (str): The path to the excel file containing the benchmarking results.\n        save_dir (str): The directory where the plots will be saved.\n    \"\"\"\n    plots, models_metadata = collect_data(data_file_path)\n    opacity = 0.8\n\n    present_models = []\n    for _, plot_vals in plots.items():\n        for data_point in plot_vals[\"data_points\"]:\n            if data_point[\"model_name\"] not in present_models:\n                present_models.append(data_point[\"model_name\"])\n\n    display_names = []\n    for model_name in present_models:\n        if model_name in models_metadata and models_metadata[model_name][\"display_name\"] is not None:\n            display_names.append(models_metadata[model_name][\"display_name\"])\n        else:\n            display_names.append(model_name)\n\n    parameters = []\n    MACs = []\n    for model_name in present_models:\n        if model_name in models_metadata:\n            parameters.append(models_metadata[model_name][\"parameters\"])\n            MACs.append(models_metadata[model_name][\"MACs\"])\n        else:\n            parameters.append(None)\n            MACs.append(None)\n\n    plot_params = any([param is not None for param in parameters])\n    plot_MACs = any([MAC is not None for MAC in MACs])\n\n    # plot models parameters and MACs\n    if plot_params and plot_MACs:\n        fig, ax1 = plt.subplots()\n        ax2 = ax1.twinx()\n        p1 = ax1.plot(display_names, parameters, 'tab:blue', label='Parameters', marker='o', alpha=opacity)\n        p2 = ax2.plot(display_names, MACs, 'tab:orange', label='MACs', marker='o', alpha=opacity)\n\n        fill_plot_gaps(ax1, p1[0])\n        fill_plot_gaps(ax2, p2[0])\n\n        ax1.set_yscale('log')\n        ax2.set_yscale('log')\n\n        ax1.set_xlabel('Models')\n        ax1.set_ylabel('Parameters')\n        ax2.set_ylabel('MACs')\n\n        lines = [p1[0], p2[0]]\n        labels = [line.get_label() for line in lines]\n        ax1.legend(lines, labels, loc='best')\n\n        ax1.grid(True, which='major', linestyle='-', linewidth=0.5)\n        ax1.grid(True, which='minor', linestyle='--', linewidth=0.2)\n        ax1.xaxis.set_tick_params(rotation=45)\n\n        save_light(fig, os.path.join(save_dir, \"params_MACs\"))\n        save_dark(fig, os.path.join(save_dir, \"dark\", \"params_MACs\"))\n\n    elif plot_params:\n        fig, ax = plt.subplots()\n        line = ax.plot(display_names, parameters, 'tab:blue', label='Parameters', marker='o', alpha=opacity)\n        fill_plot_gaps(ax, line[0])\n\n        ax.set_yscale('log')\n        ax.set_xlabel('Models')\n        ax.set_ylabel('Parameters')\n\n        ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n        ax.grid(True, which='minor', linestyle='--', linewidth=0.2)\n        ax.xaxis.set_tick_params(rotation=45)\n\n        save_light(fig, os.path.join(save_dir, \"params\"))\n        save_dark(fig, os.path.join(save_dir, \"dark\", \"params\"))\n\n    elif plot_MACs:\n        line = ax.plot(display_names, MACs, 'tab:orange', label='MACs', marker='o', alpha=opacity)\n        fill_plot_gaps(ax, line[0])\n\n        ax.set_yscale('log')\n        ax.set_xlabel('Models')\n        ax.set_ylabel('MACs')\n\n        ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n        ax.grid(True, which='minor', linestyle='--', linewidth=0.2)\n        ax.xaxis.set_tick_params(rotation=45)\n\n        save_light(fig, os.path.join(save_dir, \"MACs\"))\n        save_dark(fig, os.path.join(save_dir, \"dark\", \"MACs\"))\n\n    # collect all x points in all plots, respecting their order\n    figure_x_names = []\n    figure_x_display_names = []\n    figure_x_parameters = []\n    figure_x_MACs = []\n\n    for plot_name, plot_vals in plots.items():\n        last_found = -1\n        for data_point in plot_vals[\"data_points\"]:\n            model_name = data_point[\"model_name\"]\n            if model_name in figure_x_names:\n                last_found = figure_x_names.index(model_name)\n            else:\n                idx = last_found + 1\n                figure_x_names.insert(idx, model_name)\n                if model_name in models_metadata and models_metadata[model_name][\"display_name\"] is not None:\n                    figure_x_display_names.insert(idx, models_metadata[model_name][\"display_name\"])\n                else:\n                    figure_x_display_names.insert(idx, model_name)\n                if model_name in models_metadata:\n                    figure_x_parameters.insert(idx, models_metadata[model_name][\"parameters\"])\n                    figure_x_MACs.insert(idx, models_metadata[model_name][\"MACs\"])\n                else:\n                    figure_x_parameters.insert(idx, None)\n                    figure_x_MACs.insert(idx, None)\n                last_found = idx\n\n    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n    plots_names = []\n    plots_colors = []\n    plots_markers = []\n    plots_y_flash = []\n    plots_y_ram = []\n    plots_y_exe = []\n    plots_y_error = []\n    for plot_name, plot_vals in plots.items():\n        plots_names.append(plot_name)\n        plots_markers.append(plot_vals[\"marker\"])\n        if isinstance(plot_vals[\"color\"], float) or isinstance(plot_vals[\"color\"], int):\n            plots_colors.append(colors[int(plot_vals[\"color\"])-1])\n        else:\n            plots_colors.append(plot_vals[\"color\"])\n\n        plot_y_flash = [None] * len(figure_x_names)\n        plot_y_ram = [None] * len(figure_x_names)\n        plot_y_exe = [None] * len(figure_x_names)\n        plot_y_error = [None] * len(figure_x_names)\n        for data_point in plot_vals[\"data_points\"]:\n            model_name = data_point[\"model_name\"]\n\n            plot_y_flash[figure_x_names.index(model_name)] = data_point[\"flash\"]\n            plot_y_ram[figure_x_names.index(model_name)] = data_point[\"ram\"]\n            plot_y_exe[figure_x_names.index(model_name)] = data_point[\"exe\"][\"average\"]\n            plot_y_error[figure_x_names.index(model_name)] = data_point[\"error\"][\"average\"]\n\n        plots_y_flash.append(plot_y_flash)\n        plots_y_ram.append(plot_y_ram)\n        plots_y_exe.append(plot_y_exe)\n        plots_y_error.append(plot_y_error)\n\n    plots_x_parameters_valid = all([param is not None for param in figure_x_parameters])\n    plots_x_MACs_valid = all([MAC is not None for MAC in figure_x_MACs])\n\n    # plot models flash memory\n    fig, ax = plt.subplots()\n\n    for i in range(len(plots_names)):\n        plot_name = plots_names[i]\n        if plots_x_parameters_valid:\n            plot_x = figure_x_parameters\n            plot_y = plots_y_flash[i]\n            plot_x, plot_y = zip(*sorted(zip(plot_x, plot_y)))\n        else:\n            plot_x = figure_x_display_names\n            plot_y = plots_y_flash[i]\n\n        line = ax.plot(plot_x, plot_y, label=plot_name, color=plots_colors[i], marker=plots_markers[i], alpha=opacity)\n        fill_plot_gaps(ax, line[0])\n\n    if plots_x_parameters_valid:\n        ax.set_xscale('log')\n        ax.set_xlabel('Parameters')\n    else:\n        ax.set_xlabel('Models')\n    ax.set_yscale('log')\n    ax.set_ylabel('Flash Memory [KB]')\n    ax.legend(loc='best')\n\n    ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n    ax.grid(True, which='minor', linestyle='--', linewidth=0.2)\n    ax.xaxis.set_tick_params(rotation=45)\n\n    save_light(fig, os.path.join(save_dir, \"flash\"))\n    save_dark(fig, os.path.join(save_dir, \"dark\", \"flash\"))\n\n    # plot models RAM memory\n    fig, ax = plt.subplots()\n\n    for i in range(len(plots_names)):\n        plot_name = plots_names[i]\n        plot_x = figure_x_display_names\n        plot_y = plots_y_ram[i]\n\n        line = ax.plot(plot_x, plot_y, label=plot_name, color=plots_colors[i], marker=plots_markers[i], alpha=opacity)\n        fill_plot_gaps(ax, line[0])\n\n    ax.set_yscale('log')\n    ax.set_xlabel('Models')\n    ax.set_ylabel('RAM Memory [KB]')\n    ax.legend(loc='best')\n\n    ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n    ax.grid(True, which='minor', linestyle='--', linewidth=0.2)\n    ax.xaxis.set_tick_params(rotation=45)\n\n    save_light(fig, os.path.join(save_dir, \"ram\"))\n    save_dark(fig, os.path.join(save_dir, \"dark\", \"ram\"))\n\n    # plot models execution time\n    fig, ax = plt.subplots()\n\n    for i in range(len(plots_names)):\n        plot_name = plots_names[i]\n        if plots_x_MACs_valid:\n            plot_x = figure_x_MACs\n            plot_y = plots_y_exe[i]\n            plot_x, plot_y = zip(*sorted(zip(plot_x, plot_y)))\n        else:\n            plot_x = figure_x_display_names\n            plot_y = plots_y_exe[i]\n\n        line = ax.plot(plot_x, plot_y, label=plot_name, color=plots_colors[i], marker=plots_markers[i], alpha=opacity)\n        fill_plot_gaps(ax, line[0])\n\n    if plots_x_MACs_valid:\n        ax.set_xscale('log')\n        ax.set_xlabel('MACs')\n    else:\n        ax.set_xlabel('Models')\n    ax.set_yscale('log')\n    ax.set_ylabel('Execution Time [ms]')\n    ax.legend(loc='best')\n\n    ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n    ax.grid(True, which='minor', linestyle='--', linewidth=0.2)\n    ax.xaxis.set_tick_params(rotation=45)\n\n    save_light(fig, os.path.join(save_dir, \"exe\"))\n    save_dark(fig, os.path.join(save_dir, \"dark\", \"exe\"))\n\n    # plot models error\n    fig, ax = plt.subplots()\n\n    for i in range(len(plots_names)):\n        plot_name = plots_names[i]\n        plot_x = figure_x_display_names\n        plot_y = plots_y_error[i]\n\n        line = ax.plot(plot_x, plot_y, label=plot_name, color=plots_colors[i], marker=plots_markers[i], alpha=opacity)\n        fill_plot_gaps(ax, line[0])\n\n    ax.set_xlabel('Models')\n    ax.set_ylabel('Error [MAE]')\n    ax.legend(loc='best')\n\n    ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n    ax.grid(True, which='minor', linestyle='--', linewidth=0.2)\n    ax.xaxis.set_tick_params(rotation=45)\n\n    save_light(fig, os.path.join(save_dir, \"error\"))\n    save_dark(fig, os.path.join(save_dir, \"dark\", \"error\"))\n</code></pre>"},{"location":"api/utils/result_plotter/#edgemark.models.utils.result_plotter.save_dark","title":"save_dark","text":"<pre><code>save_dark(fig, file_path)\n</code></pre> <p>Save the figure with a dark background. Note: The attributes of the figure will be changed after using this function.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The figure to save.</p> required <code>file_path</code> <code>str</code> <p>The path to save the figure, excluding the file extension.</p> required Source code in <code>edgemark/models/utils/result_plotter.py</code> <pre><code>def save_dark(fig, file_path):\n    \"\"\"\n    Save the figure with a dark background.\n    Note: The attributes of the figure will be changed after using this function.\n\n    Args:\n        fig (matplotlib.figure.Figure): The figure to save.\n        file_path (str): The path to save the figure, excluding the file extension.\n    \"\"\"\n    fig.patch.set_facecolor((30/255, 30/255, 30/255))\n    axes = fig.get_axes()\n    for ax in axes:\n        ax.set_facecolor((30/255, 30/255, 30/255))\n        ax.xaxis.label.set_color('white')\n        ax.yaxis.label.set_color('white')\n        ax.title.set_color('white')\n        ax.tick_params(axis='x', colors='white')\n        ax.tick_params(axis='y', colors='white')\n        ax.spines['bottom'].set_color('white')\n        ax.spines['top'].set_color('white')\n        ax.spines['right'].set_color('white')\n        ax.spines['left'].set_color('white')\n\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    fig.savefig(file_path + \".png\", bbox_inches='tight', dpi=300)\n    fig.savefig(file_path + \".pdf\", bbox_inches='tight')\n</code></pre>"},{"location":"api/utils/result_plotter/#edgemark.models.utils.result_plotter.save_light","title":"save_light","text":"<pre><code>save_light(fig, file_path)\n</code></pre> <p>Save the figure with a light background. Note: The attributes of the figure will be changed after using this function.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The figure to save.</p> required <code>file_path</code> <code>str</code> <p>The path to save the figure, excluding the file extension.</p> required Source code in <code>edgemark/models/utils/result_plotter.py</code> <pre><code>def save_light(fig, file_path):\n    \"\"\"\n    Save the figure with a light background.\n    Note: The attributes of the figure will be changed after using this function.\n\n    Args:\n        fig (matplotlib.figure.Figure): The figure to save.\n        file_path (str): The path to save the figure, excluding the file extension.\n    \"\"\"\n    fig.patch.set_facecolor('white')\n    axes = fig.get_axes()\n    for ax in axes:\n        ax.set_facecolor('white')\n        ax.xaxis.label.set_color('black')\n        ax.yaxis.label.set_color('black')\n        ax.title.set_color('black')\n        ax.tick_params(axis='x', colors='black')\n        ax.tick_params(axis='y', colors='black')\n        ax.spines['bottom'].set_color('black')\n        ax.spines['top'].set_color('black')\n        ax.spines['right'].set_color('black')\n        ax.spines['left'].set_color('black')\n\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    fig.savefig(file_path + \".png\", bbox_inches='tight', dpi=300)\n    fig.savefig(file_path + \".pdf\", bbox_inches='tight')\n</code></pre>"},{"location":"api/utils/utils/","title":"Utils","text":""},{"location":"api/utils/utils/#edgemark.models.utils.utils","title":"edgemark.models.utils.utils","text":""},{"location":"api/utils/utils/#edgemark.models.utils.utils.find_target_files","title":"find_target_files","text":"<pre><code>find_target_files(dir)\n</code></pre> <p>Find all target files in the given directory and its subdirectories. The files and directories starting with '.' are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>The directory to search for target files.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of relative paths to the target files.</p> Source code in <code>edgemark/models/utils/utils.py</code> <pre><code>def find_target_files(dir):\n    \"\"\"\n    Find all target files in the given directory and its subdirectories.\n    The files and directories starting with '.' are ignored.\n\n    Args:\n        dir (str): The directory to search for target files.\n\n    Returns:\n        list: A list of relative paths to the target files.\n    \"\"\"\n\n    target_files = []\n\n    for dirpath, dirnames, filenames in os.walk(dir):\n        dirnames[:] = [d for d in dirnames if not d.startswith('.')]\n\n        for filename in filenames:\n            if filename.startswith('.'):\n                continue\n\n            if filename.endswith('.yaml') or filename.endswith('.yml'):\n                full_path = os.path.join(dirpath, filename)\n                relative_path = os.path.relpath(full_path, start=dir)\n                relative_path = relative_path.replace('\\\\', '/')\n                target_files.append(relative_path)\n\n    return target_files\n</code></pre>"},{"location":"api/utils/utils/#edgemark.models.utils.utils.get_abs_path","title":"get_abs_path","text":"<pre><code>get_abs_path(relative_path)\n</code></pre> <p>Get the absolute path of the given relative path to the root directory of the project.</p> <p>Parameters:</p> Name Type Description Default <code>relative_path</code> <code>str</code> <p>The relative path to the root directory of the project.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The absolute path of the given relative path to the root directory of the project.</p> Source code in <code>edgemark/models/utils/utils.py</code> <pre><code>def get_abs_path(relative_path):\n    \"\"\"\n    Get the absolute path of the given relative path to the root directory of the project.\n\n    Args:\n        relative_path (str): The relative path to the root directory of the project.\n\n    Returns:\n        str: The absolute path of the given relative path to the root directory of the project.\n    \"\"\"\n    return os.path.abspath(relative_path).replace('\\\\', '/')\n</code></pre>"},{"location":"results/","title":"Results","text":"<p>In pursue of evaluating the performance of different setups, we conducted a series of studies. The results are detailed on the following pages.</p> <p>Test Demon!</p> <p>The results you will find in the following pages are based on thousands of tests, with each test taking approximately 2-3 minutes to complete.</p> <p>Each test measures four key metrics:</p> <ul> <li> <p>Execution Time: The time it takes to run the model on the target device. It is measured by iteratively running the model for 10 times and taking the average. (1)</p> <ol> <li> The input to the model remains the same for each iteration. Changing the input, having a warm-up phase, or re-running the test have negligible to no effect on the results. Additionally, the standard deviation of the execution time is negligible and is saved in the detailed results.</li> </ol> </li> <li> <p>Error: The error rate of the model. It is calculated as the average of the normalized absolute difference between the model's output and the expected output (the output produced when the model is run on a PC). To determine this, 10 different inputs are fed into the model, and the average error rate is computed. The standard deviation of the error rate is also included in the detailed results. An error rate below 0.05 is generally considered acceptable for a model.</p> </li> <li> <p>Flash Size: The program's flash memory size. (1)</p> <ol> <li> The GCC compiler reports the sizes of the text, data, and bss sections. <p>By default, Flash = text + data, but since the tested tools have a negligible effect on the data section and the data section is primarily used for storing data samples (which we want exclude from affecting the flash size), we assume Flash = text.</p> <p><ul> <li>For the NUCLEO-L4R5ZI C/C++ project, the base program (including printf and timer functionalities but without any model or library) occupies (text=28000, data=476, and bss=3568) bytes.</li> <li>For the Renesas RX65N C project, the base program occupies (text=30972, data=3508, and bss=7368) bytes.</li> <li>For the Renesas RX65N C++ project, the base program occupies (text=33372, data=3500, and bss=7376) bytes.</li> <li>The minimum program size for each tool can be inferred from its FC_0 model data.</li> </ul></p></li> </ol> </li> <li> <p>RAM Usage: The program's RAM usage. (1)</p> <ol> <li> Since all tested tools follow a static memory allocation strategy, the RAM usage can be inferred from the GCC compiler's report. Typically, RAM = data + bss, but since the data section primarily holds data samples (which we do not want to affect RAM calculations), we assume RAM = bss. <p>For TFLM, we set the arena_size parameter, which modifies the size of the bss section. To find the minimum viable arena_size for each model, we incrementally increased the arena_size with a small step-size until the program ran successfully. The bss size of the program with the minimum arena_size is taken as the program's RAM usage.</p></li> </ol> </li> </ul> <p>Subjective Summary</p> <p>The Summary section of each study offers a brief overview of the results. Please note that these summaries are based on overall behavior of models and may be somewhat subjective.</p>"},{"location":"results/#studies","title":"Studies","text":"<ul> <li>TFLM Quantizations: Evaluating the performance of different quantization schemes.</li> <li>TFLM Pruning and Clustering: Exploring the effects of pruning and clustering.</li> <li>TFLM vs Edge Impulse: A comparison between TFLM and Edge Impulse.</li> <li>TFLM vs Ekkono: A comparison between TFLM and Ekkono.</li> <li>TFLM vs eAI Translator: A comparison between TFLM and Renesas eAI Translator.</li> <li>RNN, LSTM, GRU: Comparing the performance of different RNNs.</li> <li>Compiler Optimization Levels: The impact of varying compiler optimization levels.</li> <li>Importance of FPU: Situations where the Floating Point Unit (FPU) becomes beneficial.</li> <li>STM vs Renesas: A comparison between the NUCLEO-L4R5ZI and Renesas RX65N boards.</li> <li>GCC vs CCRX: A comparison of the GCC and CCRX compilers for the Renesas RX65N.</li> </ul>"},{"location":"results/#models","title":"Models","text":"<p>In our experiments, we used four types of models:</p> <ul> <li>FC: Fully connected neural network.</li> <li>CNN: Convolutional neural network.</li> <li>RNN: Recurrent neural network. It can be either Simple RNN, LSTM, or GRU.</li> <li>TinyMLPerf: Models from the MLPerf Tiny benchmark suite.</li> </ul>"},{"location":"results/#fc","title":"FC","text":"<p>We utilized 11 FC models in our experiments. These models consist of multiple fully connected layers with Sigmoid or ReLU activation functions. Classification models include a Softmax layer at the end. Some models may also use dropout and batch normalization layers.</p> <p>FC_0 is the simplest model, containing just one neuron in both the input and output layers. It's useful for comparing the minimum resource requirements across different tools.</p> <p>As the model number increases, so do the network\u2019s size and complexity. The figure below provides details on the number of parameters and MACs in each model.</p> FC parameters and MACs"},{"location":"results/#cnn","title":"CNN","text":"<p>We employed 7 CNN models in our experiments. These models consist of multiple convolutional layers and, in some cases, additional fully connected (FC) layers. In addition to activation functions (Sigmoid, ReLU), dropout, and batch normalization layers, CNN models may also feature pooling layers.</p> <p>As with the FC models, complexity increases with model number. The figure below outlines the number of parameters and MACs for each CNN model.</p> CNN parameters and MACs"},{"location":"results/#rnn","title":"RNN","text":"<p>We utilized 7 different RNN models during our experiments. These include three Simple RNN models of varying sizes (simple_0 represents an almost minimal RNN), two additional Simple RNNs trained on a collection of Shakespeare\u2019s works, one LSTM model, and one GRU model. The Shakespeare models include an embedding layer at the beginning.</p> <p>The table below provides detailed information about the RNN models.</p> Model RNN Units Sequence Length Parameters MACs Simple 0 1 2 5 9 Simple 1 64 100 8288 827200 Simple 2 128 100 32960 3292800 Shakespeare 1 64 100 12513 1056300 Shakespeare 2 128 100 37249 3321900 LSTM 64 100 26912 2702400 GRU 64 100 20896 2094400"},{"location":"results/#tinymlperf","title":"TinyMLPerf","text":"<p>The MLPerf Tiny benchmark suite includes 4 models. While in this project we have the possibility to create these models with different options, we have loaded the default models provided by the benchmark suite.</p> <p>Below is a figure that highlights the number of parameters and MACs for these models.</p> TinyMLPerf parameters and MACs"},{"location":"results/Compiler_optimization_levels/","title":"Compiler Optimization Levels","text":"<p>In this study, we aim to find out the impact of varying compiler optimization levels (O3, Of, and Os) on the performance of TFLM models.</p> <p>We use FC and CNN models deployed on the NUCLEO-L4R5ZI board for our experiments.</p> <p></p> <p>Model Type:  Any FC CNN </p>"},{"location":"results/Compiler_optimization_levels/#models","title":"Models","text":"FC parameters and MACs CNN parameters and MACs"},{"location":"results/Compiler_optimization_levels/#error","title":"Error","text":"FC - NUCLEO-L4R5ZI CNN - NUCLEO-L4R5ZI"},{"location":"results/Compiler_optimization_levels/#execution-time","title":"Execution Time","text":"FC - NUCLEO-L4R5ZI CNN - NUCLEO-L4R5ZI"},{"location":"results/Compiler_optimization_levels/#flash-size","title":"Flash Size","text":"FC - NUCLEO-L4R5ZI CNN - NUCLEO-L4R5ZI"},{"location":"results/Compiler_optimization_levels/#ram-usage","title":"RAM Usage","text":"FC - NUCLEO-L4R5ZI CNN - NUCLEO-L4R5ZI"},{"location":"results/Compiler_optimization_levels/#summary","title":"Summary","text":"<ul> <li> <p>Model Correctness: The optimization levels do not change the correctness of the models.</p> </li> <li> <p>Execution Time: The O3 and Of are the same and faster than Os.</p> </li> <li> <p>Flash Size: The flash size of Os is slightly better than O3 and Of. (1)</p> <ol> <li> The difference is in the program's base size (without the model) and vanishes as the model size increases.</li> </ol> </li> <li> <p>RAM Usage: The required RAM of all optimization levels is almost the same.</p> </li> <li> <p>Conclusion: The O3 and Of optimization levels are similar and better than Os in terms of execution time. If the flash size is a concern, Os might be slightly better than O3 and Of.</p> </li> </ul>"},{"location":"results/GCC_vs_CCRX/","title":"GCC vs CCRX","text":"<p>In this study, we look for the performance differences between a publicly available compiler (GCC) and a proprietary industrial compiler (CCRX) for the Renesas RX65N board.</p> <p>FC and CNN models converted by Renesas eAI Translator were used in our experiments.</p> <p>No Flash Size or RAM Usage</p> <p>Since we could not obtain the flash size and RAM usage for the CCRX compiler, we have omitted these metrics from the study.</p> <p></p> <p>Model Type:  Any FC CNN </p>"},{"location":"results/GCC_vs_CCRX/#models","title":"Models","text":"FC parameters and MACs CNN parameters and MACs"},{"location":"results/GCC_vs_CCRX/#error","title":"Error","text":"FC - Renesas RX65N CNN - Renesas RX65N"},{"location":"results/GCC_vs_CCRX/#execution-time","title":"Execution Time","text":"FC - Renesas RX65N CNN - Renesas RX65N"},{"location":"results/GCC_vs_CCRX/#summary","title":"Summary","text":"<ul> <li> <p>Model Correctness: As a result of using the eAI Translator, we have seen that some models fail to output correct results using GCC. The same applies to CCRX, and even the int8 only version of CNN_4 which previously had acceptable error, now has a high error rate. Other than these cases, the two have the same error rates.</p> </li> <li> <p>Execution Time: The execution time of GCC is better than CCRX for all models.</p> </li> <li> <p>Conclusion: GCC is a bit more reliable than CCRX and executes the models faster.</p> </li> </ul>"},{"location":"results/Importance_of_FPU/","title":"Importance of FPU","text":"<p>This study aims to reveal the importance of the Floating Point Unit (FPU) in TinyML applications.</p> <p>We use FC and CNN models deployed on the NUCLEO-L4R5ZI board for our experiments.</p> <p></p> <p>Model Type:  Any FC CNN </p>"},{"location":"results/Importance_of_FPU/#models","title":"Models","text":"FC parameters and MACs CNN parameters and MACs"},{"location":"results/Importance_of_FPU/#error","title":"Error","text":"FC - NUCLEO-L4R5ZI CNN - NUCLEO-L4R5ZI"},{"location":"results/Importance_of_FPU/#execution-time","title":"Execution Time","text":"FC - NUCLEO-L4R5ZI CNN - NUCLEO-L4R5ZI"},{"location":"results/Importance_of_FPU/#flash-size","title":"Flash Size","text":"FC - NUCLEO-L4R5ZI CNN - NUCLEO-L4R5ZI"},{"location":"results/Importance_of_FPU/#ram-usage","title":"RAM Usage","text":"FC - NUCLEO-L4R5ZI CNN - NUCLEO-L4R5ZI"},{"location":"results/Importance_of_FPU/#summary","title":"Summary","text":"<p>The only effect of the FPU is on the execution time of the basic models which is very significant. Still, if using int8 only, utilizing the CMSIS-NN library can be even more beneficial and there is no need for the FPU.</p>"},{"location":"results/RNN_LSTM_GRU/","title":"RNN, LSTM, GRU","text":"<p>This study highlights the resource requirements of Simple RNN, LSTM, and GRU models.</p> <p>Only TFLM could be used for these models (1). Also, all the models were deployed on the NUCLEO-L4R5ZI board.</p> <ol> <li> Edge Impulse requires an enterprise account, Renesas eAI Translator cannot convert RNNs (might be solved by further adjustments), and Ekkono does not support RNNs.</li> </ol> <p>Simple 0 is excluded from the figures due to its negligible resource requirements compared to the other models and keeping the figures readable. Simple 2 is also excluded from the figures because its basic version failed to run. All models' information is available in the tables below.</p> Model Variant Parameters MACs Error Exe (ms) Flash (kB) RAM (kB) Simple 0 basic 5 9 0 0.107 110.4375 8.140625 Simple 0 int8 only 5 9 0.005785 0.14567 111.640625 7.1328125 Simple 1 basic 8288 827200 0 107.2074 218.65625 112.6328125 Simple 1 int8 only 8288 827200 0.004366 292.9049 590.8125 103.2578125 Simple 2 basic 32960 3292800 - - - - Simple 2 int8 only 32960 3292800 0.004072 292.9049 615.1875 106.3828125 Shakespeare 1 basic 12513 1056300 0 141.1776 251.578125 127.625 Shakespeare 1 int8 only 12513 1056300 0.020862 168.4068 620.84375 107.5859375 Shakespeare 2 basic 37249 3321900 0 377.1719 348.203125 175.625 Shakespeare 2 int8 only 37249 3321900 0.021797 323.4767 645.1953125 107.5859375 LSTM basic 26912 2702400 0 362.2378 472.9609375 268.734375 LSTM int8 only 26912 2702400 0.013989 565.3515 769.8203125 258.359375 GRU basic 20896 2094400 0 276.7367 496.984375 298.734375 GRU int8 only 20896 2094400 0.044773 374.4158 809.7890625 295.359375 <p> Simple_1 is named Simple in figures.</p> <p></p>"},{"location":"results/RNN_LSTM_GRU/#models","title":"Models","text":"RNN parameters and MACs"},{"location":"results/RNN_LSTM_GRU/#error","title":"Error","text":"RNN error"},{"location":"results/RNN_LSTM_GRU/#execution-time","title":"Execution Time","text":"RNN execution time"},{"location":"results/RNN_LSTM_GRU/#flash-size","title":"Flash Size","text":"RNN flash size"},{"location":"results/RNN_LSTM_GRU/#ram-usage","title":"RAM Usage","text":"RNN RAM usage"},{"location":"results/RNN_LSTM_GRU/#summary","title":"Summary","text":"<ul> <li> <p>Model Correctness: Except the basic version of Simple 2 which runs out of memory, all models got relatively acceptable error rates. The most concerning case belongs to GRU which might require some attention.</p> </li> <li> <p>Execution Time: It is surprising that except for Shakespeare 2, the int8 only versions of the models have higher execution times.</p> </li> <li> <p>Flash Size: It is surprising that the int8 only versions of the models have larger flash sizes.</p> </li> <li> <p>RAM Usage: int8 only requires less RAM. (1)</p> <ol> <li> We expected the int8 only to have the advantage with a bigger margin.</li> </ol> </li> <li> <p>Conclusion: In most cases, the basic version of the models is more efficient.</p> </li> </ul> LSTM vs GRU <p>It's believed that LSTM is more powerful than GRU, but GRU is more parameter and computation efficient. Also, in our study, we see that LSTM has more parameters and MACs than GRU. However, it is interesting that using TFLM, GRU has a larger flash size and RAM usage than LSTM. Still, it's executed faster than LSTM.</p>"},{"location":"results/STM_vs_Renesas/","title":"STM vs Renesas","text":"<p>In this study, we are interested in comparing the performance of two similar boards from different manufacturers: the NUCLEO-L4R5ZI from STMicroelectronics and the Renesas RX65N.</p> <p>Hobby Experiment</p> <p>This study is a hobby experiment and should not be considered as a professional benchmark.</p> <p>All four types of models were tested on both boards.</p> <p></p> <p>Model Type:  Any FC CNN RNN TinyMLPerf </p>"},{"location":"results/STM_vs_Renesas/#models","title":"Models","text":"FC parameters and MACs CNN parameters and MACs RNN parameters and MACs TinyMLPerf parameters and MACs"},{"location":"results/STM_vs_Renesas/#error","title":"Error","text":"FC error CNN error RNN error TinyMLPerf error"},{"location":"results/STM_vs_Renesas/#execution-time","title":"Execution Time","text":"FC execution time CNN execution time RNN execution time TinyMLPerf execution time"},{"location":"results/STM_vs_Renesas/#flash-size","title":"Flash Size","text":"FC flash size CNN flash size RNN flash size TinyMLPerf flash size"},{"location":"results/STM_vs_Renesas/#ram-usage","title":"RAM Usage","text":"FC RAM usage CNN RAM usage RNN RAM usage TinyMLPerf RAM usage"},{"location":"results/STM_vs_Renesas/#summary","title":"Summary","text":"<ul> <li> <p>Model Correctness: The Renesas board fails to run some of the models. Other than that, the two boards provide similar results. (1)</p> <ol> <li> Except for the int8 only version of TinyMLPerf_MBNet model.</li> </ol> </li> <li> <p>Execution Time:</p> <ul> <li> <p>The Renesas board did not utilize the CMSIS-NN library, so we should exclude the int8 only versions from the comparison.</p> </li> <li> <p>The Renesas board is just slightly faster than the STM board.</p> </li> </ul> </li> <li> <p>Flash Size: STM is a bit better in terms of flash size.</p> </li> <li> <p>RAM Usage: STM is better in terms of RAM usage.</p> </li> <li> <p>Conclusion: The two boards seem to have a relatively similar performance. The STM board might be slightly better in some cases. (1)</p> <ol> <li> These boards have many settings that might favor one over the other in certain cases. Our study was conducted in an almost default settings with a good optimization level, but it is not comprehensive enough to cover all possible scenarios.</li> </ol> </li> </ul>"},{"location":"results/TFLM_pruning_and_clustering/","title":"TFLM Pruning and Clustering","text":"<p>This study explores the impact of pruning and clustering on the performance of neural networks.</p> <p>FC and CNN models were evaluated on the NUCLEO-L4R5ZI board, using 50% sparsity for pruning and clustering with 16 centroids.</p> <p></p> <p>Model Type:  Any FC CNN </p>"},{"location":"results/TFLM_pruning_and_clustering/#models","title":"Models","text":"FC parameters and MACs CNN parameters and MACs"},{"location":"results/TFLM_pruning_and_clustering/#error","title":"Error","text":"FC error CNN error"},{"location":"results/TFLM_pruning_and_clustering/#execution-time","title":"Execution Time","text":"FC execution time CNN execution time"},{"location":"results/TFLM_pruning_and_clustering/#flash-size","title":"Flash Size","text":"FC flash size CNN flash size"},{"location":"results/TFLM_pruning_and_clustering/#ram-usage","title":"RAM Usage","text":"FC RAM usage CNN RAM usage"},{"location":"results/TFLM_pruning_and_clustering/#summary","title":"Summary","text":"<p>Despite the popularity of pruning and clustering in reducing the size of neural networks, our results demonstrate that these techniques don't improve model performance and, in fact, they increase the error rate. This happens because the pruned or clustered weights still need to be stored in memory (occupying the same space as the original weights), and operations involving these weights still need to be executed (e.g., <code>x * 0</code> takes just as long as <code>x * y</code>).</p> <p>One solution is to use structured pruning, which essentially involves designing a new model architecture\u2014such as removing specific neurons or channels. Alternatively, you could use a hardware accelerator optimized for sparse weights or clustering. Another option is to \u201cunfold\u201d the matrix multiplication and eliminate unnecessary operations, though this approach requires a large amount of flash memory, making it impractical for most applications.</p> <p>In conclusion, we advise against using unstructured pruning or clustering unless you have access to a hardware accelerator that specifically supports these techniques.</p>"},{"location":"results/TFLM_quantizations/","title":"TFLM Quantizations","text":"<p>In this study, we evaluate the performance of various quantization schemes using the TensorFlow Lite for Microcontrollers (TFLM) platform. We chose TFLM because it is the only framework that supports the majority of available quantization methods. (1)</p> <ol> <li> TFLM supports basic, dynamic, int8, int8 only, 16x8, and 16x8 only quantizations, but does not support float16. In contrast, Edge Impulse and Renesas eAI Translator only support basic and int8 only quantizations, while Ekkono supports only basic quantization.</li> </ol> <p>All FC and CNN models were tested on the NUCLEO-L4R5ZI and RenesasRX65N boards.</p> <p>CMSIS-NN</p> <p>The CMSIS-NN library can help to accelerate the execution of quantized models (1). This library is designed for ARM Cortex-M microcontrollers and was used with the NUCLEO-L4R5ZI board. Renesas has developed its own CMSIS-NN library for RX microcontrollers, but it was not employed in our evaluations.</p> <ol> <li> \"CMSIS-NN supports int8 and int16 activations and int8 weights. It also supports int4 packed weights to some extent.\" reference</li> </ol> <p></p> <p>Model Type:  Any FC CNN  \u00a0\u00a0\u00a0\u00a0Board:  Any NUCLEO-L4R5ZI RenesasRX65N </p>"},{"location":"results/TFLM_quantizations/#models","title":"Models","text":"FC parameters and MACs CNN parameters and MACs"},{"location":"results/TFLM_quantizations/#error","title":"Error","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N CNN - NUCLEO-L4R5ZI CNN - RenesasRX65N"},{"location":"results/TFLM_quantizations/#execution-time","title":"Execution Time","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N CNN - NUCLEO-L4R5ZI CNN - RenesasRX65N"},{"location":"results/TFLM_quantizations/#flash-size","title":"Flash Size","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N CNN - NUCLEO-L4R5ZI CNN - RenesasRX65N"},{"location":"results/TFLM_quantizations/#ram-usage","title":"RAM Usage","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N CNN - NUCLEO-L4R5ZI CNN - RenesasRX65N"},{"location":"results/TFLM_quantizations/#summary","title":"Summary","text":"<ul> <li> <p>Model Correctness:</p> <ul> <li> <p>The float16 quantization scheme is not supported by any of the tested frameworks and should be disregarded.</p> </li> <li> <p>The dynamic quantization also lacks a proper support (1). So, it is recommended to avoid using the dynamic quantization.</p> <ol> <li> In most cases, either the models fail to run on the boards, or they have an unacceptable error. Even when ignoring the errors, we cannot say that the dynamic quantization has any superiority over other types of quantizations.</li> </ol> </li> <li> <p>The RenesasRX65N board is unable to run some of the models. (1)</p> <ol> <li> The RenesasRX65N board cannot run the basic, int8 only, and 16x8 only versions of the FC_1 and FC_2 models. The program halts during their execution.</li> </ol> </li> <li> <p>The error rates of all other quantization schemes are acceptable. (1)</p> <ol> <li> basic is perfect, int8 variants and 16x8 variants have a negligible errors, 16x8 being better than int8.</li> </ol> </li> </ul> </li> <li> <p>Execution Time: It is complicated and hard to say which quantization scheme is better.</p> <ul> <li>FC Models: The only variant of int8 and 16x8 is faster.<ul> <li>Small Models: 16x8 only is the best.</li> <li>Large Models: int8 only is the best.</li> </ul> </li> <li> <p>CNN Models: The basic model is very slow (1). The int8 and 16x8 variants are close to each other. (2)</p> <ol> <li> Because of using the CMSIS-NN, the quantized models (especially the CNN quantized models) experience a significant speedup on the NUCLEO-L4R5ZI board. This library is not utilized on the RenesasRX65N board.</li> <li> Still, to some point, following the same pattern as FC for small and large models.</li> </ol> </li> </ul> </li> <li> <p>Flash Size: The basic model gets worse as model size increases (1). The others are almost the same. (2)</p> <ol> <li> All quantization schemes start with relatively similar flash sizes, but as the model scales, the basic version's flash size increases fourfold (four bytes per parameter), whereas the other variants increase by only one byte per parameter.</li> <li> The only variants of int8 and 16x8 are slightly more efficient in terms of flash usage.</li> </ol> </li> <li> <p>RAM Usage: It is complicated, but int8 only is either equally the best or better than all others.</p> </li> <li> <p>Conclusion: The choice of quantization scheme depends on many factors, however, the int8 only quantization is a good choice for most cases. (1)</p> <ol> <li> The basic and int8 only variants are the two model types that will be used in other studies.</li> </ol> </li> </ul>"},{"location":"results/TFLM_vs_Edge_Impulse/","title":"TFLM vs Edge Impulse","text":"<p>The goal of this study is to compare the performance of TensorFlow Lite for Microcontrollers (TFLM) and Edge Impulse.</p> <p>According to the Edge Impulse documentation, an enterprise account is needed to run RNN models. As a result, this study focuses on FC, CNN, and TinyMLPerf models.</p> <p></p> <p>Model Type:  Any FC CNN TinyMLPerf  \u00a0\u00a0\u00a0\u00a0Board:  Any NUCLEO-L4R5ZI RenesasRX65N </p>"},{"location":"results/TFLM_vs_Edge_Impulse/#models","title":"Models","text":"FC parameters and MACs CNN parameters and MACs TinyMLPerf parameters and MACs"},{"location":"results/TFLM_vs_Edge_Impulse/#error","title":"Error","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N CNN - NUCLEO-L4R5ZI CNN - RenesasRX65N TinyMLPerf - NUCLEO-L4R5ZI TinyMLPerf - RenesasRX65N"},{"location":"results/TFLM_vs_Edge_Impulse/#execution-time","title":"Execution Time","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N CNN - NUCLEO-L4R5ZI CNN - RenesasRX65N TinyMLPerf - NUCLEO-L4R5ZI TinyMLPerf - RenesasRX65N"},{"location":"results/TFLM_vs_Edge_Impulse/#flash-size","title":"Flash Size","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N CNN - NUCLEO-L4R5ZI CNN - RenesasRX65N TinyMLPerf - NUCLEO-L4R5ZI TinyMLPerf - RenesasRX65N"},{"location":"results/TFLM_vs_Edge_Impulse/#ram-usage","title":"RAM Usage","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N CNN - NUCLEO-L4R5ZI CNN - RenesasRX65N TinyMLPerf - NUCLEO-L4R5ZI TinyMLPerf - RenesasRX65N"},{"location":"results/TFLM_vs_Edge_Impulse/#summary","title":"Summary","text":"<ul> <li> <p>Model Correctness:</p> <ul> <li> <p>Since TinyMLPerf_MBNet was too large for all tests except its int8 only version with TFLM, we have excluded it from the comparison.</p> </li> <li> <p>Some models failed to run on the RenesasRX65N board. (1)</p> <ol> <li> In all cases, the program halts for an unknown reason.</li> </ol> </li> <li> <p>The error rates of the remaining models are acceptable. (1)</p> <ol> <li> basic is perfect, Edge Impulse is better than TFLM in int8 only models.</li> </ol> </li> </ul> </li> <li> <p>RenesasRX65N:</p> <ul> <li> <p>On the Renesas board, TFLM outperforms Edge Impulse in terms of execution time, flash size, and RAM usage (1). As a result, TFLM is recommended for this board.</p> <ol> <li> One contributing factor could be that the GCC compiler had link-time optimization (<code>-flto</code>) enabled for TFLM, but this was not possible for Edge Impulse (Edge Impulse encountered errors with <code>-flto</code>). However, this cannot be the only reason, because in a few tested scenarios where we have turned off <code>-flto</code> for TFLM, the results were still better than Edge Impulse.</li> </ol> </li> </ul> </li> <li> <p>NUCLEO-L4R5ZI:</p> <ul> <li> <p>Execution Time: TFLM performed better for FC models, particularly for smaller models. For other models, the performance of both is similar, with a slight edge for TFLM.</p> </li> <li> <p>Flash Size: Edge Impulse is better than TFLM.</p> </li> <li> <p>RAM Usage: For small FC models, Edge Impulse is slightly better. For the others, the two are almost the same.</p> </li> </ul> </li> <li> <p>Conclusion:</p> <ul> <li> <p>For the RenesasRX65N board, TFLM is the preferable choice</p> </li> <li> <p>For the NUCLEO-L4R5ZI board, TFLM is recommended if execution time is the priority. However, if flash size and RAM usage are more critical, Edge Impulse may be the better option.</p> </li> </ul> </li> </ul>"},{"location":"results/TFLM_vs_Ekkono/","title":"TFLM vs Ekkono","text":"<p>The goal of this study is to compare the performance of TensorFlow Lite for Microcontrollers (TFLM) and Ekkono.</p> <p>Ekkono is only able to run FC models, so we have limited our comparison to FC models only.</p> <p>Slightly Modified Models</p> <p>Since Ekkono is not able to do classification, we have slightly changed some models to make them suitable for regression. The changes are minimal and should not have a noticeable impact on the results.</p> <p></p> <p>Board:  Any NUCLEO-L4R5ZI RenesasRX65N </p>"},{"location":"results/TFLM_vs_Ekkono/#models","title":"Models","text":"FC parameters and MACs"},{"location":"results/TFLM_vs_Ekkono/#error","title":"Error","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N"},{"location":"results/TFLM_vs_Ekkono/#execution-time","title":"Execution Time","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N"},{"location":"results/TFLM_vs_Ekkono/#flash-size","title":"Flash Size","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N"},{"location":"results/TFLM_vs_Ekkono/#ram-usage","title":"RAM Usage","text":"FC - NUCLEO-L4R5ZI FC - RenesasRX65N"},{"location":"results/TFLM_vs_Ekkono/#summary","title":"Summary","text":"<ul> <li> <p>Model Correctness:</p> <ul> <li> <p>Some models failed to run on the RenesasRX65N board. (1)</p> <ol> <li> For FC_1 and FC_2 models, the program halts for an unknown reason.</li> </ol> </li> <li> <p>Ekkono and TFLM basic are perfect. TFLM int8 only has a bit of error which is acceptable.</p> </li> </ul> </li> <li> <p>Execution Time: For small models, Ekkono is faster than TFLM. However, as the model size increases, TFLM becomes faster.</p> </li> <li> <p>Flash Size: For small models, Ekkono has a smaller flash size. However, as the model size increases, TFLM int8 only becomes more efficient. (1)</p> <ol> <li> The Ekkono library itself has a smaller footprint than the TFLM library. As the model grows, the TFLM library's overhead becomes negligible compared to the model size. In case of TFLM int8 only, the model size grows 1/4th compared to TFLM basic or Ekkono which results in a smaller flash size.</li> </ol> </li> <li> <p>RAM Usage: For small models, Ekkono requires a smaller RAM. However, as the model size increases, TFLM int8 only becomes more efficient. (1)</p> <ol> <li> Same story as the flash size.</li> </ol> </li> <li> <p>Conclusion: For small models, Ekkono is more efficient. However, as the model size increases, TFLM int8 only becomes more efficient.</p> </li> </ul>"},{"location":"results/TFLM_vs_eAI_Translator/","title":"TFLM vs Renesas eAI Translator","text":"<p>The goal of this study is to compare the performance of TensorFlow Lite for Microcontrollers (TFLM) and Renesas eAI Translator.</p> <p>According to the eAI Translator's documentation, it should be able to convert RNN models. However, we faced some errors during the conversion (1). As a result, this study focuses on FC, CNN, and TinyMLPerf models. Also, eAI Translator is designed to work with Renesas boards, so we only have the RenesasRX65N board in our study.</p> <ol> <li> Still, we guess it should be possible to convert RNN models with some modifications and under certain conditions.</li> </ol> <p></p> <p>Model Type:  Any FC CNN TinyMLPerf </p>"},{"location":"results/TFLM_vs_eAI_Translator/#models","title":"Models","text":"FC parameters and MACs CNN parameters and MACs TinyMLPerf parameters and MACs"},{"location":"results/TFLM_vs_eAI_Translator/#error","title":"Error","text":"FC - RenesasRX65N CNN - RenesasRX65N TinyMLPerf - RenesasRX65N"},{"location":"results/TFLM_vs_eAI_Translator/#execution-time","title":"Execution Time","text":"FC - RenesasRX65N CNN - RenesasRX65N TinyMLPerf - RenesasRX65N"},{"location":"results/TFLM_vs_eAI_Translator/#flash-size","title":"Flash Size","text":"FC - RenesasRX65N CNN - RenesasRX65N TinyMLPerf - RenesasRX65N"},{"location":"results/TFLM_vs_eAI_Translator/#ram-usage","title":"RAM Usage","text":"FC - RenesasRX65N CNN - RenesasRX65N TinyMLPerf - RenesasRX65N"},{"location":"results/TFLM_vs_eAI_Translator/#summary","title":"Summary","text":"<ul> <li> <p>Model Correctness:</p> <ul> <li> <p>Some models failed to run on the board. (1)</p> <ol> <li> For the failed FC models, the program halts for an unknown reason. For the failed TinyMLPerf models, the program is too large for the RenesasRX65N board.</li> </ol> </li> <li> <p>The error rate of the TFLM and eAI Translator models are normally the same, but for some big models, the eAI Translator has an unacceptable error. (1)</p> <ol> <li> Namely, CNN_5, CNN_6, CNN_7, TinyMLPerf_MBNet, and TinyMLPerf_ResNet models.</li> </ol> </li> </ul> </li> <li> <p>Execution Time:</p> <ul> <li> <p>Please note that we have not utilized CMSIS-NN with TFLM. As a result, the int8 only version of TFLM could potentially yield better results. Thus, we have excluded the int8 only variants from our execution time comparisons.</p> </li> <li> <p>eAI Translator is usually better. (1)</p> <ol> <li> eAI Translator is better especially for smaller models. For bigger ones the two are almost the same, or TFLM might even get a bit better.</li> </ol> </li> </ul> </li> <li> <p>Flash Size: eAI Translator is better.</p> </li> <li> <p>RAM Usage: eAI Translator is slightly better.</p> </li> <li> <p>Conclusion: If the error of the eAI Translator is acceptable, it is a better choice than TFLM for RenesasRX65N.</p> </li> </ul>"}]}